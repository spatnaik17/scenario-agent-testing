<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>Scenario API documentation</title>
<meta name="description" content="Configuration module for Scenario …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://scenario.langwatch.ai/favicon.ico" type="image/x-icon" />
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/scenario/"
><b class="navbar__title text--truncate">Scenario</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/scenario/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/scenario/reference/scenario/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/langwatch/scenario"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scenario.config</code></h1>
</header>
<section id="section-intro">
<p>Configuration module for Scenario.</p>
<p>This module provides configuration classes for customizing the behavior of the
Scenario testing framework, including LLM model settings, execution parameters,
and debugging options.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Configuration module for Scenario.

This module provides configuration classes for customizing the behavior of the
Scenario testing framework, including LLM model settings, execution parameters,
and debugging options.
&#34;&#34;&#34;

from typing import Optional, Union, ClassVar
from pydantic import BaseModel


class ModelConfig(BaseModel):
    &#34;&#34;&#34;
    Configuration for LLM model settings.

    This class encapsulates all the parameters needed to configure an LLM model
    for use with user simulator and judge agents in the Scenario framework.

    Attributes:
        model: The model identifier (e.g., &#34;openai/gpt-4.1&#34;, &#34;anthropic/claude-3-sonnet&#34;)
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for response generation (0.0 = deterministic, 1.0 = creative)
        max_tokens: Maximum number of tokens to generate in responses

    Example:
        ```
        model_config = ModelConfig(
            model=&#34;openai/gpt-4.1&#34;,
            api_key=&#34;your-api-key&#34;,
            temperature=0.1,
            max_tokens=1000
        )
        ```
    &#34;&#34;&#34;

    model: str
    api_key: Optional[str] = None
    temperature: float = 0.0
    max_tokens: Optional[int] = None


class ScenarioConfig(BaseModel):
    &#34;&#34;&#34;
    Global configuration class for the Scenario testing framework.

    This class allows users to set default behavior and parameters that apply
    to all scenario executions, including the LLM model to use for simulator
    and judge agents, execution limits, and debugging options.

    Attributes:
        default_model: Default LLM model configuration for agents (can be string or ModelConfig)
        max_turns: Maximum number of conversation turns before scenario times out
        verbose: Whether to show detailed output during execution (True/False or verbosity level)
        cache_key: Key for caching scenario results to ensure deterministic behavior
        debug: Whether to enable debug mode with step-by-step interaction

    Example:
        ```
        # Configure globally for all scenarios
        scenario.configure(
            default_model=&#34;openai/gpt-4.1&#34;,
            max_turns=15,
            verbose=True,
            cache_key=&#34;my-test-suite-v1&#34;,
            debug=False
        )

        # Or create a specific config instance
        config = ScenarioConfig(
            default_model=ModelConfig(
                model=&#34;openai/gpt-4.1&#34;,
                temperature=0.2
            ),
            max_turns=20
        )
        ```
    &#34;&#34;&#34;

    default_model: Optional[Union[str, ModelConfig]] = None
    max_turns: Optional[int] = 10
    verbose: Optional[Union[bool, int]] = True
    cache_key: Optional[str] = None
    debug: Optional[bool] = False

    default_config: ClassVar[Optional[&#34;ScenarioConfig&#34;]] = None

    @classmethod
    def configure(
        cls,
        default_model: Optional[str] = None,
        max_turns: Optional[int] = None,
        verbose: Optional[Union[bool, int]] = None,
        cache_key: Optional[str] = None,
        debug: Optional[bool] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Set global configuration settings for all scenario executions.

        This method allows you to configure default behavior that will be applied
        to all scenarios unless explicitly overridden in individual scenario runs.

        Args:
            default_model: Default LLM model identifier for user simulator and judge agents
            max_turns: Maximum number of conversation turns before timeout (default: 10)
            verbose: Enable verbose output during scenario execution
            cache_key: Cache key for deterministic scenario behavior across runs
            debug: Enable debug mode for step-by-step execution with user intervention

        Example:
            ```
            import scenario

            # Set up default configuration
            scenario.configure(
                default_model=&#34;openai/gpt-4.1&#34;,
                max_turns=15,
                verbose=True,
                debug=False
            )

            # All subsequent scenario runs will use these defaults
            result = await scenario.run(
                name=&#34;my test&#34;,
                description=&#34;Test scenario&#34;,
                agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
            )
            ```
        &#34;&#34;&#34;
        existing_config = cls.default_config or ScenarioConfig()

        cls.default_config = existing_config.merge(
            ScenarioConfig(
                default_model=default_model,
                max_turns=max_turns,
                verbose=verbose,
                cache_key=cache_key,
                debug=debug,
            )
        )

    def merge(self, other: &#34;ScenarioConfig&#34;) -&gt; &#34;ScenarioConfig&#34;:
        &#34;&#34;&#34;
        Merge this configuration with another configuration.

        Values from the other configuration will override values in this
        configuration where they are not None.

        Args:
            other: Another ScenarioConfig instance to merge with

        Returns:
            A new ScenarioConfig instance with merged values

        Example:
            ```
            base_config = ScenarioConfig(max_turns=10, verbose=True)
            override_config = ScenarioConfig(max_turns=20)

            merged = base_config.merge(override_config)
            # Result: max_turns=20, verbose=True
            ```
        &#34;&#34;&#34;
        return ScenarioConfig(
            **{
                **self.items(),
                **other.items(),
            }
        )

    def items(self):
        &#34;&#34;&#34;
        Get configuration items as a dictionary.

        Returns:
            Dictionary of configuration key-value pairs, excluding None values

        Example:
            ```
            config = ScenarioConfig(max_turns=15, verbose=True)
            items = config.items()
            # Result: {&#34;max_turns&#34;: 15, &#34;verbose&#34;: True}
            ```
        &#34;&#34;&#34;
        return {k: getattr(self, k) for k in self.model_dump(exclude_none=True).keys()}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scenario.config.ModelConfig"><code class="flex name class">
<span>class <span class="ident">ModelConfig</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for LLM model settings.</p>
<p>This class encapsulates all the parameters needed to configure an LLM model
for use with user simulator and judge agents in the Scenario framework.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>The model identifier (e.g., "openai/gpt-4.1", "anthropic/claude-3-sonnet")</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Optional API key for the model provider</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for response generation (0.0 = deterministic, 1.0 = creative)</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum number of tokens to generate in responses</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>model_config = ModelConfig(
    model=&quot;openai/gpt-4.1&quot;,
    api_key=&quot;your-api-key&quot;,
    temperature=0.1,
    max_tokens=1000
)
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelConfig(BaseModel):
    &#34;&#34;&#34;
    Configuration for LLM model settings.

    This class encapsulates all the parameters needed to configure an LLM model
    for use with user simulator and judge agents in the Scenario framework.

    Attributes:
        model: The model identifier (e.g., &#34;openai/gpt-4.1&#34;, &#34;anthropic/claude-3-sonnet&#34;)
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for response generation (0.0 = deterministic, 1.0 = creative)
        max_tokens: Maximum number of tokens to generate in responses

    Example:
        ```
        model_config = ModelConfig(
            model=&#34;openai/gpt-4.1&#34;,
            api_key=&#34;your-api-key&#34;,
            temperature=0.1,
            max_tokens=1000
        )
        ```
    &#34;&#34;&#34;

    model: str
    api_key: Optional[str] = None
    temperature: float = 0.0
    max_tokens: Optional[int] = None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.config.ModelConfig.api_key"><code class="name">var <span class="ident">api_key</span> : str | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ModelConfig.max_tokens"><code class="name">var <span class="ident">max_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ModelConfig.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ModelConfig.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ModelConfig.temperature"><code class="name">var <span class="ident">temperature</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="scenario.config.ScenarioConfig"><code class="flex name class">
<span>class <span class="ident">ScenarioConfig</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Global configuration class for the Scenario testing framework.</p>
<p>This class allows users to set default behavior and parameters that apply
to all scenario executions, including the LLM model to use for simulator
and judge agents, execution limits, and debugging options.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>default_model</code></strong></dt>
<dd>Default LLM model configuration for agents (can be string or ModelConfig)</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum number of conversation turns before scenario times out</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to show detailed output during execution (True/False or verbosity level)</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Key for caching scenario results to ensure deterministic behavior</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Whether to enable debug mode with step-by-step interaction</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code># Configure globally for all scenarios
scenario.configure(
    default_model=&quot;openai/gpt-4.1&quot;,
    max_turns=15,
    verbose=True,
    cache_key=&quot;my-test-suite-v1&quot;,
    debug=False
)

# Or create a specific config instance
config = ScenarioConfig(
    default_model=ModelConfig(
        model=&quot;openai/gpt-4.1&quot;,
        temperature=0.2
    ),
    max_turns=20
)
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioConfig(BaseModel):
    &#34;&#34;&#34;
    Global configuration class for the Scenario testing framework.

    This class allows users to set default behavior and parameters that apply
    to all scenario executions, including the LLM model to use for simulator
    and judge agents, execution limits, and debugging options.

    Attributes:
        default_model: Default LLM model configuration for agents (can be string or ModelConfig)
        max_turns: Maximum number of conversation turns before scenario times out
        verbose: Whether to show detailed output during execution (True/False or verbosity level)
        cache_key: Key for caching scenario results to ensure deterministic behavior
        debug: Whether to enable debug mode with step-by-step interaction

    Example:
        ```
        # Configure globally for all scenarios
        scenario.configure(
            default_model=&#34;openai/gpt-4.1&#34;,
            max_turns=15,
            verbose=True,
            cache_key=&#34;my-test-suite-v1&#34;,
            debug=False
        )

        # Or create a specific config instance
        config = ScenarioConfig(
            default_model=ModelConfig(
                model=&#34;openai/gpt-4.1&#34;,
                temperature=0.2
            ),
            max_turns=20
        )
        ```
    &#34;&#34;&#34;

    default_model: Optional[Union[str, ModelConfig]] = None
    max_turns: Optional[int] = 10
    verbose: Optional[Union[bool, int]] = True
    cache_key: Optional[str] = None
    debug: Optional[bool] = False

    default_config: ClassVar[Optional[&#34;ScenarioConfig&#34;]] = None

    @classmethod
    def configure(
        cls,
        default_model: Optional[str] = None,
        max_turns: Optional[int] = None,
        verbose: Optional[Union[bool, int]] = None,
        cache_key: Optional[str] = None,
        debug: Optional[bool] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Set global configuration settings for all scenario executions.

        This method allows you to configure default behavior that will be applied
        to all scenarios unless explicitly overridden in individual scenario runs.

        Args:
            default_model: Default LLM model identifier for user simulator and judge agents
            max_turns: Maximum number of conversation turns before timeout (default: 10)
            verbose: Enable verbose output during scenario execution
            cache_key: Cache key for deterministic scenario behavior across runs
            debug: Enable debug mode for step-by-step execution with user intervention

        Example:
            ```
            import scenario

            # Set up default configuration
            scenario.configure(
                default_model=&#34;openai/gpt-4.1&#34;,
                max_turns=15,
                verbose=True,
                debug=False
            )

            # All subsequent scenario runs will use these defaults
            result = await scenario.run(
                name=&#34;my test&#34;,
                description=&#34;Test scenario&#34;,
                agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
            )
            ```
        &#34;&#34;&#34;
        existing_config = cls.default_config or ScenarioConfig()

        cls.default_config = existing_config.merge(
            ScenarioConfig(
                default_model=default_model,
                max_turns=max_turns,
                verbose=verbose,
                cache_key=cache_key,
                debug=debug,
            )
        )

    def merge(self, other: &#34;ScenarioConfig&#34;) -&gt; &#34;ScenarioConfig&#34;:
        &#34;&#34;&#34;
        Merge this configuration with another configuration.

        Values from the other configuration will override values in this
        configuration where they are not None.

        Args:
            other: Another ScenarioConfig instance to merge with

        Returns:
            A new ScenarioConfig instance with merged values

        Example:
            ```
            base_config = ScenarioConfig(max_turns=10, verbose=True)
            override_config = ScenarioConfig(max_turns=20)

            merged = base_config.merge(override_config)
            # Result: max_turns=20, verbose=True
            ```
        &#34;&#34;&#34;
        return ScenarioConfig(
            **{
                **self.items(),
                **other.items(),
            }
        )

    def items(self):
        &#34;&#34;&#34;
        Get configuration items as a dictionary.

        Returns:
            Dictionary of configuration key-value pairs, excluding None values

        Example:
            ```
            config = ScenarioConfig(max_turns=15, verbose=True)
            items = config.items()
            # Result: {&#34;max_turns&#34;: 15, &#34;verbose&#34;: True}
            ```
        &#34;&#34;&#34;
        return {k: getattr(self, k) for k in self.model_dump(exclude_none=True).keys()}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.config.ScenarioConfig.cache_key"><code class="name">var <span class="ident">cache_key</span> : str | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.debug"><code class="name">var <span class="ident">debug</span> : bool | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.default_config"><code class="name">var <span class="ident">default_config</span> : ClassVar[<a title="scenario.config.ScenarioConfig" href="#scenario.config.ScenarioConfig">ScenarioConfig</a> | None]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.default_model"><code class="name">var <span class="ident">default_model</span> : str | <a title="scenario.config.ModelConfig" href="#scenario.config.ModelConfig">ModelConfig</a> | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.max_turns"><code class="name">var <span class="ident">max_turns</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.config.ScenarioConfig.verbose"><code class="name">var <span class="ident">verbose</span> : bool | int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scenario.config.ScenarioConfig.configure"><code class="name flex">
<span>def <span class="ident">configure</span></span>(<span>default_model: str | None = None, max_turns: int | None = None, verbose: bool | int | None = None, cache_key: str | None = None, debug: bool | None = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set global configuration settings for all scenario executions.</p>
<p>This method allows you to configure default behavior that will be applied
to all scenarios unless explicitly overridden in individual scenario runs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>default_model</code></strong></dt>
<dd>Default LLM model identifier for user simulator and judge agents</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum number of conversation turns before timeout (default: 10)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Enable verbose output during scenario execution</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Cache key for deterministic scenario behavior across runs</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Enable debug mode for step-by-step execution with user intervention</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Set up default configuration
scenario.configure(
    default_model=&quot;openai/gpt-4.1&quot;,
    max_turns=15,
    verbose=True,
    debug=False
)

# All subsequent scenario runs will use these defaults
result = await scenario.run(
    name=&quot;my test&quot;,
    description=&quot;Test scenario&quot;,
    agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
)
</code></pre></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.config.ScenarioConfig.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get configuration items as a dictionary.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary of configuration key-value pairs, excluding None values</p>
<h2 id="example">Example</h2>
<pre><code>config = ScenarioConfig(max_turns=15, verbose=True)
items = config.items()
# Result: {&quot;max_turns&quot;: 15, &quot;verbose&quot;: True}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    &#34;&#34;&#34;
    Get configuration items as a dictionary.

    Returns:
        Dictionary of configuration key-value pairs, excluding None values

    Example:
        ```
        config = ScenarioConfig(max_turns=15, verbose=True)
        items = config.items()
        # Result: {&#34;max_turns&#34;: 15, &#34;verbose&#34;: True}
        ```
    &#34;&#34;&#34;
    return {k: getattr(self, k) for k in self.model_dump(exclude_none=True).keys()}</code></pre>
</details>
</dd>
<dt id="scenario.config.ScenarioConfig.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, other: <a title="scenario.config.ScenarioConfig" href="#scenario.config.ScenarioConfig">ScenarioConfig</a>) ‑> <a title="scenario.config.ScenarioConfig" href="#scenario.config.ScenarioConfig">ScenarioConfig</a></span>
</code></dt>
<dd>
<div class="desc"><p>Merge this configuration with another configuration.</p>
<p>Values from the other configuration will override values in this
configuration where they are not None.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>Another ScenarioConfig instance to merge with</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A new ScenarioConfig instance with merged values</p>
<h2 id="example">Example</h2>
<pre><code>base_config = ScenarioConfig(max_turns=10, verbose=True)
override_config = ScenarioConfig(max_turns=20)

merged = base_config.merge(override_config)
# Result: max_turns=20, verbose=True
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, other: &#34;ScenarioConfig&#34;) -&gt; &#34;ScenarioConfig&#34;:
    &#34;&#34;&#34;
    Merge this configuration with another configuration.

    Values from the other configuration will override values in this
    configuration where they are not None.

    Args:
        other: Another ScenarioConfig instance to merge with

    Returns:
        A new ScenarioConfig instance with merged values

    Example:
        ```
        base_config = ScenarioConfig(max_turns=10, verbose=True)
        override_config = ScenarioConfig(max_turns=20)

        merged = base_config.merge(override_config)
        # Result: max_turns=20, verbose=True
        ```
    &#34;&#34;&#34;
    return ScenarioConfig(
        **{
            **self.items(),
            **other.items(),
        }
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/" style="color: #000">← Back to Docs</a>
<h1>Scenario API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scenario" href="index.html">scenario</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scenario.config.ModelConfig" href="#scenario.config.ModelConfig">ModelConfig</a></code></h4>
<ul class="">
<li><code><a title="scenario.config.ModelConfig.api_key" href="#scenario.config.ModelConfig.api_key">api_key</a></code></li>
<li><code><a title="scenario.config.ModelConfig.max_tokens" href="#scenario.config.ModelConfig.max_tokens">max_tokens</a></code></li>
<li><code><a title="scenario.config.ModelConfig.model" href="#scenario.config.ModelConfig.model">model</a></code></li>
<li><code><a title="scenario.config.ModelConfig.model_config" href="#scenario.config.ModelConfig.model_config">model_config</a></code></li>
<li><code><a title="scenario.config.ModelConfig.temperature" href="#scenario.config.ModelConfig.temperature">temperature</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.config.ScenarioConfig" href="#scenario.config.ScenarioConfig">ScenarioConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.config.ScenarioConfig.cache_key" href="#scenario.config.ScenarioConfig.cache_key">cache_key</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.configure" href="#scenario.config.ScenarioConfig.configure">configure</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.debug" href="#scenario.config.ScenarioConfig.debug">debug</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.default_config" href="#scenario.config.ScenarioConfig.default_config">default_config</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.default_model" href="#scenario.config.ScenarioConfig.default_model">default_model</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.items" href="#scenario.config.ScenarioConfig.items">items</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.max_turns" href="#scenario.config.ScenarioConfig.max_turns">max_turns</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.merge" href="#scenario.config.ScenarioConfig.merge">merge</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.model_config" href="#scenario.config.ScenarioConfig.model_config">model_config</a></code></li>
<li><code><a title="scenario.config.ScenarioConfig.verbose" href="#scenario.config.ScenarioConfig.verbose">verbose</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
