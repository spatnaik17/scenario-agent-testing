<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>Scenario API documentation</title>
<meta name="description" content="Judge agent module for evaluating scenario conversations …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://scenario.langwatch.ai/favicon.ico" type="image/x-icon" />
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/"
><b class="navbar__title text--truncate">Scenario</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/reference/python/scenario/"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/langwatch/scenario"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scenario.judge_agent</code></h1>
</header>
<section id="section-intro">
<p>Judge agent module for evaluating scenario conversations.</p>
<p>This module provides the JudgeAgent class, which evaluates ongoing conversations
between users and agents to determine if success criteria are met. The judge
makes real-time decisions about whether scenarios should continue or end with
success/failure verdicts.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Judge agent module for evaluating scenario conversations.

This module provides the JudgeAgent class, which evaluates ongoing conversations
between users and agents to determine if success criteria are met. The judge
makes real-time decisions about whether scenarios should continue or end with
success/failure verdicts.
&#34;&#34;&#34;

import json
import logging
import re
from typing import List, Optional, cast

from litellm import Choices, completion
from litellm.files.main import ModelResponse

from scenario.cache import scenario_cache
from scenario.agent_adapter import AgentAdapter
from scenario.config import ModelConfig, ScenarioConfig

from ._error_messages import agent_not_configured_error_message
from .types import AgentInput, AgentReturnTypes, AgentRole, ScenarioResult


logger = logging.getLogger(&#34;scenario&#34;)


class JudgeAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that evaluates conversations against success criteria.

    The JudgeAgent watches conversations in real-time and makes decisions about
    whether the agent under test is meeting the specified criteria. It can either
    allow the conversation to continue or end it with a success/failure verdict.

    The judge uses function calling to make structured decisions and provides
    detailed reasoning for its verdicts. It evaluates each criterion independently
    and provides comprehensive feedback about what worked and what didn&#39;t.

    Attributes:
        role: Always AgentRole.JUDGE for judge agents
        model: LLM model identifier to use for evaluation
        api_base: Optional base URL where the model is hosted
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for evaluation consistency
        max_tokens: Maximum tokens for judge reasoning
        criteria: List of success criteria to evaluate against
        system_prompt: Custom system prompt to override default judge behavior

    Example:
        ```
        import scenario

        # Basic judge agent with criteria
        judge = scenario.JudgeAgent(
            criteria=[
                &#34;Agent provides helpful responses&#34;,
                &#34;Agent asks relevant follow-up questions&#34;,
                &#34;Agent does not provide harmful information&#34;
            ]
        )

        # Customized judge with specific model and behavior
        strict_judge = scenario.JudgeAgent(
            model=&#34;openai/gpt-4.1&#34;,
            criteria=[
                &#34;Code examples are syntactically correct&#34;,
                &#34;Explanations are technically accurate&#34;,
                &#34;Security best practices are mentioned&#34;
            ],
            temperature=0.0,  # More deterministic evaluation
            system_prompt=&#34;You are a strict technical reviewer evaluating code quality.&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;coding assistant test&#34;,
            description=&#34;User asks for help with Python functions&#34;,
            agents=[
                coding_agent,
                scenario.UserSimulatorAgent(),
                judge
            ]
        )

        print(f&#34;Passed criteria: {result.passed_criteria}&#34;)
        print(f&#34;Failed criteria: {result.failed_criteria}&#34;)
        ```

    Note:
        - Judge agents evaluate conversations continuously, not just at the end
        - They can end scenarios early if clear success/failure conditions are met
        - Provide detailed reasoning for their decisions
        - Support both positive criteria (things that should happen) and negative criteria (things that shouldn&#39;t)
    &#34;&#34;&#34;

    role = AgentRole.JUDGE

    model: str
    api_base: Optional[str]
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    criteria: List[str]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        criteria: Optional[List[str]] = None,
        model: Optional[str] = None,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a judge agent with evaluation criteria.

        Args:
            criteria: List of success criteria to evaluate the conversation against.
                     Can include both positive requirements (&#34;Agent provides helpful responses&#34;)
                     and negative constraints (&#34;Agent should not provide personal information&#34;).
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1&#34;).
                   If not provided, uses the default model from global configuration.
            api_base: Optional base URL where the model is hosted. If not provided,
                      uses the base URL from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for evaluation (0.0-1.0).
                        Lower values (0.0-0.2) recommended for consistent evaluation.
            max_tokens: Maximum number of tokens for judge reasoning and explanations.
            system_prompt: Custom system prompt to override default judge behavior.
                          Use this to create specialized evaluation perspectives.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Customer service judge
            cs_judge = JudgeAgent(
                criteria=[
                    &#34;Agent replies with the refund policy&#34;,
                    &#34;Agent offers next steps for the customer&#34;,
                ],
                temperature=0.1
            )

            # Technical accuracy judge
            tech_judge = JudgeAgent(
                criteria=[
                    &#34;Agent adds a code review pointing out the code compilation errors&#34;,
                    &#34;Agent adds a code review about the missing security headers&#34;
                ],
                system_prompt=&#34;You are a senior software engineer reviewing code for production use.&#34;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the judge agent
        self.criteria = criteria or []
        self.api_base = api_base
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_base = (
                api_base or ScenarioConfig.default_config.default_model.api_base
            )
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Evaluate the current conversation state against the configured criteria.

        This method analyzes the conversation history and determines whether the
        scenario should continue or end with a verdict. It uses function calling
        to make structured decisions and provides detailed reasoning.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: Either an empty list (continue scenario) or a
                            ScenarioResult (end scenario with verdict)

        Raises:
            Exception: If the judge cannot make a valid decision or if there&#39;s an
                      error in the evaluation process

        Note:
            - Returns empty list [] to continue the scenario
            - Returns ScenarioResult to end with success/failure
            - Provides detailed reasoning for all decisions
            - Evaluates each criterion independently
            - Can end scenarios early if clear violation or success is detected
        &#34;&#34;&#34;

        scenario = input.scenario_state

        criteria_str = &#34;\n&#34;.join(
            [f&#34;{idx + 1}. {criterion}&#34; for idx, criterion in enumerate(self.criteria)]
        )

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are an LLM as a judge watching a simulated conversation as it plays out live to determine if the agent under test meets the criteria or not.
&lt;/role&gt;

&lt;goal&gt;
Your goal is to determine if you already have enough information to make a verdict of the scenario below, or if the conversation should continue for longer.
If you do have enough information, use the finish_test tool to determine if all the criteria have been met, if not, use the continue_test tool to let the next step play out.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;criteria&gt;
{criteria_str}
&lt;/criteria&gt;

&lt;rules&gt;
- Be strict, do not let the conversation continue if the agent already broke one of the &#34;do not&#34; or &#34;should not&#34; criterias.
- DO NOT make any judgment calls that are not explicitly listed in the success or failure criteria, withhold judgement if necessary
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            *input.messages,
        ]

        is_last_message = (
            input.scenario_state.current_turn == input.scenario_state.config.max_turns
        )

        if is_last_message:
            messages.append(
                {
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: &#34;&#34;&#34;
System:

&lt;finish_test&gt;
This is the last message, conversation has reached the maximum number of turns, give your final verdict,
if you don&#39;t have enough information to make a verdict, say inconclusive with max turns reached.
&lt;/finish_test&gt;
&#34;&#34;&#34;,
                }
            )

        # Define the tools
        criteria_names = [
            re.sub(
                r&#34;[^a-zA-Z0-9]&#34;,
                &#34;_&#34;,
                criterion.replace(&#34; &#34;, &#34;_&#34;).replace(&#34;&#39;&#34;, &#34;&#34;).lower(),
            )[:70]
            for criterion in self.criteria
        ]
        tools = [
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;continue_test&#34;,
                    &#34;description&#34;: &#34;Continue the test with the next step&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {},
                        &#34;required&#34;: [],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;finish_test&#34;,
                    &#34;description&#34;: &#34;Complete the test with a final verdict&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {
                            &#34;criteria&#34;: {
                                &#34;type&#34;: &#34;object&#34;,
                                &#34;properties&#34;: {
                                    criteria_names[idx]: {
                                        &#34;enum&#34;: [True, False, &#34;inconclusive&#34;],
                                        &#34;description&#34;: criterion,
                                    }
                                    for idx, criterion in enumerate(self.criteria)
                                },
                                &#34;required&#34;: criteria_names,
                                &#34;additionalProperties&#34;: False,
                                &#34;description&#34;: &#34;Strict verdict for each criterion&#34;,
                            },
                            &#34;reasoning&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;description&#34;: &#34;Explanation of what the final verdict should be&#34;,
                            },
                            &#34;verdict&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;enum&#34;: [&#34;success&#34;, &#34;failure&#34;, &#34;inconclusive&#34;],
                                &#34;description&#34;: &#34;The final verdict of the test&#34;,
                            },
                        },
                        &#34;required&#34;: [&#34;criteria&#34;, &#34;reasoning&#34;, &#34;verdict&#34;],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
        ]

        enforce_judgment = input.judgment_request
        has_criteria = len(self.criteria) &gt; 0

        if enforce_judgment and not has_criteria:
            return ScenarioResult(
                success=False,
                messages=[],
                reasoning=&#34;TestingAgent was called as a judge, but it has no criteria to judge against&#34;,
            )

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=(
                    {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;finish_test&#34;}}
                    if (is_last_message or enforce_judgment) and has_criteria
                    else &#34;required&#34;
                ),
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            # Check if the LLM chose to use the tool
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                if tool_call.function.name == &#34;continue_test&#34;:
                    return []

                if tool_call.function.name == &#34;finish_test&#34;:
                    # Parse the tool call arguments
                    try:
                        args = json.loads(tool_call.function.arguments)
                        verdict = args.get(&#34;verdict&#34;, &#34;inconclusive&#34;)
                        reasoning = args.get(&#34;reasoning&#34;, &#34;No reasoning provided&#34;)
                        criteria = args.get(&#34;criteria&#34;, {})

                        passed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == True
                        ]
                        failed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == False
                        ]

                        # Return the appropriate ScenarioResult based on the verdict
                        return ScenarioResult(
                            success=verdict == &#34;success&#34; and len(failed_criteria) == 0,
                            messages=messages,
                            reasoning=reasoning,
                            passed_criteria=passed_criteria,
                            failed_criteria=failed_criteria,
                        )
                    except json.JSONDecodeError:
                        raise Exception(
                            f&#34;Failed to parse tool call arguments from judge agent: {tool_call.function.arguments}&#34;
                        )

                else:
                    raise Exception(
                        f&#34;Invalid tool call from judge agent: {tool_call.function.name}&#34;
                    )

            else:
                raise Exception(
                    f&#34;Invalid response from judge agent, tool calls not found: {message.__repr__()}&#34;
                )

        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scenario.judge_agent.JudgeAgent"><code class="flex name class">
<span>class <span class="ident">JudgeAgent</span></span>
<span>(</span><span>*, criteria: List[str] | None = None, model: str | None = None, api_base: str | None = None, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, system_prompt: str | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Agent that evaluates conversations against success criteria.</p>
<p>The JudgeAgent watches conversations in real-time and makes decisions about
whether the agent under test is meeting the specified criteria. It can either
allow the conversation to continue or end it with a success/failure verdict.</p>
<p>The judge uses function calling to make structured decisions and provides
detailed reasoning for its verdicts. It evaluates each criterion independently
and provides comprehensive feedback about what worked and what didn't.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong></dt>
<dd>Always AgentRole.JUDGE for judge agents</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier to use for evaluation</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Optional API key for the model provider</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for evaluation consistency</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum tokens for judge reasoning</dd>
<dt><strong><code>criteria</code></strong></dt>
<dd>List of success criteria to evaluate against</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default judge behavior</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Basic judge agent with criteria
judge = scenario.JudgeAgent(
    criteria=[
        &quot;Agent provides helpful responses&quot;,
        &quot;Agent asks relevant follow-up questions&quot;,
        &quot;Agent does not provide harmful information&quot;
    ]
)

# Customized judge with specific model and behavior
strict_judge = scenario.JudgeAgent(
    model=&quot;openai/gpt-4.1&quot;,
    criteria=[
        &quot;Code examples are syntactically correct&quot;,
        &quot;Explanations are technically accurate&quot;,
        &quot;Security best practices are mentioned&quot;
    ],
    temperature=0.0,  # More deterministic evaluation
    system_prompt=&quot;You are a strict technical reviewer evaluating code quality.&quot;
)

# Use in scenario
result = await scenario.run(
    name=&quot;coding assistant test&quot;,
    description=&quot;User asks for help with Python functions&quot;,
    agents=[
        coding_agent,
        scenario.UserSimulatorAgent(),
        judge
    ]
)

print(f&quot;Passed criteria: {result.passed_criteria}&quot;)
print(f&quot;Failed criteria: {result.failed_criteria}&quot;)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Judge agents evaluate conversations continuously, not just at the end</li>
<li>They can end scenarios early if clear success/failure conditions are met</li>
<li>Provide detailed reasoning for their decisions</li>
<li>Support both positive criteria (things that should happen) and negative criteria (things that shouldn't)</li>
</ul>
<p>Initialize a judge agent with evaluation criteria.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>criteria</code></strong></dt>
<dd>List of success criteria to evaluate the conversation against.
Can include both positive requirements ("Agent provides helpful responses")
and negative constraints ("Agent should not provide personal information").</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier (e.g., "openai/gpt-4.1").
If not provided, uses the default model from global configuration.</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted. If not provided,
uses the base URL from global configuration.</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>API key for the model provider. If not provided,
uses the key from global configuration or environment.</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for evaluation (0.0-1.0).
Lower values (0.0-0.2) recommended for consistent evaluation.</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum number of tokens for judge reasoning and explanations.</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default judge behavior.
Use this to create specialized evaluation perspectives.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model is configured either in parameters or global config</dd>
</dl>
<h2 id="example_1">Example</h2>
<pre><code># Customer service judge
cs_judge = JudgeAgent(
    criteria=[
        &quot;Agent replies with the refund policy&quot;,
        &quot;Agent offers next steps for the customer&quot;,
    ],
    temperature=0.1
)

# Technical accuracy judge
tech_judge = JudgeAgent(
    criteria=[
        &quot;Agent adds a code review pointing out the code compilation errors&quot;,
        &quot;Agent adds a code review about the missing security headers&quot;
    ],
    system_prompt=&quot;You are a senior software engineer reviewing code for production use.&quot;
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JudgeAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that evaluates conversations against success criteria.

    The JudgeAgent watches conversations in real-time and makes decisions about
    whether the agent under test is meeting the specified criteria. It can either
    allow the conversation to continue or end it with a success/failure verdict.

    The judge uses function calling to make structured decisions and provides
    detailed reasoning for its verdicts. It evaluates each criterion independently
    and provides comprehensive feedback about what worked and what didn&#39;t.

    Attributes:
        role: Always AgentRole.JUDGE for judge agents
        model: LLM model identifier to use for evaluation
        api_base: Optional base URL where the model is hosted
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for evaluation consistency
        max_tokens: Maximum tokens for judge reasoning
        criteria: List of success criteria to evaluate against
        system_prompt: Custom system prompt to override default judge behavior

    Example:
        ```
        import scenario

        # Basic judge agent with criteria
        judge = scenario.JudgeAgent(
            criteria=[
                &#34;Agent provides helpful responses&#34;,
                &#34;Agent asks relevant follow-up questions&#34;,
                &#34;Agent does not provide harmful information&#34;
            ]
        )

        # Customized judge with specific model and behavior
        strict_judge = scenario.JudgeAgent(
            model=&#34;openai/gpt-4.1&#34;,
            criteria=[
                &#34;Code examples are syntactically correct&#34;,
                &#34;Explanations are technically accurate&#34;,
                &#34;Security best practices are mentioned&#34;
            ],
            temperature=0.0,  # More deterministic evaluation
            system_prompt=&#34;You are a strict technical reviewer evaluating code quality.&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;coding assistant test&#34;,
            description=&#34;User asks for help with Python functions&#34;,
            agents=[
                coding_agent,
                scenario.UserSimulatorAgent(),
                judge
            ]
        )

        print(f&#34;Passed criteria: {result.passed_criteria}&#34;)
        print(f&#34;Failed criteria: {result.failed_criteria}&#34;)
        ```

    Note:
        - Judge agents evaluate conversations continuously, not just at the end
        - They can end scenarios early if clear success/failure conditions are met
        - Provide detailed reasoning for their decisions
        - Support both positive criteria (things that should happen) and negative criteria (things that shouldn&#39;t)
    &#34;&#34;&#34;

    role = AgentRole.JUDGE

    model: str
    api_base: Optional[str]
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    criteria: List[str]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        criteria: Optional[List[str]] = None,
        model: Optional[str] = None,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a judge agent with evaluation criteria.

        Args:
            criteria: List of success criteria to evaluate the conversation against.
                     Can include both positive requirements (&#34;Agent provides helpful responses&#34;)
                     and negative constraints (&#34;Agent should not provide personal information&#34;).
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1&#34;).
                   If not provided, uses the default model from global configuration.
            api_base: Optional base URL where the model is hosted. If not provided,
                      uses the base URL from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for evaluation (0.0-1.0).
                        Lower values (0.0-0.2) recommended for consistent evaluation.
            max_tokens: Maximum number of tokens for judge reasoning and explanations.
            system_prompt: Custom system prompt to override default judge behavior.
                          Use this to create specialized evaluation perspectives.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Customer service judge
            cs_judge = JudgeAgent(
                criteria=[
                    &#34;Agent replies with the refund policy&#34;,
                    &#34;Agent offers next steps for the customer&#34;,
                ],
                temperature=0.1
            )

            # Technical accuracy judge
            tech_judge = JudgeAgent(
                criteria=[
                    &#34;Agent adds a code review pointing out the code compilation errors&#34;,
                    &#34;Agent adds a code review about the missing security headers&#34;
                ],
                system_prompt=&#34;You are a senior software engineer reviewing code for production use.&#34;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the judge agent
        self.criteria = criteria or []
        self.api_base = api_base
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_base = (
                api_base or ScenarioConfig.default_config.default_model.api_base
            )
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Evaluate the current conversation state against the configured criteria.

        This method analyzes the conversation history and determines whether the
        scenario should continue or end with a verdict. It uses function calling
        to make structured decisions and provides detailed reasoning.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: Either an empty list (continue scenario) or a
                            ScenarioResult (end scenario with verdict)

        Raises:
            Exception: If the judge cannot make a valid decision or if there&#39;s an
                      error in the evaluation process

        Note:
            - Returns empty list [] to continue the scenario
            - Returns ScenarioResult to end with success/failure
            - Provides detailed reasoning for all decisions
            - Evaluates each criterion independently
            - Can end scenarios early if clear violation or success is detected
        &#34;&#34;&#34;

        scenario = input.scenario_state

        criteria_str = &#34;\n&#34;.join(
            [f&#34;{idx + 1}. {criterion}&#34; for idx, criterion in enumerate(self.criteria)]
        )

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are an LLM as a judge watching a simulated conversation as it plays out live to determine if the agent under test meets the criteria or not.
&lt;/role&gt;

&lt;goal&gt;
Your goal is to determine if you already have enough information to make a verdict of the scenario below, or if the conversation should continue for longer.
If you do have enough information, use the finish_test tool to determine if all the criteria have been met, if not, use the continue_test tool to let the next step play out.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;criteria&gt;
{criteria_str}
&lt;/criteria&gt;

&lt;rules&gt;
- Be strict, do not let the conversation continue if the agent already broke one of the &#34;do not&#34; or &#34;should not&#34; criterias.
- DO NOT make any judgment calls that are not explicitly listed in the success or failure criteria, withhold judgement if necessary
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            *input.messages,
        ]

        is_last_message = (
            input.scenario_state.current_turn == input.scenario_state.config.max_turns
        )

        if is_last_message:
            messages.append(
                {
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: &#34;&#34;&#34;
System:

&lt;finish_test&gt;
This is the last message, conversation has reached the maximum number of turns, give your final verdict,
if you don&#39;t have enough information to make a verdict, say inconclusive with max turns reached.
&lt;/finish_test&gt;
&#34;&#34;&#34;,
                }
            )

        # Define the tools
        criteria_names = [
            re.sub(
                r&#34;[^a-zA-Z0-9]&#34;,
                &#34;_&#34;,
                criterion.replace(&#34; &#34;, &#34;_&#34;).replace(&#34;&#39;&#34;, &#34;&#34;).lower(),
            )[:70]
            for criterion in self.criteria
        ]
        tools = [
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;continue_test&#34;,
                    &#34;description&#34;: &#34;Continue the test with the next step&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {},
                        &#34;required&#34;: [],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;finish_test&#34;,
                    &#34;description&#34;: &#34;Complete the test with a final verdict&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {
                            &#34;criteria&#34;: {
                                &#34;type&#34;: &#34;object&#34;,
                                &#34;properties&#34;: {
                                    criteria_names[idx]: {
                                        &#34;enum&#34;: [True, False, &#34;inconclusive&#34;],
                                        &#34;description&#34;: criterion,
                                    }
                                    for idx, criterion in enumerate(self.criteria)
                                },
                                &#34;required&#34;: criteria_names,
                                &#34;additionalProperties&#34;: False,
                                &#34;description&#34;: &#34;Strict verdict for each criterion&#34;,
                            },
                            &#34;reasoning&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;description&#34;: &#34;Explanation of what the final verdict should be&#34;,
                            },
                            &#34;verdict&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;enum&#34;: [&#34;success&#34;, &#34;failure&#34;, &#34;inconclusive&#34;],
                                &#34;description&#34;: &#34;The final verdict of the test&#34;,
                            },
                        },
                        &#34;required&#34;: [&#34;criteria&#34;, &#34;reasoning&#34;, &#34;verdict&#34;],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
        ]

        enforce_judgment = input.judgment_request
        has_criteria = len(self.criteria) &gt; 0

        if enforce_judgment and not has_criteria:
            return ScenarioResult(
                success=False,
                messages=[],
                reasoning=&#34;TestingAgent was called as a judge, but it has no criteria to judge against&#34;,
            )

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=(
                    {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;finish_test&#34;}}
                    if (is_last_message or enforce_judgment) and has_criteria
                    else &#34;required&#34;
                ),
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            # Check if the LLM chose to use the tool
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                if tool_call.function.name == &#34;continue_test&#34;:
                    return []

                if tool_call.function.name == &#34;finish_test&#34;:
                    # Parse the tool call arguments
                    try:
                        args = json.loads(tool_call.function.arguments)
                        verdict = args.get(&#34;verdict&#34;, &#34;inconclusive&#34;)
                        reasoning = args.get(&#34;reasoning&#34;, &#34;No reasoning provided&#34;)
                        criteria = args.get(&#34;criteria&#34;, {})

                        passed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == True
                        ]
                        failed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == False
                        ]

                        # Return the appropriate ScenarioResult based on the verdict
                        return ScenarioResult(
                            success=verdict == &#34;success&#34; and len(failed_criteria) == 0,
                            messages=messages,
                            reasoning=reasoning,
                            passed_criteria=passed_criteria,
                            failed_criteria=failed_criteria,
                        )
                    except json.JSONDecodeError:
                        raise Exception(
                            f&#34;Failed to parse tool call arguments from judge agent: {tool_call.function.arguments}&#34;
                        )

                else:
                    raise Exception(
                        f&#34;Invalid tool call from judge agent: {tool_call.function.name}&#34;
                    )

            else:
                raise Exception(
                    f&#34;Invalid response from judge agent, tool calls not found: {message.__repr__()}&#34;
                )

        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.judge_agent.JudgeAgent.api_base"><code class="name">var <span class="ident">api_base</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.api_key"><code class="name">var <span class="ident">api_key</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.criteria"><code class="name">var <span class="ident">criteria</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.max_tokens"><code class="name">var <span class="ident">max_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.system_prompt"><code class="name">var <span class="ident">system_prompt</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.judge_agent.JudgeAgent.temperature"><code class="name">var <span class="ident">temperature</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.judge_agent.JudgeAgent.call"><code class="name flex">
<span>async def <span class="ident">call</span></span>(<span>self, input: <a title="scenario.types.AgentInput" href="types.html#scenario.types.AgentInput">AgentInput</a>) ‑> str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam] | <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the current conversation state against the configured criteria.</p>
<p>This method analyzes the conversation history and determines whether the
scenario should continue or end with a verdict. It uses function calling
to make structured decisions and provides detailed reasoning.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>AgentInput containing conversation history and scenario context</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AgentReturnTypes</code></dt>
<dd>Either an empty list (continue scenario) or a
ScenarioResult (end scenario with verdict)</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If the judge cannot make a valid decision or if there's an
error in the evaluation process</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Returns empty list [] to continue the scenario</li>
<li>Returns ScenarioResult to end with success/failure</li>
<li>Provides detailed reasoning for all decisions</li>
<li>Evaluates each criterion independently</li>
<li>Can end scenarios early if clear violation or success is detected</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Evaluate the current conversation state against the configured criteria.

        This method analyzes the conversation history and determines whether the
        scenario should continue or end with a verdict. It uses function calling
        to make structured decisions and provides detailed reasoning.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: Either an empty list (continue scenario) or a
                            ScenarioResult (end scenario with verdict)

        Raises:
            Exception: If the judge cannot make a valid decision or if there&#39;s an
                      error in the evaluation process

        Note:
            - Returns empty list [] to continue the scenario
            - Returns ScenarioResult to end with success/failure
            - Provides detailed reasoning for all decisions
            - Evaluates each criterion independently
            - Can end scenarios early if clear violation or success is detected
        &#34;&#34;&#34;

        scenario = input.scenario_state

        criteria_str = &#34;\n&#34;.join(
            [f&#34;{idx + 1}. {criterion}&#34; for idx, criterion in enumerate(self.criteria)]
        )

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are an LLM as a judge watching a simulated conversation as it plays out live to determine if the agent under test meets the criteria or not.
&lt;/role&gt;

&lt;goal&gt;
Your goal is to determine if you already have enough information to make a verdict of the scenario below, or if the conversation should continue for longer.
If you do have enough information, use the finish_test tool to determine if all the criteria have been met, if not, use the continue_test tool to let the next step play out.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;criteria&gt;
{criteria_str}
&lt;/criteria&gt;

&lt;rules&gt;
- Be strict, do not let the conversation continue if the agent already broke one of the &#34;do not&#34; or &#34;should not&#34; criterias.
- DO NOT make any judgment calls that are not explicitly listed in the success or failure criteria, withhold judgement if necessary
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            *input.messages,
        ]

        is_last_message = (
            input.scenario_state.current_turn == input.scenario_state.config.max_turns
        )

        if is_last_message:
            messages.append(
                {
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: &#34;&#34;&#34;
System:

&lt;finish_test&gt;
This is the last message, conversation has reached the maximum number of turns, give your final verdict,
if you don&#39;t have enough information to make a verdict, say inconclusive with max turns reached.
&lt;/finish_test&gt;
&#34;&#34;&#34;,
                }
            )

        # Define the tools
        criteria_names = [
            re.sub(
                r&#34;[^a-zA-Z0-9]&#34;,
                &#34;_&#34;,
                criterion.replace(&#34; &#34;, &#34;_&#34;).replace(&#34;&#39;&#34;, &#34;&#34;).lower(),
            )[:70]
            for criterion in self.criteria
        ]
        tools = [
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;continue_test&#34;,
                    &#34;description&#34;: &#34;Continue the test with the next step&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {},
                        &#34;required&#34;: [],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;finish_test&#34;,
                    &#34;description&#34;: &#34;Complete the test with a final verdict&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {
                            &#34;criteria&#34;: {
                                &#34;type&#34;: &#34;object&#34;,
                                &#34;properties&#34;: {
                                    criteria_names[idx]: {
                                        &#34;enum&#34;: [True, False, &#34;inconclusive&#34;],
                                        &#34;description&#34;: criterion,
                                    }
                                    for idx, criterion in enumerate(self.criteria)
                                },
                                &#34;required&#34;: criteria_names,
                                &#34;additionalProperties&#34;: False,
                                &#34;description&#34;: &#34;Strict verdict for each criterion&#34;,
                            },
                            &#34;reasoning&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;description&#34;: &#34;Explanation of what the final verdict should be&#34;,
                            },
                            &#34;verdict&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;enum&#34;: [&#34;success&#34;, &#34;failure&#34;, &#34;inconclusive&#34;],
                                &#34;description&#34;: &#34;The final verdict of the test&#34;,
                            },
                        },
                        &#34;required&#34;: [&#34;criteria&#34;, &#34;reasoning&#34;, &#34;verdict&#34;],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
        ]

        enforce_judgment = input.judgment_request
        has_criteria = len(self.criteria) &gt; 0

        if enforce_judgment and not has_criteria:
            return ScenarioResult(
                success=False,
                messages=[],
                reasoning=&#34;TestingAgent was called as a judge, but it has no criteria to judge against&#34;,
            )

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=(
                    {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;finish_test&#34;}}
                    if (is_last_message or enforce_judgment) and has_criteria
                    else &#34;required&#34;
                ),
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            # Check if the LLM chose to use the tool
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                if tool_call.function.name == &#34;continue_test&#34;:
                    return []

                if tool_call.function.name == &#34;finish_test&#34;:
                    # Parse the tool call arguments
                    try:
                        args = json.loads(tool_call.function.arguments)
                        verdict = args.get(&#34;verdict&#34;, &#34;inconclusive&#34;)
                        reasoning = args.get(&#34;reasoning&#34;, &#34;No reasoning provided&#34;)
                        criteria = args.get(&#34;criteria&#34;, {})

                        passed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == True
                        ]
                        failed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == False
                        ]

                        # Return the appropriate ScenarioResult based on the verdict
                        return ScenarioResult(
                            success=verdict == &#34;success&#34; and len(failed_criteria) == 0,
                            messages=messages,
                            reasoning=reasoning,
                            passed_criteria=passed_criteria,
                            failed_criteria=failed_criteria,
                        )
                    except json.JSONDecodeError:
                        raise Exception(
                            f&#34;Failed to parse tool call arguments from judge agent: {tool_call.function.arguments}&#34;
                        )

                else:
                    raise Exception(
                        f&#34;Invalid tool call from judge agent: {tool_call.function.name}&#34;
                    )

            else:
                raise Exception(
                    f&#34;Invalid response from judge agent, tool calls not found: {message.__repr__()}&#34;
                )

        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></b></code>:
<ul class="hlist">
<li><code><a title="scenario.agent_adapter.AgentAdapter.role" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter.role">role</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/" style="color: #000">← Back to Docs</a>
<h1>Scenario API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scenario" href="index.html">scenario</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scenario.judge_agent.JudgeAgent" href="#scenario.judge_agent.JudgeAgent">JudgeAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.judge_agent.JudgeAgent.api_base" href="#scenario.judge_agent.JudgeAgent.api_base">api_base</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.api_key" href="#scenario.judge_agent.JudgeAgent.api_key">api_key</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.call" href="#scenario.judge_agent.JudgeAgent.call">call</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.criteria" href="#scenario.judge_agent.JudgeAgent.criteria">criteria</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.max_tokens" href="#scenario.judge_agent.JudgeAgent.max_tokens">max_tokens</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.model" href="#scenario.judge_agent.JudgeAgent.model">model</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.system_prompt" href="#scenario.judge_agent.JudgeAgent.system_prompt">system_prompt</a></code></li>
<li><code><a title="scenario.judge_agent.JudgeAgent.temperature" href="#scenario.judge_agent.JudgeAgent.temperature">temperature</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
