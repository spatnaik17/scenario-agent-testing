<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>Scenario API documentation</title>
<meta name="description" content="Pytest plugin for Scenario testing library …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://scenario.langwatch.ai/favicon.ico" type="image/x-icon" />
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/scenario/"
><b class="navbar__title text--truncate">Scenario</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/scenario/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/scenario/reference/scenario/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/langwatch/scenario"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scenario.pytest_plugin</code></h1>
</header>
<section id="section-intro">
<p>Pytest plugin for Scenario testing library.</p>
<p>This module provides pytest integration for the Scenario framework, including
automatic test reporting, debug mode support, and collection of scenario
results across test runs. It enables seamless integration with existing
pytest-based testing workflows.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Pytest plugin for Scenario testing library.

This module provides pytest integration for the Scenario framework, including
automatic test reporting, debug mode support, and collection of scenario
results across test runs. It enables seamless integration with existing
pytest-based testing workflows.
&#34;&#34;&#34;

import pytest
from typing import TypedDict
import functools
from termcolor import colored

from scenario.config import ScenarioConfig
from scenario.types import ScenarioResult

from .scenario_executor import ScenarioExecutor


class ScenarioReporterResults(TypedDict):
    &#34;&#34;&#34;
    Type definition for scenario test results stored by the reporter.

    Attributes:
        scenario: The ScenarioExecutor instance that ran the test
        result: The ScenarioResult containing test outcome and details
    &#34;&#34;&#34;

    scenario: ScenarioExecutor
    result: ScenarioResult


# ScenarioReporter class definition moved outside the fixture for global use
class ScenarioReporter:
    &#34;&#34;&#34;
    Collects and reports on scenario test results across a pytest session.

    This class automatically collects results from all scenario tests run during
    a pytest session and provides comprehensive reporting including success rates,
    timing information, and detailed failure analysis.

    The reporter is automatically instantiated by the pytest plugin and collects
    results from all scenario.run() calls without requiring explicit user setup.

    Attributes:
        results: List of all scenario test results collected during the session
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize an empty scenario reporter.&#34;&#34;&#34;
        self.results: list[ScenarioReporterResults] = []

    def add_result(self, scenario: ScenarioExecutor, result: ScenarioResult):
        &#34;&#34;&#34;
        Add a test result to the reporter.

        This method is called automatically by the pytest plugin whenever
        a scenario.run() call completes. It stores both the scenario
        configuration and the test result for later reporting.

        Args:
            scenario: The ScenarioExecutor instance that ran the test
            result: The ScenarioResult containing test outcome and details
        &#34;&#34;&#34;
        self.results.append({&#34;scenario&#34;: scenario, &#34;result&#34;: result})

    def get_summary(self):
        &#34;&#34;&#34;
        Get a summary of all test results.

        Calculates aggregate statistics across all scenario tests that
        have been run during the current pytest session.

        Returns:
            Dictionary containing summary statistics:
            - total: Total number of scenarios run
            - passed: Number of scenarios that passed
            - failed: Number of scenarios that failed
            - success_rate: Percentage of scenarios that passed (0-100)
        &#34;&#34;&#34;
        total = len(self.results)
        passed = sum(1 for r in self.results if r[&#34;result&#34;].success)
        failed = total - passed

        return {
            &#34;total&#34;: total,
            &#34;passed&#34;: passed,
            &#34;failed&#34;: failed,
            &#34;success_rate&#34;: round(passed / total * 100, 2) if total else 0,
        }

    def print_report(self):
        &#34;&#34;&#34;
        Print a detailed report of all test results.

        Outputs a comprehensive report to the console showing:
        - Overall summary statistics
        - Individual scenario results with success/failure status
        - Detailed reasoning for each scenario outcome
        - Timing information when available
        - Criteria pass/fail breakdown for judge-evaluated scenarios

        The report is automatically printed at the end of pytest sessions,
        but can also be called manually for intermediate reporting.

        Example output:
        ```
        === Scenario Test Report ===
        Total Scenarios: 5
        Passed: 4
        Failed: 1
        Success Rate: 80%

        1. weather query test - PASSED in 2.34s (agent: 1.12s)
           Reasoning: Agent successfully provided weather information
           Passed Criteria: 2/2

        2. complex math problem - FAILED in 5.67s (agent: 3.45s)
           Reasoning: Agent provided incorrect calculation
           Failed Criteria: 1
        ```
        &#34;&#34;&#34;
        if not self.results:
            return  # Skip report if no results

        summary = self.get_summary()

        print(&#34;\n&#34; + colored(&#34;=== Scenario Test Report ===&#34;, &#34;cyan&#34;, attrs=[&#34;bold&#34;]))
        print(colored(f&#34;Total Scenarios: {summary[&#39;total&#39;]}&#34;, &#34;white&#34;))
        print(
            colored(
                f&#34;Passed: {summary[&#39;passed&#39;]}&#34;,
                &#34;green&#34; if summary[&#34;passed&#34;] &gt; 0 else &#34;white&#34;,
            )
        )
        print(
            colored(
                f&#34;Failed: {summary[&#39;failed&#39;]}&#34;,
                &#34;red&#34; if summary[&#34;failed&#34;] &gt; 0 else &#34;white&#34;,
            )
        )

        # Color the success rate based on its value
        success_rate = summary[&#34;success_rate&#34;]
        rate_color = (
            &#34;green&#34;
            if success_rate == 100
            else &#34;yellow&#34; if success_rate &gt;= 70 else &#34;red&#34;
        )
        print(colored(f&#34;Success Rate: {success_rate}%&#34;, rate_color))

        for idx, item in enumerate(self.results, 1):
            scenario = item[&#34;scenario&#34;]
            result = item[&#34;result&#34;]

            status = &#34;PASSED&#34; if result.success else &#34;FAILED&#34;
            status_color = &#34;green&#34; if result.success else &#34;red&#34;

            time = &#34;&#34;
            if result.total_time and result.agent_time:
                time = f&#34; in {result.total_time:.2f}s (agent: {result.agent_time:.2f}s)&#34;

            print(
                f&#34;\n{idx}. {scenario.name} - {colored(status, status_color, attrs=[&#39;bold&#39;])}{time}&#34;
            )

            print(
                colored(
                    f&#34;   Reasoning: {result.reasoning}&#34;,
                    &#34;green&#34; if result.success else &#34;red&#34;,
                )
            )

            if hasattr(result, &#34;passed_criteria&#34;) and result.passed_criteria:
                criteria_count = len(result.passed_criteria)
                total_criteria = len(result.passed_criteria) + len(
                    result.failed_criteria
                )
                criteria_color = (
                    &#34;green&#34; if criteria_count == total_criteria else &#34;yellow&#34;
                )
                print(
                    colored(
                        f&#34;   Passed Criteria: {criteria_count}/{total_criteria}&#34;,
                        criteria_color,
                    )
                )

            if hasattr(result, &#34;failed_criteria&#34;) and result.failed_criteria:
                print(
                    colored(
                        f&#34;   Failed Criteria: {len(result.failed_criteria)}&#34;,
                        &#34;red&#34;,
                    )
                )


# Store the original run method
original_run = ScenarioExecutor.run


@pytest.hookimpl(trylast=True)
def pytest_configure(config):
    &#34;&#34;&#34;
    Configure pytest integration for Scenario testing.

    This hook is called when pytest starts and sets up:
    - Registration of the @pytest.mark.agent_test marker
    - Debug mode configuration from command line arguments
    - Global scenario reporter for collecting results
    - Automatic result collection from all scenario.run() calls

    Args:
        config: pytest configuration object

    Note:
        This function runs automatically when pytest loads the plugin.
        Users don&#39;t need to call it directly.

    Debug Mode:
        When --debug is passed to pytest, enables step-by-step scenario
        execution with user intervention capabilities.

    Example:
        ```bash
        # Enable debug mode for all scenarios
        pytest tests/ --debug -s

        # Run normally
        pytest tests/
        ```
    &#34;&#34;&#34;
    # Register the marker
    config.addinivalue_line(
        &#34;markers&#34;, &#34;agent_test: mark test as an agent scenario test&#34;
    )

    if config.getoption(&#34;--debug&#34;):
        print(colored(&#34;\nScenario debug mode enabled (--debug).&#34;, &#34;yellow&#34;))
        ScenarioConfig.configure(verbose=True, debug=True)

    # Create a global reporter instance
    config._scenario_reporter = ScenarioReporter()

    # Create a patched version of Scenario.run that auto-reports
    @functools.wraps(original_run)
    async def auto_reporting_run(self, *args, **kwargs):
        result = await original_run(self, *args, **kwargs)

        # Always report to the global reporter
        # Ensure the reporter exists before adding result
        if hasattr(config, &#34;_scenario_reporter&#34;):
            config._scenario_reporter.add_result(self, result)
        else:
            # Handle case where reporter might not be initialized (should not happen with current setup)
            print(colored(&#34;Warning: Scenario reporter not found during run.&#34;, &#34;yellow&#34;))

        return result

    # Apply the patch
    ScenarioExecutor.run = auto_reporting_run


@pytest.hookimpl(trylast=True)
def pytest_unconfigure(config):
    &#34;&#34;&#34;
    Clean up pytest integration when pytest exits.

    This hook is called when pytest is shutting down and:
    - Prints the final scenario test report
    - Restores the original ScenarioExecutor.run method
    - Cleans up any remaining resources

    Args:
        config: pytest configuration object

    Note:
        This function runs automatically when pytest exits.
        Users don&#39;t need to call it directly.
    &#34;&#34;&#34;
    # Print the final report
    if hasattr(config, &#34;_scenario_reporter&#34;):
        config._scenario_reporter.print_report()

    # Restore the original method
    ScenarioExecutor.run = original_run


@pytest.fixture
def scenario_reporter(request):
    &#34;&#34;&#34;
    Pytest fixture for accessing the global scenario reporter.

    This fixture provides access to the same reporter that&#39;s used for automatic
    reporting, allowing tests to explicitly interact with the reporter if needed.

    Args:
        request: pytest request object containing test context

    Yields:
        ScenarioReporter: The global reporter instance collecting all scenario results

    Example:
        ```
        @pytest.mark.agent_test
        def test_with_custom_reporting(scenario_reporter):
            # Run your scenarios
            result1 = await scenario.run(
                name=&#34;test 1&#34;,
                description=&#34;First test&#34;,
                agents=[agent, user_sim, judge]
            )

            result2 = await scenario.run(
                name=&#34;test 2&#34;,
                description=&#34;Second test&#34;,
                agents=[agent, user_sim, judge]
            )

            # Access collected results
            assert len(scenario_reporter.results) == 2

            # Check success rate
            summary = scenario_reporter.get_summary()
            assert summary[&#39;success_rate&#39;] &gt;= 90

            # Print intermediate report
            if summary[&#39;failed&#39;] &gt; 0:
                scenario_reporter.print_report()
        ```

    Note:
        The reporter automatically collects results from all scenario.run() calls,
        so you don&#39;t need to manually add results unless you&#39;re doing custom reporting.
    &#34;&#34;&#34;
    # Get the global reporter from pytest config
    reporter = request.config._scenario_reporter
    yield reporter
    # No need to print report here as it&#39;s handled by pytest_unconfigure</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="scenario.pytest_plugin.pytest_configure"><code class="name flex">
<span>def <span class="ident">pytest_configure</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure pytest integration for Scenario testing.</p>
<p>This hook is called when pytest starts and sets up:
- Registration of the @pytest.mark.agent_test marker
- Debug mode configuration from command line arguments
- Global scenario reporter for collecting results
- Automatic result collection from all scenario.run() calls</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>pytest configuration object</dd>
</dl>
<h2 id="note">Note</h2>
<p>This function runs automatically when pytest loads the plugin.
Users don't need to call it directly.</p>
<p>Debug Mode:
When &ndash;debug is passed to pytest, enables step-by-step scenario
execution with user intervention capabilities.</p>
<h2 id="example">Example</h2>
<pre><code class="language-bash"># Enable debug mode for all scenarios
pytest tests/ --debug -s

# Run normally
pytest tests/
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.hookimpl(trylast=True)
def pytest_configure(config):
    &#34;&#34;&#34;
    Configure pytest integration for Scenario testing.

    This hook is called when pytest starts and sets up:
    - Registration of the @pytest.mark.agent_test marker
    - Debug mode configuration from command line arguments
    - Global scenario reporter for collecting results
    - Automatic result collection from all scenario.run() calls

    Args:
        config: pytest configuration object

    Note:
        This function runs automatically when pytest loads the plugin.
        Users don&#39;t need to call it directly.

    Debug Mode:
        When --debug is passed to pytest, enables step-by-step scenario
        execution with user intervention capabilities.

    Example:
        ```bash
        # Enable debug mode for all scenarios
        pytest tests/ --debug -s

        # Run normally
        pytest tests/
        ```
    &#34;&#34;&#34;
    # Register the marker
    config.addinivalue_line(
        &#34;markers&#34;, &#34;agent_test: mark test as an agent scenario test&#34;
    )

    if config.getoption(&#34;--debug&#34;):
        print(colored(&#34;\nScenario debug mode enabled (--debug).&#34;, &#34;yellow&#34;))
        ScenarioConfig.configure(verbose=True, debug=True)

    # Create a global reporter instance
    config._scenario_reporter = ScenarioReporter()

    # Create a patched version of Scenario.run that auto-reports
    @functools.wraps(original_run)
    async def auto_reporting_run(self, *args, **kwargs):
        result = await original_run(self, *args, **kwargs)

        # Always report to the global reporter
        # Ensure the reporter exists before adding result
        if hasattr(config, &#34;_scenario_reporter&#34;):
            config._scenario_reporter.add_result(self, result)
        else:
            # Handle case where reporter might not be initialized (should not happen with current setup)
            print(colored(&#34;Warning: Scenario reporter not found during run.&#34;, &#34;yellow&#34;))

        return result

    # Apply the patch
    ScenarioExecutor.run = auto_reporting_run</code></pre>
</details>
</dd>
<dt id="scenario.pytest_plugin.pytest_unconfigure"><code class="name flex">
<span>def <span class="ident">pytest_unconfigure</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Clean up pytest integration when pytest exits.</p>
<p>This hook is called when pytest is shutting down and:
- Prints the final scenario test report
- Restores the original ScenarioExecutor.run method
- Cleans up any remaining resources</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>pytest configuration object</dd>
</dl>
<h2 id="note">Note</h2>
<p>This function runs automatically when pytest exits.
Users don't need to call it directly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.hookimpl(trylast=True)
def pytest_unconfigure(config):
    &#34;&#34;&#34;
    Clean up pytest integration when pytest exits.

    This hook is called when pytest is shutting down and:
    - Prints the final scenario test report
    - Restores the original ScenarioExecutor.run method
    - Cleans up any remaining resources

    Args:
        config: pytest configuration object

    Note:
        This function runs automatically when pytest exits.
        Users don&#39;t need to call it directly.
    &#34;&#34;&#34;
    # Print the final report
    if hasattr(config, &#34;_scenario_reporter&#34;):
        config._scenario_reporter.print_report()

    # Restore the original method
    ScenarioExecutor.run = original_run</code></pre>
</details>
</dd>
<dt id="scenario.pytest_plugin.scenario_reporter"><code class="name flex">
<span>def <span class="ident">scenario_reporter</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"><p>Pytest fixture for accessing the global scenario reporter.</p>
<p>This fixture provides access to the same reporter that's used for automatic
reporting, allowing tests to explicitly interact with the reporter if needed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>request</code></strong></dt>
<dd>pytest request object containing test context</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code><a title="scenario.pytest_plugin.ScenarioReporter" href="#scenario.pytest_plugin.ScenarioReporter">ScenarioReporter</a></code></dt>
<dd>The global reporter instance collecting all scenario results</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>@pytest.mark.agent_test
def test_with_custom_reporting(scenario_reporter):
    # Run your scenarios
    result1 = await scenario.run(
        name=&quot;test 1&quot;,
        description=&quot;First test&quot;,
        agents=[agent, user_sim, judge]
    )

    result2 = await scenario.run(
        name=&quot;test 2&quot;,
        description=&quot;Second test&quot;,
        agents=[agent, user_sim, judge]
    )

    # Access collected results
    assert len(scenario_reporter.results) == 2

    # Check success rate
    summary = scenario_reporter.get_summary()
    assert summary['success_rate'] &gt;= 90

    # Print intermediate report
    if summary['failed'] &gt; 0:
        scenario_reporter.print_report()
</code></pre>
<h2 id="note">Note</h2>
<p>The reporter automatically collects results from all scenario.run() calls,
so you don't need to manually add results unless you're doing custom reporting.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def scenario_reporter(request):
    &#34;&#34;&#34;
    Pytest fixture for accessing the global scenario reporter.

    This fixture provides access to the same reporter that&#39;s used for automatic
    reporting, allowing tests to explicitly interact with the reporter if needed.

    Args:
        request: pytest request object containing test context

    Yields:
        ScenarioReporter: The global reporter instance collecting all scenario results

    Example:
        ```
        @pytest.mark.agent_test
        def test_with_custom_reporting(scenario_reporter):
            # Run your scenarios
            result1 = await scenario.run(
                name=&#34;test 1&#34;,
                description=&#34;First test&#34;,
                agents=[agent, user_sim, judge]
            )

            result2 = await scenario.run(
                name=&#34;test 2&#34;,
                description=&#34;Second test&#34;,
                agents=[agent, user_sim, judge]
            )

            # Access collected results
            assert len(scenario_reporter.results) == 2

            # Check success rate
            summary = scenario_reporter.get_summary()
            assert summary[&#39;success_rate&#39;] &gt;= 90

            # Print intermediate report
            if summary[&#39;failed&#39;] &gt; 0:
                scenario_reporter.print_report()
        ```

    Note:
        The reporter automatically collects results from all scenario.run() calls,
        so you don&#39;t need to manually add results unless you&#39;re doing custom reporting.
    &#34;&#34;&#34;
    # Get the global reporter from pytest config
    reporter = request.config._scenario_reporter
    yield reporter
    # No need to print report here as it&#39;s handled by pytest_unconfigure</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scenario.pytest_plugin.ScenarioReporter"><code class="flex name class">
<span>class <span class="ident">ScenarioReporter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Collects and reports on scenario test results across a pytest session.</p>
<p>This class automatically collects results from all scenario tests run during
a pytest session and provides comprehensive reporting including success rates,
timing information, and detailed failure analysis.</p>
<p>The reporter is automatically instantiated by the pytest plugin and collects
results from all scenario.run() calls without requiring explicit user setup.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>results</code></strong></dt>
<dd>List of all scenario test results collected during the session</dd>
</dl>
<p>Initialize an empty scenario reporter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioReporter:
    &#34;&#34;&#34;
    Collects and reports on scenario test results across a pytest session.

    This class automatically collects results from all scenario tests run during
    a pytest session and provides comprehensive reporting including success rates,
    timing information, and detailed failure analysis.

    The reporter is automatically instantiated by the pytest plugin and collects
    results from all scenario.run() calls without requiring explicit user setup.

    Attributes:
        results: List of all scenario test results collected during the session
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize an empty scenario reporter.&#34;&#34;&#34;
        self.results: list[ScenarioReporterResults] = []

    def add_result(self, scenario: ScenarioExecutor, result: ScenarioResult):
        &#34;&#34;&#34;
        Add a test result to the reporter.

        This method is called automatically by the pytest plugin whenever
        a scenario.run() call completes. It stores both the scenario
        configuration and the test result for later reporting.

        Args:
            scenario: The ScenarioExecutor instance that ran the test
            result: The ScenarioResult containing test outcome and details
        &#34;&#34;&#34;
        self.results.append({&#34;scenario&#34;: scenario, &#34;result&#34;: result})

    def get_summary(self):
        &#34;&#34;&#34;
        Get a summary of all test results.

        Calculates aggregate statistics across all scenario tests that
        have been run during the current pytest session.

        Returns:
            Dictionary containing summary statistics:
            - total: Total number of scenarios run
            - passed: Number of scenarios that passed
            - failed: Number of scenarios that failed
            - success_rate: Percentage of scenarios that passed (0-100)
        &#34;&#34;&#34;
        total = len(self.results)
        passed = sum(1 for r in self.results if r[&#34;result&#34;].success)
        failed = total - passed

        return {
            &#34;total&#34;: total,
            &#34;passed&#34;: passed,
            &#34;failed&#34;: failed,
            &#34;success_rate&#34;: round(passed / total * 100, 2) if total else 0,
        }

    def print_report(self):
        &#34;&#34;&#34;
        Print a detailed report of all test results.

        Outputs a comprehensive report to the console showing:
        - Overall summary statistics
        - Individual scenario results with success/failure status
        - Detailed reasoning for each scenario outcome
        - Timing information when available
        - Criteria pass/fail breakdown for judge-evaluated scenarios

        The report is automatically printed at the end of pytest sessions,
        but can also be called manually for intermediate reporting.

        Example output:
        ```
        === Scenario Test Report ===
        Total Scenarios: 5
        Passed: 4
        Failed: 1
        Success Rate: 80%

        1. weather query test - PASSED in 2.34s (agent: 1.12s)
           Reasoning: Agent successfully provided weather information
           Passed Criteria: 2/2

        2. complex math problem - FAILED in 5.67s (agent: 3.45s)
           Reasoning: Agent provided incorrect calculation
           Failed Criteria: 1
        ```
        &#34;&#34;&#34;
        if not self.results:
            return  # Skip report if no results

        summary = self.get_summary()

        print(&#34;\n&#34; + colored(&#34;=== Scenario Test Report ===&#34;, &#34;cyan&#34;, attrs=[&#34;bold&#34;]))
        print(colored(f&#34;Total Scenarios: {summary[&#39;total&#39;]}&#34;, &#34;white&#34;))
        print(
            colored(
                f&#34;Passed: {summary[&#39;passed&#39;]}&#34;,
                &#34;green&#34; if summary[&#34;passed&#34;] &gt; 0 else &#34;white&#34;,
            )
        )
        print(
            colored(
                f&#34;Failed: {summary[&#39;failed&#39;]}&#34;,
                &#34;red&#34; if summary[&#34;failed&#34;] &gt; 0 else &#34;white&#34;,
            )
        )

        # Color the success rate based on its value
        success_rate = summary[&#34;success_rate&#34;]
        rate_color = (
            &#34;green&#34;
            if success_rate == 100
            else &#34;yellow&#34; if success_rate &gt;= 70 else &#34;red&#34;
        )
        print(colored(f&#34;Success Rate: {success_rate}%&#34;, rate_color))

        for idx, item in enumerate(self.results, 1):
            scenario = item[&#34;scenario&#34;]
            result = item[&#34;result&#34;]

            status = &#34;PASSED&#34; if result.success else &#34;FAILED&#34;
            status_color = &#34;green&#34; if result.success else &#34;red&#34;

            time = &#34;&#34;
            if result.total_time and result.agent_time:
                time = f&#34; in {result.total_time:.2f}s (agent: {result.agent_time:.2f}s)&#34;

            print(
                f&#34;\n{idx}. {scenario.name} - {colored(status, status_color, attrs=[&#39;bold&#39;])}{time}&#34;
            )

            print(
                colored(
                    f&#34;   Reasoning: {result.reasoning}&#34;,
                    &#34;green&#34; if result.success else &#34;red&#34;,
                )
            )

            if hasattr(result, &#34;passed_criteria&#34;) and result.passed_criteria:
                criteria_count = len(result.passed_criteria)
                total_criteria = len(result.passed_criteria) + len(
                    result.failed_criteria
                )
                criteria_color = (
                    &#34;green&#34; if criteria_count == total_criteria else &#34;yellow&#34;
                )
                print(
                    colored(
                        f&#34;   Passed Criteria: {criteria_count}/{total_criteria}&#34;,
                        criteria_color,
                    )
                )

            if hasattr(result, &#34;failed_criteria&#34;) and result.failed_criteria:
                print(
                    colored(
                        f&#34;   Failed Criteria: {len(result.failed_criteria)}&#34;,
                        &#34;red&#34;,
                    )
                )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scenario.pytest_plugin.ScenarioReporter.add_result"><code class="name flex">
<span>def <span class="ident">add_result</span></span>(<span>self, scenario: <a title="scenario.scenario_executor.ScenarioExecutor" href="scenario_executor.html#scenario.scenario_executor.ScenarioExecutor">ScenarioExecutor</a>, result: <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a test result to the reporter.</p>
<p>This method is called automatically by the pytest plugin whenever
a scenario.run() call completes. It stores both the scenario
configuration and the test result for later reporting.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scenario</code></strong></dt>
<dd>The ScenarioExecutor instance that ran the test</dd>
<dt><strong><code>result</code></strong></dt>
<dd>The ScenarioResult containing test outcome and details</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_result(self, scenario: ScenarioExecutor, result: ScenarioResult):
    &#34;&#34;&#34;
    Add a test result to the reporter.

    This method is called automatically by the pytest plugin whenever
    a scenario.run() call completes. It stores both the scenario
    configuration and the test result for later reporting.

    Args:
        scenario: The ScenarioExecutor instance that ran the test
        result: The ScenarioResult containing test outcome and details
    &#34;&#34;&#34;
    self.results.append({&#34;scenario&#34;: scenario, &#34;result&#34;: result})</code></pre>
</details>
</dd>
<dt id="scenario.pytest_plugin.ScenarioReporter.get_summary"><code class="name flex">
<span>def <span class="ident">get_summary</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a summary of all test results.</p>
<p>Calculates aggregate statistics across all scenario tests that
have been run during the current pytest session.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary containing summary statistics:
- total: Total number of scenarios run
- passed: Number of scenarios that passed
- failed: Number of scenarios that failed
- success_rate: Percentage of scenarios that passed (0-100)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_summary(self):
    &#34;&#34;&#34;
    Get a summary of all test results.

    Calculates aggregate statistics across all scenario tests that
    have been run during the current pytest session.

    Returns:
        Dictionary containing summary statistics:
        - total: Total number of scenarios run
        - passed: Number of scenarios that passed
        - failed: Number of scenarios that failed
        - success_rate: Percentage of scenarios that passed (0-100)
    &#34;&#34;&#34;
    total = len(self.results)
    passed = sum(1 for r in self.results if r[&#34;result&#34;].success)
    failed = total - passed

    return {
        &#34;total&#34;: total,
        &#34;passed&#34;: passed,
        &#34;failed&#34;: failed,
        &#34;success_rate&#34;: round(passed / total * 100, 2) if total else 0,
    }</code></pre>
</details>
</dd>
<dt id="scenario.pytest_plugin.ScenarioReporter.print_report"><code class="name flex">
<span>def <span class="ident">print_report</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a detailed report of all test results.</p>
<p>Outputs a comprehensive report to the console showing:
- Overall summary statistics
- Individual scenario results with success/failure status
- Detailed reasoning for each scenario outcome
- Timing information when available
- Criteria pass/fail breakdown for judge-evaluated scenarios</p>
<p>The report is automatically printed at the end of pytest sessions,
but can also be called manually for intermediate reporting.</p>
<p>Example output:</p>
<pre><code>=== Scenario Test Report ===
Total Scenarios: 5
Passed: 4
Failed: 1
Success Rate: 80%

1. weather query test - PASSED in 2.34s (agent: 1.12s)
   Reasoning: Agent successfully provided weather information
   Passed Criteria: 2/2

2. complex math problem - FAILED in 5.67s (agent: 3.45s)
   Reasoning: Agent provided incorrect calculation
   Failed Criteria: 1
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_report(self):
    &#34;&#34;&#34;
    Print a detailed report of all test results.

    Outputs a comprehensive report to the console showing:
    - Overall summary statistics
    - Individual scenario results with success/failure status
    - Detailed reasoning for each scenario outcome
    - Timing information when available
    - Criteria pass/fail breakdown for judge-evaluated scenarios

    The report is automatically printed at the end of pytest sessions,
    but can also be called manually for intermediate reporting.

    Example output:
    ```
    === Scenario Test Report ===
    Total Scenarios: 5
    Passed: 4
    Failed: 1
    Success Rate: 80%

    1. weather query test - PASSED in 2.34s (agent: 1.12s)
       Reasoning: Agent successfully provided weather information
       Passed Criteria: 2/2

    2. complex math problem - FAILED in 5.67s (agent: 3.45s)
       Reasoning: Agent provided incorrect calculation
       Failed Criteria: 1
    ```
    &#34;&#34;&#34;
    if not self.results:
        return  # Skip report if no results

    summary = self.get_summary()

    print(&#34;\n&#34; + colored(&#34;=== Scenario Test Report ===&#34;, &#34;cyan&#34;, attrs=[&#34;bold&#34;]))
    print(colored(f&#34;Total Scenarios: {summary[&#39;total&#39;]}&#34;, &#34;white&#34;))
    print(
        colored(
            f&#34;Passed: {summary[&#39;passed&#39;]}&#34;,
            &#34;green&#34; if summary[&#34;passed&#34;] &gt; 0 else &#34;white&#34;,
        )
    )
    print(
        colored(
            f&#34;Failed: {summary[&#39;failed&#39;]}&#34;,
            &#34;red&#34; if summary[&#34;failed&#34;] &gt; 0 else &#34;white&#34;,
        )
    )

    # Color the success rate based on its value
    success_rate = summary[&#34;success_rate&#34;]
    rate_color = (
        &#34;green&#34;
        if success_rate == 100
        else &#34;yellow&#34; if success_rate &gt;= 70 else &#34;red&#34;
    )
    print(colored(f&#34;Success Rate: {success_rate}%&#34;, rate_color))

    for idx, item in enumerate(self.results, 1):
        scenario = item[&#34;scenario&#34;]
        result = item[&#34;result&#34;]

        status = &#34;PASSED&#34; if result.success else &#34;FAILED&#34;
        status_color = &#34;green&#34; if result.success else &#34;red&#34;

        time = &#34;&#34;
        if result.total_time and result.agent_time:
            time = f&#34; in {result.total_time:.2f}s (agent: {result.agent_time:.2f}s)&#34;

        print(
            f&#34;\n{idx}. {scenario.name} - {colored(status, status_color, attrs=[&#39;bold&#39;])}{time}&#34;
        )

        print(
            colored(
                f&#34;   Reasoning: {result.reasoning}&#34;,
                &#34;green&#34; if result.success else &#34;red&#34;,
            )
        )

        if hasattr(result, &#34;passed_criteria&#34;) and result.passed_criteria:
            criteria_count = len(result.passed_criteria)
            total_criteria = len(result.passed_criteria) + len(
                result.failed_criteria
            )
            criteria_color = (
                &#34;green&#34; if criteria_count == total_criteria else &#34;yellow&#34;
            )
            print(
                colored(
                    f&#34;   Passed Criteria: {criteria_count}/{total_criteria}&#34;,
                    criteria_color,
                )
            )

        if hasattr(result, &#34;failed_criteria&#34;) and result.failed_criteria:
            print(
                colored(
                    f&#34;   Failed Criteria: {len(result.failed_criteria)}&#34;,
                    &#34;red&#34;,
                )
            )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scenario.pytest_plugin.ScenarioReporterResults"><code class="flex name class">
<span>class <span class="ident">ScenarioReporterResults</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Type definition for scenario test results stored by the reporter.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scenario</code></strong></dt>
<dd>The ScenarioExecutor instance that ran the test</dd>
<dt><strong><code>result</code></strong></dt>
<dd>The ScenarioResult containing test outcome and details</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioReporterResults(TypedDict):
    &#34;&#34;&#34;
    Type definition for scenario test results stored by the reporter.

    Attributes:
        scenario: The ScenarioExecutor instance that ran the test
        result: The ScenarioResult containing test outcome and details
    &#34;&#34;&#34;

    scenario: ScenarioExecutor
    result: ScenarioResult</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.pytest_plugin.ScenarioReporterResults.result"><code class="name">var <span class="ident">result</span> : <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scenario.pytest_plugin.ScenarioReporterResults.scenario"><code class="name">var <span class="ident">scenario</span> : <a title="scenario.scenario_executor.ScenarioExecutor" href="scenario_executor.html#scenario.scenario_executor.ScenarioExecutor">ScenarioExecutor</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/" style="color: #000">← Back to Docs</a>
<h1>Scenario API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scenario" href="index.html">scenario</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="scenario.pytest_plugin.pytest_configure" href="#scenario.pytest_plugin.pytest_configure">pytest_configure</a></code></li>
<li><code><a title="scenario.pytest_plugin.pytest_unconfigure" href="#scenario.pytest_plugin.pytest_unconfigure">pytest_unconfigure</a></code></li>
<li><code><a title="scenario.pytest_plugin.scenario_reporter" href="#scenario.pytest_plugin.scenario_reporter">scenario_reporter</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scenario.pytest_plugin.ScenarioReporter" href="#scenario.pytest_plugin.ScenarioReporter">ScenarioReporter</a></code></h4>
<ul class="">
<li><code><a title="scenario.pytest_plugin.ScenarioReporter.add_result" href="#scenario.pytest_plugin.ScenarioReporter.add_result">add_result</a></code></li>
<li><code><a title="scenario.pytest_plugin.ScenarioReporter.get_summary" href="#scenario.pytest_plugin.ScenarioReporter.get_summary">get_summary</a></code></li>
<li><code><a title="scenario.pytest_plugin.ScenarioReporter.print_report" href="#scenario.pytest_plugin.ScenarioReporter.print_report">print_report</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.pytest_plugin.ScenarioReporterResults" href="#scenario.pytest_plugin.ScenarioReporterResults">ScenarioReporterResults</a></code></h4>
<ul class="">
<li><code><a title="scenario.pytest_plugin.ScenarioReporterResults.result" href="#scenario.pytest_plugin.ScenarioReporterResults.result">result</a></code></li>
<li><code><a title="scenario.pytest_plugin.ScenarioReporterResults.scenario" href="#scenario.pytest_plugin.ScenarioReporterResults.scenario">scenario</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
