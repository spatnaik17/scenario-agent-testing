<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>Scenario API documentation</title>
<meta name="description" content="User simulator agent module for generating realistic user interactions …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="/favicon.ico" type="image/x-icon" />
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/scenario/"
><b class="navbar__title text--truncate">Scenario</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/scenario/docs/intro"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/scenario/reference/scenario/index.html"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/langwatch/scenario"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scenario.user_simulator_agent</code></h1>
</header>
<section id="section-intro">
<p>User simulator agent module for generating realistic user interactions.</p>
<p>This module provides the UserSimulatorAgent class, which simulates human user
behavior in conversations with agents under test. The simulator generates
contextually appropriate user messages based on the scenario description and
conversation history.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
User simulator agent module for generating realistic user interactions.

This module provides the UserSimulatorAgent class, which simulates human user
behavior in conversations with agents under test. The simulator generates
contextually appropriate user messages based on the scenario description and
conversation history.
&#34;&#34;&#34;

import logging
from typing import Optional, cast

from litellm import Choices, completion
from litellm.files.main import ModelResponse

from scenario.cache import scenario_cache
from scenario.agent_adapter import AgentAdapter
from scenario._utils import reverse_roles
from scenario.config import ModelConfig, ScenarioConfig

from ._error_messages import agent_not_configured_error_message
from .types import AgentInput, AgentReturnTypes, AgentRole


logger = logging.getLogger(&#34;scenario&#34;)


class UserSimulatorAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that simulates realistic user behavior in scenario conversations.

    This agent generates user messages that are appropriate for the given scenario
    context, simulating how a real human user would interact with the agent under test.
    It uses an LLM to generate natural, contextually relevant user inputs that help
    drive the conversation forward according to the scenario description.

    Attributes:
        role: Always AgentRole.USER for user simulator agents
        model: LLM model identifier to use for generating user messages
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for response generation
        max_tokens: Maximum tokens to generate in user messages
        system_prompt: Custom system prompt to override default user simulation behavior

    Example:
        ```
        import scenario

        # Basic user simulator with default behavior
        user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1-mini&#34;
        )

        # Customized user simulator
        custom_user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1-mini&#34;,
            temperature=0.3,
            system_prompt=&#34;You are a technical user who asks detailed questions&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;user interaction test&#34;,
            description=&#34;User seeks help with Python programming&#34;,
            agents=[
                my_programming_agent,
                user_sim,
                scenario.JudgeAgent(criteria=[&#34;Provides helpful code examples&#34;])
            ]
        )
        ```

    Note:
        - The user simulator automatically generates short, natural user messages
        - It follows the scenario description to stay on topic
        - Messages are generated in a casual, human-like style (lowercase, brief, etc.)
        - The simulator will not act as an assistant - it only generates user inputs
    &#34;&#34;&#34;
    role = AgentRole.USER

    model: str
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a user simulator agent.

        Args:
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1-mini&#34;).
                   If not provided, uses the default model from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for message generation (0.0-1.0).
                        Lower values make responses more deterministic.
            max_tokens: Maximum number of tokens to generate in user messages.
                       If not provided, uses model defaults.
            system_prompt: Custom system prompt to override default user simulation behavior.
                          Use this to create specialized user personas or behaviors.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Basic user simulator
            user_sim = UserSimulatorAgent(model=&#34;openai/gpt-4.1-mini&#34;)

            # User simulator with custom persona
            expert_user = UserSimulatorAgent(
                model=&#34;openai/gpt-4.1-mini&#34;,
                temperature=0.2,
                system_prompt=&#39;&#39;&#39;
                You are an expert software developer testing an AI coding assistant.
                Ask challenging, technical questions and be demanding about code quality.
                &#39;&#39;&#39;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the user simulator agent
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Generate the next user message in the conversation.

        This method analyzes the current conversation state and scenario context
        to generate an appropriate user message that moves the conversation forward
        in a realistic, human-like manner.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: A user message in OpenAI format that continues the conversation

        Note:
            - Messages are generated in a casual, human-like style
            - The simulator follows the scenario description to stay contextually relevant
            - Uses role reversal internally to work around LLM biases toward assistant roles
            - Results are cached when cache_key is configured for deterministic testing
        &#34;&#34;&#34;

        scenario = input.scenario_state

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are pretending to be a user, you are testing an AI Agent (shown as the user role) based on a scenario.
Approach this naturally, as a human user would, with very short inputs, few words, all lowercase, imperative, not periods, like when they google or talk to chatgpt.
&lt;/role&gt;

&lt;goal&gt;
Your goal (assistant) is to interact with the Agent Under Test (user) as if you were a human user to see if it can complete the scenario successfully.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;rules&gt;
- DO NOT carry over any requests yourself, YOU ARE NOT the assistant today, you are the user
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Hello, how can I help you today?&#34;},
            *input.messages,
        ]

        # User to assistant role reversal
        # LLM models are biased to always be the assistant not the user, so we need to do this reversal otherwise models like GPT 4.5 is
        # super confused, and Claude 3.7 even starts throwing exceptions.
        messages = reverse_roles(messages)

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=[],
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            message_content = message.content
            if message_content is None:
                raise Exception(f&#34;No response from LLM: {response.__repr__()}&#34;)

            return {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message_content}
        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent"><code class="flex name class">
<span>class <span class="ident">UserSimulatorAgent</span></span>
<span>(</span><span>*, model: str | None = None, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, system_prompt: str | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Agent that simulates realistic user behavior in scenario conversations.</p>
<p>This agent generates user messages that are appropriate for the given scenario
context, simulating how a real human user would interact with the agent under test.
It uses an LLM to generate natural, contextually relevant user inputs that help
drive the conversation forward according to the scenario description.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong></dt>
<dd>Always AgentRole.USER for user simulator agents</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier to use for generating user messages</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Optional API key for the model provider</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for response generation</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum tokens to generate in user messages</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default user simulation behavior</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Basic user simulator with default behavior
user_sim = scenario.UserSimulatorAgent(
    model=&quot;openai/gpt-4.1-mini&quot;
)

# Customized user simulator
custom_user_sim = scenario.UserSimulatorAgent(
    model=&quot;openai/gpt-4.1-mini&quot;,
    temperature=0.3,
    system_prompt=&quot;You are a technical user who asks detailed questions&quot;
)

# Use in scenario
result = await scenario.run(
    name=&quot;user interaction test&quot;,
    description=&quot;User seeks help with Python programming&quot;,
    agents=[
        my_programming_agent,
        user_sim,
        scenario.JudgeAgent(criteria=[&quot;Provides helpful code examples&quot;])
    ]
)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>The user simulator automatically generates short, natural user messages</li>
<li>It follows the scenario description to stay on topic</li>
<li>Messages are generated in a casual, human-like style (lowercase, brief, etc.)</li>
<li>The simulator will not act as an assistant - it only generates user inputs</li>
</ul>
<p>Initialize a user simulator agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier (e.g., "openai/gpt-4.1-mini").
If not provided, uses the default model from global configuration.</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>API key for the model provider. If not provided,
uses the key from global configuration or environment.</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for message generation (0.0-1.0).
Lower values make responses more deterministic.</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum number of tokens to generate in user messages.
If not provided, uses model defaults.</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default user simulation behavior.
Use this to create specialized user personas or behaviors.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model is configured either in parameters or global config</dd>
</dl>
<h2 id="example_1">Example</h2>
<pre><code># Basic user simulator
user_sim = UserSimulatorAgent(model=&quot;openai/gpt-4.1-mini&quot;)

# User simulator with custom persona
expert_user = UserSimulatorAgent(
    model=&quot;openai/gpt-4.1-mini&quot;,
    temperature=0.2,
    system_prompt='''
    You are an expert software developer testing an AI coding assistant.
    Ask challenging, technical questions and be demanding about code quality.
    '''
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UserSimulatorAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that simulates realistic user behavior in scenario conversations.

    This agent generates user messages that are appropriate for the given scenario
    context, simulating how a real human user would interact with the agent under test.
    It uses an LLM to generate natural, contextually relevant user inputs that help
    drive the conversation forward according to the scenario description.

    Attributes:
        role: Always AgentRole.USER for user simulator agents
        model: LLM model identifier to use for generating user messages
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for response generation
        max_tokens: Maximum tokens to generate in user messages
        system_prompt: Custom system prompt to override default user simulation behavior

    Example:
        ```
        import scenario

        # Basic user simulator with default behavior
        user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1-mini&#34;
        )

        # Customized user simulator
        custom_user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1-mini&#34;,
            temperature=0.3,
            system_prompt=&#34;You are a technical user who asks detailed questions&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;user interaction test&#34;,
            description=&#34;User seeks help with Python programming&#34;,
            agents=[
                my_programming_agent,
                user_sim,
                scenario.JudgeAgent(criteria=[&#34;Provides helpful code examples&#34;])
            ]
        )
        ```

    Note:
        - The user simulator automatically generates short, natural user messages
        - It follows the scenario description to stay on topic
        - Messages are generated in a casual, human-like style (lowercase, brief, etc.)
        - The simulator will not act as an assistant - it only generates user inputs
    &#34;&#34;&#34;
    role = AgentRole.USER

    model: str
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a user simulator agent.

        Args:
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1-mini&#34;).
                   If not provided, uses the default model from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for message generation (0.0-1.0).
                        Lower values make responses more deterministic.
            max_tokens: Maximum number of tokens to generate in user messages.
                       If not provided, uses model defaults.
            system_prompt: Custom system prompt to override default user simulation behavior.
                          Use this to create specialized user personas or behaviors.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Basic user simulator
            user_sim = UserSimulatorAgent(model=&#34;openai/gpt-4.1-mini&#34;)

            # User simulator with custom persona
            expert_user = UserSimulatorAgent(
                model=&#34;openai/gpt-4.1-mini&#34;,
                temperature=0.2,
                system_prompt=&#39;&#39;&#39;
                You are an expert software developer testing an AI coding assistant.
                Ask challenging, technical questions and be demanding about code quality.
                &#39;&#39;&#39;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the user simulator agent
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Generate the next user message in the conversation.

        This method analyzes the current conversation state and scenario context
        to generate an appropriate user message that moves the conversation forward
        in a realistic, human-like manner.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: A user message in OpenAI format that continues the conversation

        Note:
            - Messages are generated in a casual, human-like style
            - The simulator follows the scenario description to stay contextually relevant
            - Uses role reversal internally to work around LLM biases toward assistant roles
            - Results are cached when cache_key is configured for deterministic testing
        &#34;&#34;&#34;

        scenario = input.scenario_state

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are pretending to be a user, you are testing an AI Agent (shown as the user role) based on a scenario.
Approach this naturally, as a human user would, with very short inputs, few words, all lowercase, imperative, not periods, like when they google or talk to chatgpt.
&lt;/role&gt;

&lt;goal&gt;
Your goal (assistant) is to interact with the Agent Under Test (user) as if you were a human user to see if it can complete the scenario successfully.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;rules&gt;
- DO NOT carry over any requests yourself, YOU ARE NOT the assistant today, you are the user
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Hello, how can I help you today?&#34;},
            *input.messages,
        ]

        # User to assistant role reversal
        # LLM models are biased to always be the assistant not the user, so we need to do this reversal otherwise models like GPT 4.5 is
        # super confused, and Claude 3.7 even starts throwing exceptions.
        messages = reverse_roles(messages)

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=[],
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            message_content = message.content
            if message_content is None:
                raise Exception(f&#34;No response from LLM: {response.__repr__()}&#34;)

            return {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message_content}
        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.api_key"><code class="name">var <span class="ident">api_key</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.max_tokens"><code class="name">var <span class="ident">max_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.system_prompt"><code class="name">var <span class="ident">system_prompt</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.temperature"><code class="name">var <span class="ident">temperature</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.user_simulator_agent.UserSimulatorAgent.call"><code class="name flex">
<span>async def <span class="ident">call</span></span>(<span>self, input: <a title="scenario.types.AgentInput" href="types.html#scenario.types.AgentInput">AgentInput</a>) ‑> str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam] | <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate the next user message in the conversation.</p>
<p>This method analyzes the current conversation state and scenario context
to generate an appropriate user message that moves the conversation forward
in a realistic, human-like manner.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>AgentInput containing conversation history and scenario context</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AgentReturnTypes</code></dt>
<dd>A user message in OpenAI format that continues the conversation</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Messages are generated in a casual, human-like style</li>
<li>The simulator follows the scenario description to stay contextually relevant</li>
<li>Uses role reversal internally to work around LLM biases toward assistant roles</li>
<li>Results are cached when cache_key is configured for deterministic testing</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Generate the next user message in the conversation.

        This method analyzes the current conversation state and scenario context
        to generate an appropriate user message that moves the conversation forward
        in a realistic, human-like manner.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: A user message in OpenAI format that continues the conversation

        Note:
            - Messages are generated in a casual, human-like style
            - The simulator follows the scenario description to stay contextually relevant
            - Uses role reversal internally to work around LLM biases toward assistant roles
            - Results are cached when cache_key is configured for deterministic testing
        &#34;&#34;&#34;

        scenario = input.scenario_state

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are pretending to be a user, you are testing an AI Agent (shown as the user role) based on a scenario.
Approach this naturally, as a human user would, with very short inputs, few words, all lowercase, imperative, not periods, like when they google or talk to chatgpt.
&lt;/role&gt;

&lt;goal&gt;
Your goal (assistant) is to interact with the Agent Under Test (user) as if you were a human user to see if it can complete the scenario successfully.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;rules&gt;
- DO NOT carry over any requests yourself, YOU ARE NOT the assistant today, you are the user
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Hello, how can I help you today?&#34;},
            *input.messages,
        ]

        # User to assistant role reversal
        # LLM models are biased to always be the assistant not the user, so we need to do this reversal otherwise models like GPT 4.5 is
        # super confused, and Claude 3.7 even starts throwing exceptions.
        messages = reverse_roles(messages)

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=[],
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            message_content = message.content
            if message_content is None:
                raise Exception(f&#34;No response from LLM: {response.__repr__()}&#34;)

            return {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message_content}
        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></b></code>:
<ul class="hlist">
<li><code><a title="scenario.agent_adapter.AgentAdapter.role" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter.role">role</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/" style="color: #000">← Back to Docs</a>
<h1>Scenario API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scenario" href="index.html">scenario</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scenario.user_simulator_agent.UserSimulatorAgent" href="#scenario.user_simulator_agent.UserSimulatorAgent">UserSimulatorAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.api_key" href="#scenario.user_simulator_agent.UserSimulatorAgent.api_key">api_key</a></code></li>
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.call" href="#scenario.user_simulator_agent.UserSimulatorAgent.call">call</a></code></li>
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.max_tokens" href="#scenario.user_simulator_agent.UserSimulatorAgent.max_tokens">max_tokens</a></code></li>
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.model" href="#scenario.user_simulator_agent.UserSimulatorAgent.model">model</a></code></li>
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.system_prompt" href="#scenario.user_simulator_agent.UserSimulatorAgent.system_prompt">system_prompt</a></code></li>
<li><code><a title="scenario.user_simulator_agent.UserSimulatorAgent.temperature" href="#scenario.user_simulator_agent.UserSimulatorAgent.temperature">temperature</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
