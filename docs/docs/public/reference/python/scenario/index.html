<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>Scenario API documentation</title>
<meta name="description" content="Scenario: Agent Testing Framework through Simulation Testing …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://scenario.langwatch.ai/favicon.ico" type="image/x-icon" />
</head>
<body>
<style>
.navbar.navbar--fixed-top{
background-color: #fff;
box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.1);
display: flex;
height: 3.75rem;
padding: 0.5rem 1rem;
}
.navbar__inner {
display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;
}
.navbar__items {
align-items: center;
display: flex;
flex: 1;
min-width: 0;
}
.navbar__items--right {
flex: 0 0 auto;
justify-content: flex-end;
}
.navbar__link {
color: #1c1e21;
font-weight: 500;
}
.navbar__link:hover, .navbar__link--active {
color: #2e8555;
text-decoration: none;
}
.navbar__item {
display: inline-block;
padding: 0.25em 0.75em;
}
.navbar a {
text-decoration: none;
transition: color 200ms cubic-bezier(0.08, 0.52, 0.52, 1);
}
.navbar__brand {
align-items: center;
color: #1c1e21;
display: flex;
margin-right: 1rem;
min-width: 0;
}
.iconExternalLink_node_modules-\@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module {
margin-left: 0.3rem;
}
</style>
<nav aria-label="Main" class="navbar navbar--fixed-top">
<div class="navbar__inner" style="display: flex;
flex-wrap: wrap;
justify-content: space-between;
width: 100%;">
<div class="navbar__items">
<a class="navbar__brand" href="/"
><b class="navbar__title text--truncate">Scenario</b></a
><a
aria-current="page"
class="navbar__item navbar__link"
href="/"
>Docs</a
>
<a
aria-current="page"
class="navbar__item navbar__link navbar__link--active"
href="/reference/python/scenario/"
>Reference</a
>
</div>
<div class="navbar__items navbar__items--right">
<a
href="https://github.com/langwatch/scenario"
target="_blank"
rel="noopener noreferrer"
class="navbar__item navbar__link"
style="display: flex; align-items: center"
>GitHub<svg
width="13.5"
height="13.5"
aria-hidden="true"
viewBox="0 0 24 24"
class="iconExternalLink_node_modules-@docusaurus-theme-classic-lib-theme-Icon-ExternalLink-styles-module"
>
<path
fill="currentColor"
d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"
></path></svg
></a>
</div>
</div>
<div role="presentation" class="navbar-sidebar__backdrop"></div>
</nav>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>scenario</code></h1>
</header>
<section id="section-intro">
<p>Scenario: Agent Testing Framework through Simulation Testing</p>
<p>Scenario is a comprehensive testing framework for AI agents that uses simulation testing
to validate agent behavior through realistic conversations. It enables testing of both
happy paths and edge cases by simulating user interactions and evaluating agent responses
against configurable success criteria.</p>
<p>Key Features:</p>
<ul>
<li>
<p>End-to-end conversation testing with specified scenarios</p>
</li>
<li>
<p>Flexible control from fully scripted to completely automated simulations</p>
</li>
<li>
<p>Multi-turn evaluation designed for complex conversational agents</p>
</li>
<li>
<p>Works with any testing framework (pytest, unittest, etc.)</p>
</li>
<li>
<p>Framework-agnostic integration with any LLM or agent architecture</p>
</li>
<li>
<p>Built-in caching for deterministic and faster test execution</p>
</li>
</ul>
<p>Basic Usage:</p>
<pre><code>import scenario

# Configure global settings
scenario.configure(default_model="openai/gpt-4.1")

# Create your agent adapter
class MyAgent(scenario.AgentAdapter):
    async def call(self, input: scenario.AgentInput) -&gt; scenario.AgentReturnTypes:
        return my_agent_function(input.last_new_user_message_str())

# Run a scenario test
result = await scenario.run(
    name="customer service test",
    description="Customer asks about billing, agent should help politely",
    agents=[
        MyAgent(),
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[
            "Agent is polite and professional",
            "Agent addresses the billing question",
            "Agent provides clear next steps"
        ])
    ]
)

assert result.success
</code></pre>
<p>Advanced Usage:</p>
<pre><code># Script-controlled scenario with custom evaluations
def check_tool_usage(state: scenario.ScenarioState) -&gt; None:
    assert state.has_tool_call("get_customer_info")

result = await scenario.run(
    name="scripted interaction",
    description="Test specific conversation flow",
    agents=[
        MyAgent(),
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=["Agent provides helpful response"])
    ],
    script=[
        scenario.user("I have a billing question"),
        scenario.agent(),
        check_tool_usage,  # Custom assertion
        scenario.proceed(turns=2),  # Let it continue automatically
        scenario.succeed("All requirements met")
    ]
)
</code></pre>
<p>Integration with Testing Frameworks:</p>
<pre><code>import pytest

@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_weather_agent():
    result = await scenario.run(
        name="weather query",
        description="User asks about weather in a specific city",
        agents=[
            WeatherAgent(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(criteria=["Provides accurate weather information"])
        ]
    )
    assert result.success
</code></pre>
<p>For more examples and detailed documentation, visit: <a href="https://github.com/langwatch/scenario">https://github.com/langwatch/scenario</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Scenario: Agent Testing Framework through Simulation Testing

Scenario is a comprehensive testing framework for AI agents that uses simulation testing
to validate agent behavior through realistic conversations. It enables testing of both
happy paths and edge cases by simulating user interactions and evaluating agent responses
against configurable success criteria.

Key Features:

- End-to-end conversation testing with specified scenarios

- Flexible control from fully scripted to completely automated simulations

- Multi-turn evaluation designed for complex conversational agents

- Works with any testing framework (pytest, unittest, etc.)

- Framework-agnostic integration with any LLM or agent architecture

- Built-in caching for deterministic and faster test execution

Basic Usage:

    import scenario

    # Configure global settings
    scenario.configure(default_model=&#34;openai/gpt-4.1&#34;)

    # Create your agent adapter
    class MyAgent(scenario.AgentAdapter):
        async def call(self, input: scenario.AgentInput) -&gt; scenario.AgentReturnTypes:
            return my_agent_function(input.last_new_user_message_str())

    # Run a scenario test
    result = await scenario.run(
        name=&#34;customer service test&#34;,
        description=&#34;Customer asks about billing, agent should help politely&#34;,
        agents=[
            MyAgent(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(criteria=[
                &#34;Agent is polite and professional&#34;,
                &#34;Agent addresses the billing question&#34;,
                &#34;Agent provides clear next steps&#34;
            ])
        ]
    )

    assert result.success

Advanced Usage:

    # Script-controlled scenario with custom evaluations
    def check_tool_usage(state: scenario.ScenarioState) -&gt; None:
        assert state.has_tool_call(&#34;get_customer_info&#34;)

    result = await scenario.run(
        name=&#34;scripted interaction&#34;,
        description=&#34;Test specific conversation flow&#34;,
        agents=[
            MyAgent(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(criteria=[&#34;Agent provides helpful response&#34;])
        ],
        script=[
            scenario.user(&#34;I have a billing question&#34;),
            scenario.agent(),
            check_tool_usage,  # Custom assertion
            scenario.proceed(turns=2),  # Let it continue automatically
            scenario.succeed(&#34;All requirements met&#34;)
        ]
    )

Integration with Testing Frameworks:

    import pytest

    @pytest.mark.agent_test
    @pytest.mark.asyncio
    async def test_weather_agent():
        result = await scenario.run(
            name=&#34;weather query&#34;,
            description=&#34;User asks about weather in a specific city&#34;,
            agents=[
                WeatherAgent(),
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Provides accurate weather information&#34;])
            ]
        )
        assert result.success

For more examples and detailed documentation, visit: https://github.com/langwatch/scenario
&#34;&#34;&#34;

# First import non-dependent modules
from .types import ScenarioResult, AgentInput, AgentRole, AgentReturnTypes
from .config import ScenarioConfig

# Then import modules with dependencies
from .scenario_executor import run
from .scenario_state import ScenarioState
from .agent_adapter import AgentAdapter
from .judge_agent import JudgeAgent
from .user_simulator_agent import UserSimulatorAgent
from .cache import scenario_cache
from .script import message, user, agent, judge, proceed, succeed, fail

# Import pytest plugin components
# from .pytest_plugin import pytest_configure, scenario_reporter

configure = ScenarioConfig.configure

default_config = ScenarioConfig.default_config

cache = scenario_cache

__all__ = [
    # Functions
    &#34;run&#34;,
    &#34;configure&#34;,
    &#34;default_config&#34;,
    &#34;cache&#34;,
    # Script
    &#34;message&#34;,
    &#34;proceed&#34;,
    &#34;succeed&#34;,
    &#34;fail&#34;,
    &#34;judge&#34;,
    &#34;agent&#34;,
    &#34;user&#34;,
    # Types
    &#34;ScenarioResult&#34;,
    &#34;AgentInput&#34;,
    &#34;AgentRole&#34;,
    &#34;ScenarioConfig&#34;,
    &#34;AgentReturnTypes&#34;,
    # Classes
    &#34;ScenarioState&#34;,
    &#34;AgentAdapter&#34;,
    &#34;UserSimulatorAgent&#34;,
    &#34;JudgeAgent&#34;,
]
__version__ = &#34;0.1.0&#34;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="scenario.agent_adapter" href="agent_adapter.html">scenario.agent_adapter</a></code></dt>
<dd>
<div class="desc"><p>Agent adapter module for integrating custom agents with the Scenario framework …</p></div>
</dd>
<dt><code class="name"><a title="scenario.config" href="config/index.html">scenario.config</a></code></dt>
<dd>
<div class="desc"><p>Configuration module for Scenario …</p></div>
</dd>
<dt><code class="name"><a title="scenario.judge_agent" href="judge_agent.html">scenario.judge_agent</a></code></dt>
<dd>
<div class="desc"><p>Judge agent module for evaluating scenario conversations …</p></div>
</dd>
<dt><code class="name"><a title="scenario.pytest_plugin" href="pytest_plugin.html">scenario.pytest_plugin</a></code></dt>
<dd>
<div class="desc"><p>Pytest plugin for Scenario testing library …</p></div>
</dd>
<dt><code class="name"><a title="scenario.scenario_executor" href="scenario_executor.html">scenario.scenario_executor</a></code></dt>
<dd>
<div class="desc"><p>Scenario execution engine for agent testing …</p></div>
</dd>
<dt><code class="name"><a title="scenario.scenario_state" href="scenario_state.html">scenario.scenario_state</a></code></dt>
<dd>
<div class="desc"><p>Scenario state management module …</p></div>
</dd>
<dt><code class="name"><a title="scenario.script" href="script.html">scenario.script</a></code></dt>
<dd>
<div class="desc"><p>Scenario script DSL (Domain Specific Language) module …</p></div>
</dd>
<dt><code class="name"><a title="scenario.types" href="types.html">scenario.types</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scenario.user_simulator_agent" href="user_simulator_agent.html">scenario.user_simulator_agent</a></code></dt>
<dd>
<div class="desc"><p>User simulator agent module for generating realistic user interactions …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="scenario.agent"><code class="name flex">
<span>def <span class="ident">agent</span></span>(<span>content: str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | None = None) ‑> Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate or specify an agent response in the conversation.</p>
<p>If content is provided, it will be used as the agent response. If no content
is provided, the agent under test will be called to generate its response
based on the current conversation state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Optional agent response content. Can be a string or full message dict.
If None, the agent under test will generate content automatically.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>result = await scenario.run(
    name=&quot;agent response test&quot;,
    description=&quot;Testing agent responses&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent provides appropriate responses&quot;])
    ],
    script=[
        scenario.user(&quot;Hello&quot;),

        # Let agent generate its own response
        scenario.agent(),

        # Or specify exact agent response for testing edge cases
        scenario.agent(&quot;I'm sorry, I'm currently unavailable&quot;),
        scenario.user(),  # See how user simulator reacts

        # Structured agent response with tool calls
        scenario.message({
            &quot;role&quot;: &quot;assistant&quot;,
            &quot;content&quot;: &quot;Let me search for that information&quot;,
            &quot;tool_calls&quot;: [{&quot;id&quot;: &quot;call_123&quot;, &quot;type&quot;: &quot;function&quot;, ...}]
        }),
        scenario.succeed()
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def agent(
    content: Optional[Union[str, ChatCompletionMessageParam]] = None,
) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Generate or specify an agent response in the conversation.

    If content is provided, it will be used as the agent response. If no content
    is provided, the agent under test will be called to generate its response
    based on the current conversation state.

    Args:
        content: Optional agent response content. Can be a string or full message dict.
                If None, the agent under test will generate content automatically.

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        result = await scenario.run(
            name=&#34;agent response test&#34;,
            description=&#34;Testing agent responses&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent provides appropriate responses&#34;])
            ],
            script=[
                scenario.user(&#34;Hello&#34;),

                # Let agent generate its own response
                scenario.agent(),

                # Or specify exact agent response for testing edge cases
                scenario.agent(&#34;I&#39;m sorry, I&#39;m currently unavailable&#34;),
                scenario.user(),  # See how user simulator reacts

                # Structured agent response with tool calls
                scenario.message({
                    &#34;role&#34;: &#34;assistant&#34;,
                    &#34;content&#34;: &#34;Let me search for that information&#34;,
                    &#34;tool_calls&#34;: [{&#34;id&#34;: &#34;call_123&#34;, &#34;type&#34;: &#34;function&#34;, ...}]
                }),
                scenario.succeed()
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.agent(content)</code></pre>
</details>
</dd>
<dt id="scenario.cache"><code class="name flex">
<span>def <span class="ident">cache</span></span>(<span>ignore=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator for caching function calls during scenario execution.</p>
<p>This decorator caches function calls based on the scenario's cache_key,
scenario configuration, and function arguments. It enables deterministic
testing by ensuring the same inputs always produce the same outputs,
making tests repeatable and faster on subsequent runs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ignore</code></strong></dt>
<dd>List of argument names to exclude from the cache key computation.
Commonly used to ignore 'self' for instance methods or other
non-deterministic arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Decorator function that can be applied to any function or method</p>
<h2 id="example">Example</h2>
<pre><code>import scenario

class MyAgent:
    @scenario.cache(ignore=[&quot;self&quot;])
    def invoke(self, message: str, context: dict) -&gt; str:
        # This LLM call will be cached
        response = llm_client.complete(
            model=&quot;gpt-4&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
        )
        return response.choices[0].message.content

# Usage in tests
scenario.configure(cache_key=&quot;my-test-suite-v1&quot;)

# First run: makes actual LLM calls and caches results
result1 = await scenario.run(...)

# Second run: uses cached results, much faster
result2 = await scenario.run(...)
# result1 and result2 will be identical
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Caching only occurs when a cache_key is set in the scenario configuration</li>
<li>The cache key is computed from scenario config, function arguments, and cache_key</li>
<li>AgentInput objects are specially handled to exclude thread_id from caching</li>
<li>Both sync and async functions are supported</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scenario_cache(ignore=[]):
    &#34;&#34;&#34;
    Decorator for caching function calls during scenario execution.

    This decorator caches function calls based on the scenario&#39;s cache_key,
    scenario configuration, and function arguments. It enables deterministic
    testing by ensuring the same inputs always produce the same outputs,
    making tests repeatable and faster on subsequent runs.

    Args:
        ignore: List of argument names to exclude from the cache key computation.
                Commonly used to ignore &#39;self&#39; for instance methods or other
                non-deterministic arguments.

    Returns:
        Decorator function that can be applied to any function or method

    Example:
        ```
        import scenario

        class MyAgent:
            @scenario.cache(ignore=[&#34;self&#34;])
            def invoke(self, message: str, context: dict) -&gt; str:
                # This LLM call will be cached
                response = llm_client.complete(
                    model=&#34;gpt-4&#34;,
                    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message}]
                )
                return response.choices[0].message.content

        # Usage in tests
        scenario.configure(cache_key=&#34;my-test-suite-v1&#34;)

        # First run: makes actual LLM calls and caches results
        result1 = await scenario.run(...)

        # Second run: uses cached results, much faster
        result2 = await scenario.run(...)
        # result1 and result2 will be identical
        ```

    Note:
        - Caching only occurs when a cache_key is set in the scenario configuration
        - The cache key is computed from scenario config, function arguments, and cache_key
        - AgentInput objects are specially handled to exclude thread_id from caching
        - Both sync and async functions are supported
    &#34;&#34;&#34;

    @wrapt.decorator
    def wrapper(wrapped: Callable, instance=None, args=[], kwargs={}):
        scenario: &#34;ScenarioExecutor&#34; = context_scenario.get()

        if not scenario.config.cache_key:
            return wrapped(*args, **kwargs)

        sig = inspect.signature(wrapped)
        parameters = list(sig.parameters.values())

        all_args = {
            str(parameter.name): value for parameter, value in zip(parameters, args)
        }
        for arg in [&#34;self&#34;] + ignore:
            if arg in all_args:
                del all_args[arg]

        for key, value in all_args.items():
            if isinstance(value, AgentInput):
                scenario_state = value.scenario_state.model_dump(exclude={&#34;thread_id&#34;})
                all_args[key] = value.model_dump(exclude={&#34;thread_id&#34;})
                all_args[key][&#34;scenario_state&#34;] = scenario_state

        cache_key = json.dumps(
            {
                &#34;cache_key&#34;: scenario.config.cache_key,
                &#34;scenario&#34;: scenario.config.model_dump(exclude={&#34;agents&#34;}),
                &#34;all_args&#34;: all_args,
            },
            cls=SerializableWithStringFallback,
        )

        # if is an async function, we need to wrap it in a sync function
        if inspect.iscoroutinefunction(wrapped):
            return _async_cached_call(wrapped, args, kwargs, cache_key=cache_key)
        else:
            return _cached_call(wrapped, args, kwargs, cache_key=cache_key)

    return wrapper</code></pre>
</details>
</dd>
<dt id="scenario.configure"><code class="name flex">
<span>def <span class="ident">configure</span></span>(<span>default_model: str | None = None, max_turns: int | None = None, verbose: bool | int | None = None, cache_key: str | None = None, debug: bool | None = None, headless: bool | None = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set global configuration settings for all scenario executions.</p>
<p>This method allows you to configure default behavior that will be applied
to all scenarios unless explicitly overridden in individual scenario runs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>default_model</code></strong></dt>
<dd>Default LLM model identifier for user simulator and judge agents</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum number of conversation turns before timeout (default: 10)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Enable verbose output during scenario execution</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Cache key for deterministic scenario behavior across runs</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Enable debug mode for step-by-step execution with user intervention</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Set up default configuration
scenario.configure(
    default_model=&quot;openai/gpt-4.1-mini&quot;,
    max_turns=15,
    verbose=True,
    debug=False
)

# All subsequent scenario runs will use these defaults
result = await scenario.run(
    name=&quot;my test&quot;,
    description=&quot;Test scenario&quot;,
    agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
)
</code></pre></div>
</dd>
<dt id="scenario.fail"><code class="name flex">
<span>def <span class="ident">fail</span></span>(<span>reasoning: str | None = None) ‑> Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Immediately end the scenario with a failure result.</p>
<p>This function terminates the scenario execution and marks it as failed,
bypassing any further agent interactions or judge evaluations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>reasoning</code></strong></dt>
<dd>Optional explanation for why the scenario failed</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>def safety_check(state: ScenarioState) -&gt; None:
    last_msg = state.last_message()
    content = last_msg.get(&quot;content&quot;, &quot;&quot;)

    if &quot;harmful&quot; in content.lower():
        return scenario.fail(&quot;Agent produced harmful content&quot;)()

result = await scenario.run(
    name=&quot;safety check test&quot;,
    description=&quot;Test safety boundaries&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent maintains safety guidelines&quot;])
    ],
    script=[
        scenario.user(&quot;Tell me something dangerous&quot;),
        scenario.agent(),
        safety_check,

        # Or explicit failure
        scenario.fail(&quot;Agent failed to meet safety requirements&quot;)
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fail(reasoning: Optional[str] = None) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Immediately end the scenario with a failure result.

    This function terminates the scenario execution and marks it as failed,
    bypassing any further agent interactions or judge evaluations.

    Args:
        reasoning: Optional explanation for why the scenario failed

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        def safety_check(state: ScenarioState) -&gt; None:
            last_msg = state.last_message()
            content = last_msg.get(&#34;content&#34;, &#34;&#34;)

            if &#34;harmful&#34; in content.lower():
                return scenario.fail(&#34;Agent produced harmful content&#34;)()

        result = await scenario.run(
            name=&#34;safety check test&#34;,
            description=&#34;Test safety boundaries&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent maintains safety guidelines&#34;])
            ],
            script=[
                scenario.user(&#34;Tell me something dangerous&#34;),
                scenario.agent(),
                safety_check,

                # Or explicit failure
                scenario.fail(&#34;Agent failed to meet safety requirements&#34;)
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.fail(reasoning)</code></pre>
</details>
</dd>
<dt id="scenario.judge"><code class="name flex">
<span>def <span class="ident">judge</span></span>(<span>content: str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | None = None) ‑> Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Invoke the judge agent to evaluate the current conversation state.</p>
<p>This function forces the judge agent to make a decision about whether
the scenario should continue or end with a success/failure verdict.
The judge will evaluate based on its configured criteria.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Optional message content for the judge. Usually None to let
the judge evaluate based on its criteria.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>result = await scenario.run(
    name=&quot;judge evaluation test&quot;,
    description=&quot;Testing judge at specific points&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent provides coding help effectively&quot;])
    ],
    script=[
        scenario.user(&quot;Can you help me code?&quot;),
        scenario.agent(),

        # Force judge evaluation after first exchange
        scenario.judge(),  # May continue or end scenario

        # If scenario continues...
        scenario.user(),
        scenario.agent(),
        scenario.judge(),  # Final evaluation
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def judge(
    content: Optional[Union[str, ChatCompletionMessageParam]] = None,
) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Invoke the judge agent to evaluate the current conversation state.

    This function forces the judge agent to make a decision about whether
    the scenario should continue or end with a success/failure verdict.
    The judge will evaluate based on its configured criteria.

    Args:
        content: Optional message content for the judge. Usually None to let
                the judge evaluate based on its criteria.

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        result = await scenario.run(
            name=&#34;judge evaluation test&#34;,
            description=&#34;Testing judge at specific points&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent provides coding help effectively&#34;])
            ],
            script=[
                scenario.user(&#34;Can you help me code?&#34;),
                scenario.agent(),

                # Force judge evaluation after first exchange
                scenario.judge(),  # May continue or end scenario

                # If scenario continues...
                scenario.user(),
                scenario.agent(),
                scenario.judge(),  # Final evaluation
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.judge(content)</code></pre>
</details>
</dd>
<dt id="scenario.message"><code class="name flex">
<span>def <span class="ident">message</span></span>(<span>message: openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam) ‑> Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Add a specific message to the conversation.</p>
<p>This function allows you to inject any OpenAI-compatible message directly
into the conversation at a specific point in the script. Useful for
simulating tool responses, system messages, or specific conversational states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message</code></strong></dt>
<dd>OpenAI-compatible message to add to the conversation</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>result = await scenario.run(
    name=&quot;tool response test&quot;,
    description=&quot;Testing tool call responses&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent uses weather tool correctly&quot;])
    ],
    script=[
        scenario.user(&quot;What's the weather?&quot;),
        scenario.agent(),  # Agent calls weather tool
        scenario.message({
            &quot;role&quot;: &quot;tool&quot;,
            &quot;tool_call_id&quot;: &quot;call_123&quot;,
            &quot;content&quot;: json.dumps({&quot;temperature&quot;: &quot;75°F&quot;, &quot;condition&quot;: &quot;sunny&quot;})
        }),
        scenario.agent(),  # Agent processes tool response
        scenario.succeed()
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def message(message: ChatCompletionMessageParam) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Add a specific message to the conversation.

    This function allows you to inject any OpenAI-compatible message directly
    into the conversation at a specific point in the script. Useful for
    simulating tool responses, system messages, or specific conversational states.

    Args:
        message: OpenAI-compatible message to add to the conversation

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        result = await scenario.run(
            name=&#34;tool response test&#34;,
            description=&#34;Testing tool call responses&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent uses weather tool correctly&#34;])
            ],
            script=[
                scenario.user(&#34;What&#39;s the weather?&#34;),
                scenario.agent(),  # Agent calls weather tool
                scenario.message({
                    &#34;role&#34;: &#34;tool&#34;,
                    &#34;tool_call_id&#34;: &#34;call_123&#34;,
                    &#34;content&#34;: json.dumps({&#34;temperature&#34;: &#34;75°F&#34;, &#34;condition&#34;: &#34;sunny&#34;})
                }),
                scenario.agent(),  # Agent processes tool response
                scenario.succeed()
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.message(message)</code></pre>
</details>
</dd>
<dt id="scenario.proceed"><code class="name flex">
<span>def <span class="ident">proceed</span></span>(<span>turns: int | None = None, on_turn: Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], None] | Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], Awaitable[None]] | None = None, on_step: Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], None] | Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], Awaitable[None]] | None = None) ‑> Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Let the scenario proceed automatically for a specified number of turns.</p>
<p>This function allows the scenario to run automatically with the normal
agent interaction flow (user -&gt; agent -&gt; judge evaluation). You can
optionally provide callbacks to execute custom logic at each turn or step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>turns</code></strong></dt>
<dd>Number of turns to proceed automatically. If None, proceeds until
the judge agent decides to end the scenario or max_turns is reached.</dd>
<dt><strong><code>on_turn</code></strong></dt>
<dd>Optional callback function called at the end of each turn</dd>
<dt><strong><code>on_step</code></strong></dt>
<dd>Optional callback function called after each agent interaction</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>def log_progress(state: ScenarioState) -&gt; None:
    print(f&quot;Turn {state.current_turn}: {len(state.messages)} messages&quot;)

def check_tool_usage(state: ScenarioState) -&gt; None:
    if state.has_tool_call(&quot;dangerous_action&quot;):
        raise AssertionError(&quot;Agent used forbidden tool!&quot;)

result = await scenario.run(
    name=&quot;automatic proceeding test&quot;,
    description=&quot;Let scenario run with monitoring&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent behaves safely and helpfully&quot;])
    ],
    script=[
        scenario.user(&quot;Let's start&quot;),
        scenario.agent(),

        # Let it proceed for 3 turns with monitoring
        scenario.proceed(
            turns=3,
            on_turn=log_progress,
            on_step=check_tool_usage
        ),

        # Then do final evaluation
        scenario.judge()
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def proceed(
    turns: Optional[int] = None,
    on_turn: Optional[
        Union[
            Callable[[&#34;ScenarioState&#34;], None],
            Callable[[&#34;ScenarioState&#34;], Awaitable[None]],
        ]
    ] = None,
    on_step: Optional[
        Union[
            Callable[[&#34;ScenarioState&#34;], None],
            Callable[[&#34;ScenarioState&#34;], Awaitable[None]],
        ]
    ] = None,
) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Let the scenario proceed automatically for a specified number of turns.

    This function allows the scenario to run automatically with the normal
    agent interaction flow (user -&gt; agent -&gt; judge evaluation). You can
    optionally provide callbacks to execute custom logic at each turn or step.

    Args:
        turns: Number of turns to proceed automatically. If None, proceeds until
               the judge agent decides to end the scenario or max_turns is reached.
        on_turn: Optional callback function called at the end of each turn
        on_step: Optional callback function called after each agent interaction

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        def log_progress(state: ScenarioState) -&gt; None:
            print(f&#34;Turn {state.current_turn}: {len(state.messages)} messages&#34;)

        def check_tool_usage(state: ScenarioState) -&gt; None:
            if state.has_tool_call(&#34;dangerous_action&#34;):
                raise AssertionError(&#34;Agent used forbidden tool!&#34;)

        result = await scenario.run(
            name=&#34;automatic proceeding test&#34;,
            description=&#34;Let scenario run with monitoring&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent behaves safely and helpfully&#34;])
            ],
            script=[
                scenario.user(&#34;Let&#39;s start&#34;),
                scenario.agent(),

                # Let it proceed for 3 turns with monitoring
                scenario.proceed(
                    turns=3,
                    on_turn=log_progress,
                    on_step=check_tool_usage
                ),

                # Then do final evaluation
                scenario.judge()
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.proceed(turns, on_turn, on_step)</code></pre>
</details>
</dd>
<dt id="scenario.run"><code class="name flex">
<span>async def <span class="ident">run</span></span>(<span>name: str, description: str, agents: List[<a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a>] = [], max_turns: int | None = None, verbose: bool | int | None = None, cache_key: str | None = None, debug: bool | None = None, script: List[Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], None] | Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], Awaitable[None]] | Callable[[ForwardRef('<a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a>')], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]] | None = None, set_id: str | None = None) ‑> <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>High-level interface for running a scenario test.</p>
<p>This is the main entry point for executing scenario tests. It creates a
ScenarioExecutor instance and runs it in an isolated thread pool to support
parallel execution and prevent blocking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Human-readable name for the scenario</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Detailed description of what the scenario tests</dd>
<dt><strong><code>agents</code></strong></dt>
<dd>List of agent adapters (agent under test, user simulator, judge)</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum conversation turns before timeout (default: 10)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Show detailed output during execution</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Cache key for deterministic behavior</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Enable debug mode for step-by-step execution</dd>
<dt><strong><code>script</code></strong></dt>
<dd>Optional script steps to control scenario flow</dd>
<dt><strong><code>set_id</code></strong></dt>
<dd>Optional set identifier for grouping related scenarios</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScenarioResult containing the test outcome, conversation history,
success/failure status, and detailed reasoning</p>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Simple scenario with automatic flow
result = await scenario.run(
   name=&quot;help request&quot;,
   description=&quot;User asks for help with a technical problem&quot;,
   agents=[
       my_agent,
       scenario.UserSimulatorAgent(),
       scenario.JudgeAgent(criteria=[&quot;Agent provides helpful response&quot;])
   ],
   set_id=&quot;customer-support-tests&quot;
)

# Scripted scenario with custom evaluations
result = await scenario.run(
   name=&quot;custom interaction&quot;,
   description=&quot;Test specific conversation flow&quot;,
   agents=[
       my_agent,
       scenario.UserSimulatorAgent(),
       scenario.JudgeAgent(criteria=[&quot;Agent provides helpful response&quot;])
   ],
   script=[
       scenario.user(&quot;Hello&quot;),
       scenario.agent(),
       custom_eval,
       scenario.succeed()
   ],
   set_id=&quot;integration-tests&quot;
)

# Results analysis
print(f&quot;Test {'PASSED' if result.success else 'FAILED'}&quot;)
print(f&quot;Reasoning: {result.reasoning}&quot;)
print(f&quot;Conversation had {len(result.messages)} messages&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def run(
    name: str,
    description: str,
    agents: List[AgentAdapter] = [],
    max_turns: Optional[int] = None,
    verbose: Optional[Union[bool, int]] = None,
    cache_key: Optional[str] = None,
    debug: Optional[bool] = None,
    script: Optional[List[ScriptStep]] = None,
    set_id: Optional[str] = None,
) -&gt; ScenarioResult:
    &#34;&#34;&#34;
    High-level interface for running a scenario test.

    This is the main entry point for executing scenario tests. It creates a
    ScenarioExecutor instance and runs it in an isolated thread pool to support
    parallel execution and prevent blocking.

    Args:
        name: Human-readable name for the scenario
        description: Detailed description of what the scenario tests
        agents: List of agent adapters (agent under test, user simulator, judge)
        max_turns: Maximum conversation turns before timeout (default: 10)
        verbose: Show detailed output during execution
        cache_key: Cache key for deterministic behavior
        debug: Enable debug mode for step-by-step execution
        script: Optional script steps to control scenario flow
        set_id: Optional set identifier for grouping related scenarios

    Returns:
        ScenarioResult containing the test outcome, conversation history,
        success/failure status, and detailed reasoning

    Example:
        ```
        import scenario

        # Simple scenario with automatic flow
        result = await scenario.run(
           name=&#34;help request&#34;,
           description=&#34;User asks for help with a technical problem&#34;,
           agents=[
               my_agent,
               scenario.UserSimulatorAgent(),
               scenario.JudgeAgent(criteria=[&#34;Agent provides helpful response&#34;])
           ],
           set_id=&#34;customer-support-tests&#34;
        )

        # Scripted scenario with custom evaluations
        result = await scenario.run(
           name=&#34;custom interaction&#34;,
           description=&#34;Test specific conversation flow&#34;,
           agents=[
               my_agent,
               scenario.UserSimulatorAgent(),
               scenario.JudgeAgent(criteria=[&#34;Agent provides helpful response&#34;])
           ],
           script=[
               scenario.user(&#34;Hello&#34;),
               scenario.agent(),
               custom_eval,
               scenario.succeed()
           ],
           set_id=&#34;integration-tests&#34;
        )

        # Results analysis
        print(f&#34;Test {&#39;PASSED&#39; if result.success else &#39;FAILED&#39;}&#34;)
        print(f&#34;Reasoning: {result.reasoning}&#34;)
        print(f&#34;Conversation had {len(result.messages)} messages&#34;)
        ```
    &#34;&#34;&#34;
    scenario = ScenarioExecutor(
        name=name,
        description=description,
        agents=agents,
        max_turns=max_turns,
        verbose=verbose,
        cache_key=cache_key,
        debug=debug,
        script=script,
        set_id=set_id,
    )

    # We&#39;ll use a thread pool to run the execution logic, we
    # require a separate thread because even though asyncio is
    # being used throughout, any user code on the callback can
    # be blocking, preventing them from running scenarios in parallel
    with concurrent.futures.ThreadPoolExecutor() as executor:

        def run_in_thread():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                return loop.run_until_complete(scenario.run())
            finally:
                scenario.event_bus.drain()
                loop.close()

        # Run the function in the thread pool and await its result
        # This converts the thread&#39;s execution into a Future that the current
        # event loop can await without blocking
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(executor, run_in_thread)
        return result</code></pre>
</details>
</dd>
<dt id="scenario.succeed"><code class="name flex">
<span>def <span class="ident">succeed</span></span>(<span>reasoning: str | None = None) ‑> Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Immediately end the scenario with a success result.</p>
<p>This function terminates the scenario execution and marks it as successful,
bypassing any further agent interactions or judge evaluations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>reasoning</code></strong></dt>
<dd>Optional explanation for why the scenario succeeded</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>def custom_success_check(state: ScenarioState) -&gt; None:
    last_msg = state.last_message()
    if &quot;solution&quot; in last_msg.get(&quot;content&quot;, &quot;&quot;).lower():
        # Custom success condition met
        return scenario.succeed(&quot;Agent provided a solution&quot;)()

result = await scenario.run(
    name=&quot;custom success test&quot;,
    description=&quot;Test custom success conditions&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent provides a solution&quot;])
    ],
    script=[
        scenario.user(&quot;I need a solution&quot;),
        scenario.agent(),
        custom_success_check,

        # Or explicit success
        scenario.succeed(&quot;Agent completed the task successfully&quot;)
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def succeed(reasoning: Optional[str] = None) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Immediately end the scenario with a success result.

    This function terminates the scenario execution and marks it as successful,
    bypassing any further agent interactions or judge evaluations.

    Args:
        reasoning: Optional explanation for why the scenario succeeded

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        def custom_success_check(state: ScenarioState) -&gt; None:
            last_msg = state.last_message()
            if &#34;solution&#34; in last_msg.get(&#34;content&#34;, &#34;&#34;).lower():
                # Custom success condition met
                return scenario.succeed(&#34;Agent provided a solution&#34;)()

        result = await scenario.run(
            name=&#34;custom success test&#34;,
            description=&#34;Test custom success conditions&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent provides a solution&#34;])
            ],
            script=[
                scenario.user(&#34;I need a solution&#34;),
                scenario.agent(),
                custom_success_check,

                # Or explicit success
                scenario.succeed(&#34;Agent completed the task successfully&#34;)
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.succeed(reasoning)</code></pre>
</details>
</dd>
<dt id="scenario.user"><code class="name flex">
<span>def <span class="ident">user</span></span>(<span>content: str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | None = None) ‑> Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], None] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], Awaitable[None]] | Callable[[<a title="scenario.scenario_state.ScenarioState" href="scenario_state.html#scenario.scenario_state.ScenarioState">ScenarioState</a>], Awaitable[<a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a> | None]]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate or specify a user message in the conversation.</p>
<p>If content is provided, it will be used as the user message. If no content
is provided, the user simulator agent will automatically generate an
appropriate message based on the scenario context.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Optional user message content. Can be a string or full message dict.
If None, the user simulator will generate content automatically.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ScriptStep function that can be used in scenario scripts</p>
<h2 id="example">Example</h2>
<pre><code>result = await scenario.run(
    name=&quot;user interaction test&quot;,
    description=&quot;Testing specific user inputs&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent responds helpfully to user&quot;])
    ],
    script=[
        # Specific user message
        scenario.user(&quot;I need help with Python&quot;),
        scenario.agent(),

        # Auto-generated user message based on scenario context
        scenario.user(),
        scenario.agent(),

        # Structured user message with multimodal content
        scenario.message({
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What's in this image?&quot;},
                {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;data:image/...&quot;}}
            ]
        }),
        scenario.succeed()
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def user(
    content: Optional[Union[str, ChatCompletionMessageParam]] = None,
) -&gt; ScriptStep:
    &#34;&#34;&#34;
    Generate or specify a user message in the conversation.

    If content is provided, it will be used as the user message. If no content
    is provided, the user simulator agent will automatically generate an
    appropriate message based on the scenario context.

    Args:
        content: Optional user message content. Can be a string or full message dict.
                If None, the user simulator will generate content automatically.

    Returns:
        ScriptStep function that can be used in scenario scripts

    Example:
        ```
        result = await scenario.run(
            name=&#34;user interaction test&#34;,
            description=&#34;Testing specific user inputs&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent responds helpfully to user&#34;])
            ],
            script=[
                # Specific user message
                scenario.user(&#34;I need help with Python&#34;),
                scenario.agent(),

                # Auto-generated user message based on scenario context
                scenario.user(),
                scenario.agent(),

                # Structured user message with multimodal content
                scenario.message({
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: [
                        {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What&#39;s in this image?&#34;},
                        {&#34;type&#34;: &#34;image_url&#34;, &#34;image_url&#34;: {&#34;url&#34;: &#34;data:image/...&#34;}}
                    ]
                }),
                scenario.succeed()
            ]
        )
        ```
    &#34;&#34;&#34;
    return lambda state: state._executor.user(content)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scenario.AgentAdapter"><code class="flex name class">
<span>class <span class="ident">AgentAdapter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for integrating custom agents with the Scenario framework.</p>
<p>This adapter pattern allows you to wrap any existing agent implementation
(LLM calls, agent frameworks, or complex multi-step systems) to work with
the Scenario testing framework. The adapter receives structured input about
the conversation state and returns responses in a standardized format.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong></dt>
<dd>The role this agent plays in scenarios (USER, AGENT, or JUDGE)</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario
from my_agent import MyCustomAgent

class MyAgentAdapter(scenario.AgentAdapter):
    def __init__(self):
        self.agent = MyCustomAgent()

    async def call(self, input: scenario.AgentInput) -&gt; scenario.AgentReturnTypes:
        # Get the latest user message
        user_message = input.last_new_user_message_str()

        # Call your existing agent
        response = await self.agent.process(
            message=user_message,
            history=input.messages,
            thread_id=input.thread_id
        )

        # Return the response (can be string, message dict, or list of messages)
        return response

# Use in a scenario
result = await scenario.run(
    name=&quot;test my agent&quot;,
    description=&quot;User asks for help with a coding problem&quot;,
    agents=[
        MyAgentAdapter(),
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Provides helpful coding advice&quot;])
    ]
)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>The call method must be async</li>
<li>Return types can be: str, ChatCompletionMessageParam, List[ChatCompletionMessageParam], or ScenarioResult</li>
<li>For stateful agents, use input.thread_id to maintain conversation context</li>
<li>For stateless agents, use input.messages for the full conversation history</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AgentAdapter(ABC):
    &#34;&#34;&#34;
    Abstract base class for integrating custom agents with the Scenario framework.

    This adapter pattern allows you to wrap any existing agent implementation
    (LLM calls, agent frameworks, or complex multi-step systems) to work with
    the Scenario testing framework. The adapter receives structured input about
    the conversation state and returns responses in a standardized format.

    Attributes:
        role: The role this agent plays in scenarios (USER, AGENT, or JUDGE)

    Example:
        ```
        import scenario
        from my_agent import MyCustomAgent

        class MyAgentAdapter(scenario.AgentAdapter):
            def __init__(self):
                self.agent = MyCustomAgent()

            async def call(self, input: scenario.AgentInput) -&gt; scenario.AgentReturnTypes:
                # Get the latest user message
                user_message = input.last_new_user_message_str()

                # Call your existing agent
                response = await self.agent.process(
                    message=user_message,
                    history=input.messages,
                    thread_id=input.thread_id
                )

                # Return the response (can be string, message dict, or list of messages)
                return response

        # Use in a scenario
        result = await scenario.run(
            name=&#34;test my agent&#34;,
            description=&#34;User asks for help with a coding problem&#34;,
            agents=[
                MyAgentAdapter(),
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Provides helpful coding advice&#34;])
            ]
        )
        ```

    Note:
        - The call method must be async
        - Return types can be: str, ChatCompletionMessageParam, List[ChatCompletionMessageParam], or ScenarioResult
        - For stateful agents, use input.thread_id to maintain conversation context
        - For stateless agents, use input.messages for the full conversation history
    &#34;&#34;&#34;

    role: ClassVar[AgentRole] = AgentRole.AGENT

    @abstractmethod
    async def call(self, input: AgentInput) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Process the input and generate a response.

        This is the main method that your agent implementation must provide.
        It receives structured information about the current conversation state
        and must return a response in one of the supported formats.

        Args:
            input: AgentInput containing conversation history, thread context, and scenario state

        Returns:
            AgentReturnTypes: The agent&#39;s response, which can be:

                - str: Simple text response

                - ChatCompletionMessageParam: Single OpenAI-format message

                - List[ChatCompletionMessageParam]: Multiple messages for complex responses

                - ScenarioResult: Direct test result (typically only used by judge agents)

        Example:
            ```
            async def call(self, input: AgentInput) -&gt; AgentReturnTypes:
                # Simple string response
                user_msg = input.last_new_user_message_str()
                return f&#34;I understand you said: {user_msg}&#34;

                # Or structured message response
                return {
                    &#34;role&#34;: &#34;assistant&#34;,
                    &#34;content&#34;: &#34;Let me help you with that...&#34;,
                }

                # Or multiple messages for complex interactions
                return [
                    {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Let me search for that information...&#34;},
                    {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Here&#39;s what I found: ...&#34;}
                ]
            ```
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="scenario.judge_agent.JudgeAgent" href="judge_agent.html#scenario.judge_agent.JudgeAgent">JudgeAgent</a></li>
<li><a title="scenario.user_simulator_agent.UserSimulatorAgent" href="user_simulator_agent.html#scenario.user_simulator_agent.UserSimulatorAgent">UserSimulatorAgent</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.AgentAdapter.role"><code class="name">var <span class="ident">role</span> : ClassVar[<a title="scenario.types.AgentRole" href="types.html#scenario.types.AgentRole">AgentRole</a>]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.AgentAdapter.call"><code class="name flex">
<span>async def <span class="ident">call</span></span>(<span>self, input: <a title="scenario.types.AgentInput" href="types.html#scenario.types.AgentInput">AgentInput</a>) ‑> str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam] | <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>Process the input and generate a response.</p>
<p>This is the main method that your agent implementation must provide.
It receives structured information about the current conversation state
and must return a response in one of the supported formats.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>AgentInput containing conversation history, thread context, and scenario state</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AgentReturnTypes</code></dt>
<dd>
<p>The agent's response, which can be:</p>
<ul>
<li>
<p>str: Simple text response</p>
</li>
<li>
<p>ChatCompletionMessageParam: Single OpenAI-format message</p>
</li>
<li>
<p>List[ChatCompletionMessageParam]: Multiple messages for complex responses</p>
</li>
<li>
<p>ScenarioResult: Direct test result (typically only used by judge agents)</p>
</li>
</ul>
</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>async def call(self, input: AgentInput) -&gt; AgentReturnTypes:
    # Simple string response
    user_msg = input.last_new_user_message_str()
    return f&quot;I understand you said: {user_msg}&quot;

    # Or structured message response
    return {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Let me help you with that...&quot;,
    }

    # Or multiple messages for complex interactions
    return [
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Let me search for that information...&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Here's what I found: ...&quot;}
    ]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
async def call(self, input: AgentInput) -&gt; AgentReturnTypes:
    &#34;&#34;&#34;
    Process the input and generate a response.

    This is the main method that your agent implementation must provide.
    It receives structured information about the current conversation state
    and must return a response in one of the supported formats.

    Args:
        input: AgentInput containing conversation history, thread context, and scenario state

    Returns:
        AgentReturnTypes: The agent&#39;s response, which can be:

            - str: Simple text response

            - ChatCompletionMessageParam: Single OpenAI-format message

            - List[ChatCompletionMessageParam]: Multiple messages for complex responses

            - ScenarioResult: Direct test result (typically only used by judge agents)

    Example:
        ```
        async def call(self, input: AgentInput) -&gt; AgentReturnTypes:
            # Simple string response
            user_msg = input.last_new_user_message_str()
            return f&#34;I understand you said: {user_msg}&#34;

            # Or structured message response
            return {
                &#34;role&#34;: &#34;assistant&#34;,
                &#34;content&#34;: &#34;Let me help you with that...&#34;,
            }

            # Or multiple messages for complex interactions
            return [
                {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Let me search for that information...&#34;},
                {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Here&#39;s what I found: ...&#34;}
            ]
        ```
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scenario.AgentInput"><code class="flex name class">
<span>class <span class="ident">AgentInput</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Input data structure passed to agent adapters during scenario execution.</p>
<p>This class encapsulates all the information an agent needs to generate its next response,
including conversation history, thread context, and scenario state. It provides convenient
methods to access the most recent user messages.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>thread_id</code></strong></dt>
<dd>Unique identifier for the conversation thread</dd>
<dt><strong><code>messages</code></strong></dt>
<dd>Complete conversation history as OpenAI-compatible messages</dd>
<dt><strong><code>new_messages</code></strong></dt>
<dd>Only the new messages since the agent's last call</dd>
<dt><strong><code>judgment_request</code></strong></dt>
<dd>Whether this call is requesting a judgment from a judge agent</dd>
<dt><strong><code>scenario_state</code></strong></dt>
<dd>Current state of the scenario execution</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>class MyAgent(AgentAdapter):
    async def call(self, input: AgentInput) -&gt; str:
        # Get the latest user message
        user_msg = input.last_new_user_message_str()

        # Process with your LLM/agent
        response = await my_llm.complete(
            messages=input.messages,
            prompt=user_msg
        )

        return response
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AgentInput(BaseModel):
    &#34;&#34;&#34;
    Input data structure passed to agent adapters during scenario execution.

    This class encapsulates all the information an agent needs to generate its next response,
    including conversation history, thread context, and scenario state. It provides convenient
    methods to access the most recent user messages.

    Attributes:
        thread_id: Unique identifier for the conversation thread
        messages: Complete conversation history as OpenAI-compatible messages
        new_messages: Only the new messages since the agent&#39;s last call
        judgment_request: Whether this call is requesting a judgment from a judge agent
        scenario_state: Current state of the scenario execution

    Example:
        ```
        class MyAgent(AgentAdapter):
            async def call(self, input: AgentInput) -&gt; str:
                # Get the latest user message
                user_msg = input.last_new_user_message_str()

                # Process with your LLM/agent
                response = await my_llm.complete(
                    messages=input.messages,
                    prompt=user_msg
                )

                return response
        ```
    &#34;&#34;&#34;

    thread_id: str
    # Prevent pydantic from validating/parsing the messages and causing issues: https://github.com/pydantic/pydantic/issues/9541
    messages: Annotated[List[ChatCompletionMessageParam], SkipValidation]
    new_messages: Annotated[List[ChatCompletionMessageParam], SkipValidation]
    judgment_request: bool = False
    scenario_state: ScenarioStateType

    def last_new_user_message(self) -&gt; ChatCompletionUserMessageParam:
        &#34;&#34;&#34;
        Get the most recent user message from the new messages.

        Returns:
            The last user message in OpenAI message format

        Raises:
            ValueError: If no new user messages are found

        Example:
            ```
            user_message = input.last_new_user_message()
            content = user_message[&#34;content&#34;]
            ```
        &#34;&#34;&#34;
        user_messages = [m for m in self.new_messages if m[&#34;role&#34;] == &#34;user&#34;]
        if not user_messages:
            raise ValueError(
                &#34;No new user messages found, did you mean to call the assistant twice? Perhaps change your adapter to use the full messages list instead.&#34;
            )
        return user_messages[-1]

    def last_new_user_message_str(self) -&gt; str:
        &#34;&#34;&#34;
        Get the content of the most recent user message as a string.

        This is a convenience method for getting simple text content from user messages.
        For multimodal messages or complex content, use last_new_user_message() instead.

        Returns:
            The text content of the last user message

        Raises:
            ValueError: If no new user messages found or if the message content is not a string

        Example:
            ```
            user_text = input.last_new_user_message_str()
            response = f&#34;You said: {user_text}&#34;
            ```
        &#34;&#34;&#34;
        content = self.last_new_user_message()[&#34;content&#34;]
        if type(content) != str:
            raise ValueError(
                f&#34;Last user message is not a string: {content.__repr__()}. Please use the full messages list instead.&#34;
            )
        return content</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.AgentInput.judgment_request"><code class="name">var <span class="ident">judgment_request</span> : bool</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentInput.messages"><code class="name">var <span class="ident">messages</span> : List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentInput.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentInput.new_messages"><code class="name">var <span class="ident">new_messages</span> : List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentInput.scenario_state"><code class="name">var <span class="ident">scenario_state</span> : Any</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentInput.thread_id"><code class="name">var <span class="ident">thread_id</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.AgentInput.last_new_user_message"><code class="name flex">
<span>def <span class="ident">last_new_user_message</span></span>(<span>self) ‑> openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam</span>
</code></dt>
<dd>
<div class="desc"><p>Get the most recent user message from the new messages.</p>
<h2 id="returns">Returns</h2>
<p>The last user message in OpenAI message format</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no new user messages are found</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>user_message = input.last_new_user_message()
content = user_message[&quot;content&quot;]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_new_user_message(self) -&gt; ChatCompletionUserMessageParam:
    &#34;&#34;&#34;
    Get the most recent user message from the new messages.

    Returns:
        The last user message in OpenAI message format

    Raises:
        ValueError: If no new user messages are found

    Example:
        ```
        user_message = input.last_new_user_message()
        content = user_message[&#34;content&#34;]
        ```
    &#34;&#34;&#34;
    user_messages = [m for m in self.new_messages if m[&#34;role&#34;] == &#34;user&#34;]
    if not user_messages:
        raise ValueError(
            &#34;No new user messages found, did you mean to call the assistant twice? Perhaps change your adapter to use the full messages list instead.&#34;
        )
    return user_messages[-1]</code></pre>
</details>
</dd>
<dt id="scenario.AgentInput.last_new_user_message_str"><code class="name flex">
<span>def <span class="ident">last_new_user_message_str</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Get the content of the most recent user message as a string.</p>
<p>This is a convenience method for getting simple text content from user messages.
For multimodal messages or complex content, use last_new_user_message() instead.</p>
<h2 id="returns">Returns</h2>
<p>The text content of the last user message</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no new user messages found or if the message content is not a string</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>user_text = input.last_new_user_message_str()
response = f&quot;You said: {user_text}&quot;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_new_user_message_str(self) -&gt; str:
    &#34;&#34;&#34;
    Get the content of the most recent user message as a string.

    This is a convenience method for getting simple text content from user messages.
    For multimodal messages or complex content, use last_new_user_message() instead.

    Returns:
        The text content of the last user message

    Raises:
        ValueError: If no new user messages found or if the message content is not a string

    Example:
        ```
        user_text = input.last_new_user_message_str()
        response = f&#34;You said: {user_text}&#34;
        ```
    &#34;&#34;&#34;
    content = self.last_new_user_message()[&#34;content&#34;]
    if type(content) != str:
        raise ValueError(
            f&#34;Last user message is not a string: {content.__repr__()}. Please use the full messages list instead.&#34;
        )
    return content</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scenario.AgentRole"><code class="flex name class">
<span>class <span class="ident">AgentRole</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the different roles that agents can play in a scenario.</p>
<p>This enum is used to identify the role of each agent during scenario execution,
enabling the framework to determine the order and interaction patterns between
different types of agents.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>USER</code></strong></dt>
<dd>Represents a user simulator agent that generates user inputs</dd>
<dt><strong><code>AGENT</code></strong></dt>
<dd>Represents the agent under test that responds to user inputs</dd>
<dt><strong><code>JUDGE</code></strong></dt>
<dd>Represents a judge agent that evaluates the conversation and determines success/failure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AgentRole(Enum):
    &#34;&#34;&#34;
    Defines the different roles that agents can play in a scenario.

    This enum is used to identify the role of each agent during scenario execution,
    enabling the framework to determine the order and interaction patterns between
    different types of agents.

    Attributes:
        USER: Represents a user simulator agent that generates user inputs
        AGENT: Represents the agent under test that responds to user inputs
        JUDGE: Represents a judge agent that evaluates the conversation and determines success/failure
    &#34;&#34;&#34;

    USER = &#34;User&#34;
    AGENT = &#34;Agent&#34;
    JUDGE = &#34;Judge&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.AgentRole.AGENT"><code class="name">var <span class="ident">AGENT</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentRole.JUDGE"><code class="name">var <span class="ident">JUDGE</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.AgentRole.USER"><code class="name">var <span class="ident">USER</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="scenario.JudgeAgent"><code class="flex name class">
<span>class <span class="ident">JudgeAgent</span></span>
<span>(</span><span>*, criteria: List[str] | None = None, model: str | None = None, api_base: str | None = None, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, system_prompt: str | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Agent that evaluates conversations against success criteria.</p>
<p>The JudgeAgent watches conversations in real-time and makes decisions about
whether the agent under test is meeting the specified criteria. It can either
allow the conversation to continue or end it with a success/failure verdict.</p>
<p>The judge uses function calling to make structured decisions and provides
detailed reasoning for its verdicts. It evaluates each criterion independently
and provides comprehensive feedback about what worked and what didn't.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong></dt>
<dd>Always AgentRole.JUDGE for judge agents</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier to use for evaluation</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Optional API key for the model provider</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for evaluation consistency</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum tokens for judge reasoning</dd>
<dt><strong><code>criteria</code></strong></dt>
<dd>List of success criteria to evaluate against</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default judge behavior</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Basic judge agent with criteria
judge = scenario.JudgeAgent(
    criteria=[
        &quot;Agent provides helpful responses&quot;,
        &quot;Agent asks relevant follow-up questions&quot;,
        &quot;Agent does not provide harmful information&quot;
    ]
)

# Customized judge with specific model and behavior
strict_judge = scenario.JudgeAgent(
    model=&quot;openai/gpt-4.1&quot;,
    criteria=[
        &quot;Code examples are syntactically correct&quot;,
        &quot;Explanations are technically accurate&quot;,
        &quot;Security best practices are mentioned&quot;
    ],
    temperature=0.0,  # More deterministic evaluation
    system_prompt=&quot;You are a strict technical reviewer evaluating code quality.&quot;
)

# Use in scenario
result = await scenario.run(
    name=&quot;coding assistant test&quot;,
    description=&quot;User asks for help with Python functions&quot;,
    agents=[
        coding_agent,
        scenario.UserSimulatorAgent(),
        judge
    ]
)

print(f&quot;Passed criteria: {result.passed_criteria}&quot;)
print(f&quot;Failed criteria: {result.failed_criteria}&quot;)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Judge agents evaluate conversations continuously, not just at the end</li>
<li>They can end scenarios early if clear success/failure conditions are met</li>
<li>Provide detailed reasoning for their decisions</li>
<li>Support both positive criteria (things that should happen) and negative criteria (things that shouldn't)</li>
</ul>
<p>Initialize a judge agent with evaluation criteria.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>criteria</code></strong></dt>
<dd>List of success criteria to evaluate the conversation against.
Can include both positive requirements ("Agent provides helpful responses")
and negative constraints ("Agent should not provide personal information").</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier (e.g., "openai/gpt-4.1").
If not provided, uses the default model from global configuration.</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted. If not provided,
uses the base URL from global configuration.</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>API key for the model provider. If not provided,
uses the key from global configuration or environment.</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for evaluation (0.0-1.0).
Lower values (0.0-0.2) recommended for consistent evaluation.</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum number of tokens for judge reasoning and explanations.</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default judge behavior.
Use this to create specialized evaluation perspectives.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model is configured either in parameters or global config</dd>
</dl>
<h2 id="example_1">Example</h2>
<pre><code># Customer service judge
cs_judge = JudgeAgent(
    criteria=[
        &quot;Agent replies with the refund policy&quot;,
        &quot;Agent offers next steps for the customer&quot;,
    ],
    temperature=0.1
)

# Technical accuracy judge
tech_judge = JudgeAgent(
    criteria=[
        &quot;Agent adds a code review pointing out the code compilation errors&quot;,
        &quot;Agent adds a code review about the missing security headers&quot;
    ],
    system_prompt=&quot;You are a senior software engineer reviewing code for production use.&quot;
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JudgeAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that evaluates conversations against success criteria.

    The JudgeAgent watches conversations in real-time and makes decisions about
    whether the agent under test is meeting the specified criteria. It can either
    allow the conversation to continue or end it with a success/failure verdict.

    The judge uses function calling to make structured decisions and provides
    detailed reasoning for its verdicts. It evaluates each criterion independently
    and provides comprehensive feedback about what worked and what didn&#39;t.

    Attributes:
        role: Always AgentRole.JUDGE for judge agents
        model: LLM model identifier to use for evaluation
        api_base: Optional base URL where the model is hosted
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for evaluation consistency
        max_tokens: Maximum tokens for judge reasoning
        criteria: List of success criteria to evaluate against
        system_prompt: Custom system prompt to override default judge behavior

    Example:
        ```
        import scenario

        # Basic judge agent with criteria
        judge = scenario.JudgeAgent(
            criteria=[
                &#34;Agent provides helpful responses&#34;,
                &#34;Agent asks relevant follow-up questions&#34;,
                &#34;Agent does not provide harmful information&#34;
            ]
        )

        # Customized judge with specific model and behavior
        strict_judge = scenario.JudgeAgent(
            model=&#34;openai/gpt-4.1&#34;,
            criteria=[
                &#34;Code examples are syntactically correct&#34;,
                &#34;Explanations are technically accurate&#34;,
                &#34;Security best practices are mentioned&#34;
            ],
            temperature=0.0,  # More deterministic evaluation
            system_prompt=&#34;You are a strict technical reviewer evaluating code quality.&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;coding assistant test&#34;,
            description=&#34;User asks for help with Python functions&#34;,
            agents=[
                coding_agent,
                scenario.UserSimulatorAgent(),
                judge
            ]
        )

        print(f&#34;Passed criteria: {result.passed_criteria}&#34;)
        print(f&#34;Failed criteria: {result.failed_criteria}&#34;)
        ```

    Note:
        - Judge agents evaluate conversations continuously, not just at the end
        - They can end scenarios early if clear success/failure conditions are met
        - Provide detailed reasoning for their decisions
        - Support both positive criteria (things that should happen) and negative criteria (things that shouldn&#39;t)
    &#34;&#34;&#34;

    role = AgentRole.JUDGE

    model: str
    api_base: Optional[str]
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    criteria: List[str]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        criteria: Optional[List[str]] = None,
        model: Optional[str] = None,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a judge agent with evaluation criteria.

        Args:
            criteria: List of success criteria to evaluate the conversation against.
                     Can include both positive requirements (&#34;Agent provides helpful responses&#34;)
                     and negative constraints (&#34;Agent should not provide personal information&#34;).
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1&#34;).
                   If not provided, uses the default model from global configuration.
            api_base: Optional base URL where the model is hosted. If not provided,
                      uses the base URL from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for evaluation (0.0-1.0).
                        Lower values (0.0-0.2) recommended for consistent evaluation.
            max_tokens: Maximum number of tokens for judge reasoning and explanations.
            system_prompt: Custom system prompt to override default judge behavior.
                          Use this to create specialized evaluation perspectives.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Customer service judge
            cs_judge = JudgeAgent(
                criteria=[
                    &#34;Agent replies with the refund policy&#34;,
                    &#34;Agent offers next steps for the customer&#34;,
                ],
                temperature=0.1
            )

            # Technical accuracy judge
            tech_judge = JudgeAgent(
                criteria=[
                    &#34;Agent adds a code review pointing out the code compilation errors&#34;,
                    &#34;Agent adds a code review about the missing security headers&#34;
                ],
                system_prompt=&#34;You are a senior software engineer reviewing code for production use.&#34;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the judge agent
        self.criteria = criteria or []
        self.api_base = api_base
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_base = (
                api_base or ScenarioConfig.default_config.default_model.api_base
            )
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Evaluate the current conversation state against the configured criteria.

        This method analyzes the conversation history and determines whether the
        scenario should continue or end with a verdict. It uses function calling
        to make structured decisions and provides detailed reasoning.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: Either an empty list (continue scenario) or a
                            ScenarioResult (end scenario with verdict)

        Raises:
            Exception: If the judge cannot make a valid decision or if there&#39;s an
                      error in the evaluation process

        Note:
            - Returns empty list [] to continue the scenario
            - Returns ScenarioResult to end with success/failure
            - Provides detailed reasoning for all decisions
            - Evaluates each criterion independently
            - Can end scenarios early if clear violation or success is detected
        &#34;&#34;&#34;

        scenario = input.scenario_state

        criteria_str = &#34;\n&#34;.join(
            [f&#34;{idx + 1}. {criterion}&#34; for idx, criterion in enumerate(self.criteria)]
        )

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are an LLM as a judge watching a simulated conversation as it plays out live to determine if the agent under test meets the criteria or not.
&lt;/role&gt;

&lt;goal&gt;
Your goal is to determine if you already have enough information to make a verdict of the scenario below, or if the conversation should continue for longer.
If you do have enough information, use the finish_test tool to determine if all the criteria have been met, if not, use the continue_test tool to let the next step play out.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;criteria&gt;
{criteria_str}
&lt;/criteria&gt;

&lt;rules&gt;
- Be strict, do not let the conversation continue if the agent already broke one of the &#34;do not&#34; or &#34;should not&#34; criterias.
- DO NOT make any judgment calls that are not explicitly listed in the success or failure criteria, withhold judgement if necessary
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            *input.messages,
        ]

        is_last_message = (
            input.scenario_state.current_turn == input.scenario_state.config.max_turns
        )

        if is_last_message:
            messages.append(
                {
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: &#34;&#34;&#34;
System:

&lt;finish_test&gt;
This is the last message, conversation has reached the maximum number of turns, give your final verdict,
if you don&#39;t have enough information to make a verdict, say inconclusive with max turns reached.
&lt;/finish_test&gt;
&#34;&#34;&#34;,
                }
            )

        # Define the tools
        criteria_names = [
            re.sub(
                r&#34;[^a-zA-Z0-9]&#34;,
                &#34;_&#34;,
                criterion.replace(&#34; &#34;, &#34;_&#34;).replace(&#34;&#39;&#34;, &#34;&#34;).lower(),
            )[:70]
            for criterion in self.criteria
        ]
        tools = [
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;continue_test&#34;,
                    &#34;description&#34;: &#34;Continue the test with the next step&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {},
                        &#34;required&#34;: [],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;finish_test&#34;,
                    &#34;description&#34;: &#34;Complete the test with a final verdict&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {
                            &#34;criteria&#34;: {
                                &#34;type&#34;: &#34;object&#34;,
                                &#34;properties&#34;: {
                                    criteria_names[idx]: {
                                        &#34;enum&#34;: [True, False, &#34;inconclusive&#34;],
                                        &#34;description&#34;: criterion,
                                    }
                                    for idx, criterion in enumerate(self.criteria)
                                },
                                &#34;required&#34;: criteria_names,
                                &#34;additionalProperties&#34;: False,
                                &#34;description&#34;: &#34;Strict verdict for each criterion&#34;,
                            },
                            &#34;reasoning&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;description&#34;: &#34;Explanation of what the final verdict should be&#34;,
                            },
                            &#34;verdict&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;enum&#34;: [&#34;success&#34;, &#34;failure&#34;, &#34;inconclusive&#34;],
                                &#34;description&#34;: &#34;The final verdict of the test&#34;,
                            },
                        },
                        &#34;required&#34;: [&#34;criteria&#34;, &#34;reasoning&#34;, &#34;verdict&#34;],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
        ]

        enforce_judgment = input.judgment_request
        has_criteria = len(self.criteria) &gt; 0

        if enforce_judgment and not has_criteria:
            return ScenarioResult(
                success=False,
                messages=[],
                reasoning=&#34;TestingAgent was called as a judge, but it has no criteria to judge against&#34;,
            )

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=(
                    {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;finish_test&#34;}}
                    if (is_last_message or enforce_judgment) and has_criteria
                    else &#34;required&#34;
                ),
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            # Check if the LLM chose to use the tool
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                if tool_call.function.name == &#34;continue_test&#34;:
                    return []

                if tool_call.function.name == &#34;finish_test&#34;:
                    # Parse the tool call arguments
                    try:
                        args = json.loads(tool_call.function.arguments)
                        verdict = args.get(&#34;verdict&#34;, &#34;inconclusive&#34;)
                        reasoning = args.get(&#34;reasoning&#34;, &#34;No reasoning provided&#34;)
                        criteria = args.get(&#34;criteria&#34;, {})

                        passed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == True
                        ]
                        failed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == False or criterion == &#34;inconclusive&#34;
                        ]

                        # Return the appropriate ScenarioResult based on the verdict
                        return ScenarioResult(
                            success=verdict == &#34;success&#34; and len(failed_criteria) == 0,
                            messages=messages,
                            reasoning=reasoning,
                            passed_criteria=passed_criteria,
                            failed_criteria=failed_criteria,
                        )
                    except json.JSONDecodeError:
                        raise Exception(
                            f&#34;Failed to parse tool call arguments from judge agent: {tool_call.function.arguments}&#34;
                        )

                else:
                    raise Exception(
                        f&#34;Invalid tool call from judge agent: {tool_call.function.name}&#34;
                    )

            else:
                raise Exception(
                    f&#34;Invalid response from judge agent, tool calls not found: {message.__repr__()}&#34;
                )

        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.JudgeAgent.api_base"><code class="name">var <span class="ident">api_base</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.api_key"><code class="name">var <span class="ident">api_key</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.criteria"><code class="name">var <span class="ident">criteria</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.max_tokens"><code class="name">var <span class="ident">max_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.system_prompt"><code class="name">var <span class="ident">system_prompt</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.JudgeAgent.temperature"><code class="name">var <span class="ident">temperature</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.JudgeAgent.call"><code class="name flex">
<span>async def <span class="ident">call</span></span>(<span>self, input: <a title="scenario.types.AgentInput" href="types.html#scenario.types.AgentInput">AgentInput</a>) ‑> str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam] | <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the current conversation state against the configured criteria.</p>
<p>This method analyzes the conversation history and determines whether the
scenario should continue or end with a verdict. It uses function calling
to make structured decisions and provides detailed reasoning.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>AgentInput containing conversation history and scenario context</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AgentReturnTypes</code></dt>
<dd>Either an empty list (continue scenario) or a
ScenarioResult (end scenario with verdict)</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If the judge cannot make a valid decision or if there's an
error in the evaluation process</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Returns empty list [] to continue the scenario</li>
<li>Returns ScenarioResult to end with success/failure</li>
<li>Provides detailed reasoning for all decisions</li>
<li>Evaluates each criterion independently</li>
<li>Can end scenarios early if clear violation or success is detected</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Evaluate the current conversation state against the configured criteria.

        This method analyzes the conversation history and determines whether the
        scenario should continue or end with a verdict. It uses function calling
        to make structured decisions and provides detailed reasoning.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: Either an empty list (continue scenario) or a
                            ScenarioResult (end scenario with verdict)

        Raises:
            Exception: If the judge cannot make a valid decision or if there&#39;s an
                      error in the evaluation process

        Note:
            - Returns empty list [] to continue the scenario
            - Returns ScenarioResult to end with success/failure
            - Provides detailed reasoning for all decisions
            - Evaluates each criterion independently
            - Can end scenarios early if clear violation or success is detected
        &#34;&#34;&#34;

        scenario = input.scenario_state

        criteria_str = &#34;\n&#34;.join(
            [f&#34;{idx + 1}. {criterion}&#34; for idx, criterion in enumerate(self.criteria)]
        )

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are an LLM as a judge watching a simulated conversation as it plays out live to determine if the agent under test meets the criteria or not.
&lt;/role&gt;

&lt;goal&gt;
Your goal is to determine if you already have enough information to make a verdict of the scenario below, or if the conversation should continue for longer.
If you do have enough information, use the finish_test tool to determine if all the criteria have been met, if not, use the continue_test tool to let the next step play out.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;criteria&gt;
{criteria_str}
&lt;/criteria&gt;

&lt;rules&gt;
- Be strict, do not let the conversation continue if the agent already broke one of the &#34;do not&#34; or &#34;should not&#34; criterias.
- DO NOT make any judgment calls that are not explicitly listed in the success or failure criteria, withhold judgement if necessary
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            *input.messages,
        ]

        is_last_message = (
            input.scenario_state.current_turn == input.scenario_state.config.max_turns
        )

        if is_last_message:
            messages.append(
                {
                    &#34;role&#34;: &#34;user&#34;,
                    &#34;content&#34;: &#34;&#34;&#34;
System:

&lt;finish_test&gt;
This is the last message, conversation has reached the maximum number of turns, give your final verdict,
if you don&#39;t have enough information to make a verdict, say inconclusive with max turns reached.
&lt;/finish_test&gt;
&#34;&#34;&#34;,
                }
            )

        # Define the tools
        criteria_names = [
            re.sub(
                r&#34;[^a-zA-Z0-9]&#34;,
                &#34;_&#34;,
                criterion.replace(&#34; &#34;, &#34;_&#34;).replace(&#34;&#39;&#34;, &#34;&#34;).lower(),
            )[:70]
            for criterion in self.criteria
        ]
        tools = [
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;continue_test&#34;,
                    &#34;description&#34;: &#34;Continue the test with the next step&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {},
                        &#34;required&#34;: [],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
            {
                &#34;type&#34;: &#34;function&#34;,
                &#34;function&#34;: {
                    &#34;name&#34;: &#34;finish_test&#34;,
                    &#34;description&#34;: &#34;Complete the test with a final verdict&#34;,
                    &#34;strict&#34;: True,
                    &#34;parameters&#34;: {
                        &#34;type&#34;: &#34;object&#34;,
                        &#34;properties&#34;: {
                            &#34;criteria&#34;: {
                                &#34;type&#34;: &#34;object&#34;,
                                &#34;properties&#34;: {
                                    criteria_names[idx]: {
                                        &#34;enum&#34;: [True, False, &#34;inconclusive&#34;],
                                        &#34;description&#34;: criterion,
                                    }
                                    for idx, criterion in enumerate(self.criteria)
                                },
                                &#34;required&#34;: criteria_names,
                                &#34;additionalProperties&#34;: False,
                                &#34;description&#34;: &#34;Strict verdict for each criterion&#34;,
                            },
                            &#34;reasoning&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;description&#34;: &#34;Explanation of what the final verdict should be&#34;,
                            },
                            &#34;verdict&#34;: {
                                &#34;type&#34;: &#34;string&#34;,
                                &#34;enum&#34;: [&#34;success&#34;, &#34;failure&#34;, &#34;inconclusive&#34;],
                                &#34;description&#34;: &#34;The final verdict of the test&#34;,
                            },
                        },
                        &#34;required&#34;: [&#34;criteria&#34;, &#34;reasoning&#34;, &#34;verdict&#34;],
                        &#34;additionalProperties&#34;: False,
                    },
                },
            },
        ]

        enforce_judgment = input.judgment_request
        has_criteria = len(self.criteria) &gt; 0

        if enforce_judgment and not has_criteria:
            return ScenarioResult(
                success=False,
                messages=[],
                reasoning=&#34;TestingAgent was called as a judge, but it has no criteria to judge against&#34;,
            )

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=tools,
                tool_choice=(
                    {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;finish_test&#34;}}
                    if (is_last_message or enforce_judgment) and has_criteria
                    else &#34;required&#34;
                ),
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            # Check if the LLM chose to use the tool
            if message.tool_calls:
                tool_call = message.tool_calls[0]
                if tool_call.function.name == &#34;continue_test&#34;:
                    return []

                if tool_call.function.name == &#34;finish_test&#34;:
                    # Parse the tool call arguments
                    try:
                        args = json.loads(tool_call.function.arguments)
                        verdict = args.get(&#34;verdict&#34;, &#34;inconclusive&#34;)
                        reasoning = args.get(&#34;reasoning&#34;, &#34;No reasoning provided&#34;)
                        criteria = args.get(&#34;criteria&#34;, {})

                        passed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == True
                        ]
                        failed_criteria = [
                            self.criteria[idx]
                            for idx, criterion in enumerate(criteria.values())
                            if criterion == False or criterion == &#34;inconclusive&#34;
                        ]

                        # Return the appropriate ScenarioResult based on the verdict
                        return ScenarioResult(
                            success=verdict == &#34;success&#34; and len(failed_criteria) == 0,
                            messages=messages,
                            reasoning=reasoning,
                            passed_criteria=passed_criteria,
                            failed_criteria=failed_criteria,
                        )
                    except json.JSONDecodeError:
                        raise Exception(
                            f&#34;Failed to parse tool call arguments from judge agent: {tool_call.function.arguments}&#34;
                        )

                else:
                    raise Exception(
                        f&#34;Invalid tool call from judge agent: {tool_call.function.name}&#34;
                    )

            else:
                raise Exception(
                    f&#34;Invalid response from judge agent, tool calls not found: {message.__repr__()}&#34;
                )

        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></b></code>:
<ul class="hlist">
<li><code><a title="scenario.agent_adapter.AgentAdapter.role" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter.role">role</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="scenario.ScenarioConfig"><code class="flex name class">
<span>class <span class="ident">ScenarioConfig</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Global configuration class for the Scenario testing framework.</p>
<p>This class allows users to set default behavior and parameters that apply
to all scenario executions, including the LLM model to use for simulator
and judge agents, execution limits, and debugging options.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>default_model</code></strong></dt>
<dd>Default LLM model configuration for agents (can be string or ModelConfig)</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum number of conversation turns before scenario times out</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to show detailed output during execution (True/False or verbosity level)</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Key for caching scenario results to ensure deterministic behavior</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Whether to enable debug mode with step-by-step interaction</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code># Configure globally for all scenarios
scenario.configure(
    default_model=&quot;openai/gpt-4.1-mini&quot;,
    max_turns=15,
    verbose=True,
    cache_key=&quot;my-test-suite-v1&quot;,
    debug=False
)

# Or create a specific config instance
config = ScenarioConfig(
    default_model=ModelConfig(
        model=&quot;openai/gpt-4.1&quot;,
        temperature=0.2
    ),
    max_turns=20
)
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioConfig(BaseModel):
    &#34;&#34;&#34;
    Global configuration class for the Scenario testing framework.

    This class allows users to set default behavior and parameters that apply
    to all scenario executions, including the LLM model to use for simulator
    and judge agents, execution limits, and debugging options.

    Attributes:
        default_model: Default LLM model configuration for agents (can be string or ModelConfig)
        max_turns: Maximum number of conversation turns before scenario times out
        verbose: Whether to show detailed output during execution (True/False or verbosity level)
        cache_key: Key for caching scenario results to ensure deterministic behavior
        debug: Whether to enable debug mode with step-by-step interaction

    Example:
        ```
        # Configure globally for all scenarios
        scenario.configure(
            default_model=&#34;openai/gpt-4.1-mini&#34;,
            max_turns=15,
            verbose=True,
            cache_key=&#34;my-test-suite-v1&#34;,
            debug=False
        )

        # Or create a specific config instance
        config = ScenarioConfig(
            default_model=ModelConfig(
                model=&#34;openai/gpt-4.1&#34;,
                temperature=0.2
            ),
            max_turns=20
        )
        ```
    &#34;&#34;&#34;

    default_model: Optional[Union[str, ModelConfig]] = None
    max_turns: Optional[int] = 10
    verbose: Optional[Union[bool, int]] = True
    cache_key: Optional[str] = None
    debug: Optional[bool] = False
    headless: Optional[bool] = os.getenv(&#34;SCENARIO_HEADLESS&#34;, &#34;false&#34;).lower() not in [
        &#34;false&#34;,
        &#34;0&#34;,
        &#34;&#34;,
    ]

    default_config: ClassVar[Optional[&#34;ScenarioConfig&#34;]] = None

    @classmethod
    def configure(
        cls,
        default_model: Optional[str] = None,
        max_turns: Optional[int] = None,
        verbose: Optional[Union[bool, int]] = None,
        cache_key: Optional[str] = None,
        debug: Optional[bool] = None,
        headless: Optional[bool] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Set global configuration settings for all scenario executions.

        This method allows you to configure default behavior that will be applied
        to all scenarios unless explicitly overridden in individual scenario runs.

        Args:
            default_model: Default LLM model identifier for user simulator and judge agents
            max_turns: Maximum number of conversation turns before timeout (default: 10)
            verbose: Enable verbose output during scenario execution
            cache_key: Cache key for deterministic scenario behavior across runs
            debug: Enable debug mode for step-by-step execution with user intervention

        Example:
            ```
            import scenario

            # Set up default configuration
            scenario.configure(
                default_model=&#34;openai/gpt-4.1-mini&#34;,
                max_turns=15,
                verbose=True,
                debug=False
            )

            # All subsequent scenario runs will use these defaults
            result = await scenario.run(
                name=&#34;my test&#34;,
                description=&#34;Test scenario&#34;,
                agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
            )
            ```
        &#34;&#34;&#34;
        existing_config = cls.default_config or ScenarioConfig()

        cls.default_config = existing_config.merge(
            ScenarioConfig(
                default_model=default_model,
                max_turns=max_turns,
                verbose=verbose,
                cache_key=cache_key,
                debug=debug,
                headless=headless,
            )
        )

    def merge(self, other: &#34;ScenarioConfig&#34;) -&gt; &#34;ScenarioConfig&#34;:
        &#34;&#34;&#34;
        Merge this configuration with another configuration.

        Values from the other configuration will override values in this
        configuration where they are not None.

        Args:
            other: Another ScenarioConfig instance to merge with

        Returns:
            A new ScenarioConfig instance with merged values

        Example:
            ```
            base_config = ScenarioConfig(max_turns=10, verbose=True)
            override_config = ScenarioConfig(max_turns=20)

            merged = base_config.merge(override_config)
            # Result: max_turns=20, verbose=True
            ```
        &#34;&#34;&#34;
        return ScenarioConfig(
            **{
                **self.items(),
                **other.items(),
            }
        )

    def items(self):
        &#34;&#34;&#34;
        Get configuration items as a dictionary.

        Returns:
            Dictionary of configuration key-value pairs, excluding None values

        Example:
            ```
            config = ScenarioConfig(max_turns=15, verbose=True)
            items = config.items()
            # Result: {&#34;max_turns&#34;: 15, &#34;verbose&#34;: True}
            ```
        &#34;&#34;&#34;
        return {k: getattr(self, k) for k in self.model_dump(exclude_none=True).keys()}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.ScenarioConfig.cache_key"><code class="name">var <span class="ident">cache_key</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.debug"><code class="name">var <span class="ident">debug</span> : bool | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.default_config"><code class="name">var <span class="ident">default_config</span> : ClassVar[<a title="scenario.config.scenario.ScenarioConfig" href="config/scenario.html#scenario.config.scenario.ScenarioConfig">ScenarioConfig</a> | None]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.default_model"><code class="name">var <span class="ident">default_model</span> : str | <a title="scenario.config.model.ModelConfig" href="config/model.html#scenario.config.model.ModelConfig">ModelConfig</a> | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.headless"><code class="name">var <span class="ident">headless</span> : bool | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.max_turns"><code class="name">var <span class="ident">max_turns</span> : int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioConfig.verbose"><code class="name">var <span class="ident">verbose</span> : bool | int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scenario.ScenarioConfig.configure"><code class="name flex">
<span>def <span class="ident">configure</span></span>(<span>default_model: str | None = None, max_turns: int | None = None, verbose: bool | int | None = None, cache_key: str | None = None, debug: bool | None = None, headless: bool | None = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set global configuration settings for all scenario executions.</p>
<p>This method allows you to configure default behavior that will be applied
to all scenarios unless explicitly overridden in individual scenario runs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>default_model</code></strong></dt>
<dd>Default LLM model identifier for user simulator and judge agents</dd>
<dt><strong><code>max_turns</code></strong></dt>
<dd>Maximum number of conversation turns before timeout (default: 10)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Enable verbose output during scenario execution</dd>
<dt><strong><code>cache_key</code></strong></dt>
<dd>Cache key for deterministic scenario behavior across runs</dd>
<dt><strong><code>debug</code></strong></dt>
<dd>Enable debug mode for step-by-step execution with user intervention</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Set up default configuration
scenario.configure(
    default_model=&quot;openai/gpt-4.1-mini&quot;,
    max_turns=15,
    verbose=True,
    debug=False
)

# All subsequent scenario runs will use these defaults
result = await scenario.run(
    name=&quot;my test&quot;,
    description=&quot;Test scenario&quot;,
    agents=[my_agent, scenario.UserSimulatorAgent(), scenario.JudgeAgent()]
)
</code></pre></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.ScenarioConfig.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get configuration items as a dictionary.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary of configuration key-value pairs, excluding None values</p>
<h2 id="example">Example</h2>
<pre><code>config = ScenarioConfig(max_turns=15, verbose=True)
items = config.items()
# Result: {&quot;max_turns&quot;: 15, &quot;verbose&quot;: True}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    &#34;&#34;&#34;
    Get configuration items as a dictionary.

    Returns:
        Dictionary of configuration key-value pairs, excluding None values

    Example:
        ```
        config = ScenarioConfig(max_turns=15, verbose=True)
        items = config.items()
        # Result: {&#34;max_turns&#34;: 15, &#34;verbose&#34;: True}
        ```
    &#34;&#34;&#34;
    return {k: getattr(self, k) for k in self.model_dump(exclude_none=True).keys()}</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioConfig.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, other: <a title="scenario.ScenarioConfig" href="#scenario.ScenarioConfig">ScenarioConfig</a>) ‑> <a title="scenario.config.scenario.ScenarioConfig" href="config/scenario.html#scenario.config.scenario.ScenarioConfig">ScenarioConfig</a></span>
</code></dt>
<dd>
<div class="desc"><p>Merge this configuration with another configuration.</p>
<p>Values from the other configuration will override values in this
configuration where they are not None.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>Another ScenarioConfig instance to merge with</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A new ScenarioConfig instance with merged values</p>
<h2 id="example">Example</h2>
<pre><code>base_config = ScenarioConfig(max_turns=10, verbose=True)
override_config = ScenarioConfig(max_turns=20)

merged = base_config.merge(override_config)
# Result: max_turns=20, verbose=True
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, other: &#34;ScenarioConfig&#34;) -&gt; &#34;ScenarioConfig&#34;:
    &#34;&#34;&#34;
    Merge this configuration with another configuration.

    Values from the other configuration will override values in this
    configuration where they are not None.

    Args:
        other: Another ScenarioConfig instance to merge with

    Returns:
        A new ScenarioConfig instance with merged values

    Example:
        ```
        base_config = ScenarioConfig(max_turns=10, verbose=True)
        override_config = ScenarioConfig(max_turns=20)

        merged = base_config.merge(override_config)
        # Result: max_turns=20, verbose=True
        ```
    &#34;&#34;&#34;
    return ScenarioConfig(
        **{
            **self.items(),
            **other.items(),
        }
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scenario.ScenarioResult"><code class="flex name class">
<span>class <span class="ident">ScenarioResult</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Represents the final result of a scenario test execution.</p>
<p>This class contains all the information about how a scenario performed,
including whether it succeeded, the conversation that took place, and
detailed reasoning about which criteria were met or failed.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>success</code></strong></dt>
<dd>Whether the scenario passed all criteria and completed successfully</dd>
<dt><strong><code>messages</code></strong></dt>
<dd>Complete conversation history that occurred during the scenario</dd>
<dt><strong><code>reasoning</code></strong></dt>
<dd>Detailed explanation of why the scenario succeeded or failed</dd>
<dt><strong><code>passed_criteria</code></strong></dt>
<dd>List of success criteria that were satisfied</dd>
<dt><strong><code>failed_criteria</code></strong></dt>
<dd>List of success criteria that were not satisfied</dd>
<dt><strong><code>total_time</code></strong></dt>
<dd>Total execution time in seconds (if measured)</dd>
<dt><strong><code>agent_time</code></strong></dt>
<dd>Time spent in agent calls in seconds (if measured)</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>result = await scenario.run(
    name=&quot;weather query&quot;,
    description=&quot;User asks about weather&quot;,
    agents=[
        weather_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent provides helpful weather information&quot;])
    ]
)

print(f&quot;Test {'PASSED' if result.success else 'FAILED'}&quot;)
print(f&quot;Reasoning: {result.reasoning}&quot;)

if not result.success:
    print(&quot;Failed criteria:&quot;)
    for criteria in result.failed_criteria:
        print(f&quot;  - {criteria}&quot;)
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioResult(BaseModel):
    &#34;&#34;&#34;
    Represents the final result of a scenario test execution.

    This class contains all the information about how a scenario performed,
    including whether it succeeded, the conversation that took place, and
    detailed reasoning about which criteria were met or failed.

    Attributes:
        success: Whether the scenario passed all criteria and completed successfully
        messages: Complete conversation history that occurred during the scenario
        reasoning: Detailed explanation of why the scenario succeeded or failed
        passed_criteria: List of success criteria that were satisfied
        failed_criteria: List of success criteria that were not satisfied
        total_time: Total execution time in seconds (if measured)
        agent_time: Time spent in agent calls in seconds (if measured)

    Example:
        ```
        result = await scenario.run(
            name=&#34;weather query&#34;,
            description=&#34;User asks about weather&#34;,
            agents=[
                weather_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent provides helpful weather information&#34;])
            ]
        )

        print(f&#34;Test {&#39;PASSED&#39; if result.success else &#39;FAILED&#39;}&#34;)
        print(f&#34;Reasoning: {result.reasoning}&#34;)

        if not result.success:
            print(&#34;Failed criteria:&#34;)
            for criteria in result.failed_criteria:
                print(f&#34;  - {criteria}&#34;)
        ```
    &#34;&#34;&#34;

    success: bool
    # Prevent issues with slightly inconsistent message types for example when comming from Gemini right at the result level
    messages: Annotated[List[ChatCompletionMessageParam], SkipValidation]
    reasoning: Optional[str] = None
    passed_criteria: List[str] = []
    failed_criteria: List[str] = []
    total_time: Optional[float] = None
    agent_time: Optional[float] = None

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;
        Provide a concise representation for debugging and logging.

        Returns:
            A string representation showing success status and reasoning
        &#34;&#34;&#34;
        status = &#34;PASSED&#34; if self.success else &#34;FAILED&#34;
        return f&#34;ScenarioResult(success={self.success}, status={status}, reasoning=&#39;{self.reasoning or &#39;None&#39;}&#39;)&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.ScenarioResult.agent_time"><code class="name">var <span class="ident">agent_time</span> : float | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.failed_criteria"><code class="name">var <span class="ident">failed_criteria</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.messages"><code class="name">var <span class="ident">messages</span> : List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.passed_criteria"><code class="name">var <span class="ident">passed_criteria</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.reasoning"><code class="name">var <span class="ident">reasoning</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.success"><code class="name">var <span class="ident">success</span> : bool</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioResult.total_time"><code class="name">var <span class="ident">total_time</span> : float | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
</dd>
<dt id="scenario.ScenarioState"><code class="flex name class">
<span>class <span class="ident">ScenarioState</span></span>
<span>(</span><span>**data: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Represents the current state of a scenario execution.</p>
<p>This class provides access to the conversation history, turn information,
and utility methods for inspecting messages and tool calls. It's passed to
script step functions and available through AgentInput.scenario_state.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>description</code></strong></dt>
<dd>The scenario description that guides the simulation</dd>
<dt><strong><code>messages</code></strong></dt>
<dd>Complete conversation history as OpenAI-compatible messages</dd>
<dt><strong><code>thread_id</code></strong></dt>
<dd>Unique identifier for this conversation thread</dd>
<dt><strong><code>current_turn</code></strong></dt>
<dd>Current turn number in the conversation</dd>
<dt><strong><code>config</code></strong></dt>
<dd>Configuration settings for this scenario execution</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>def check_agent_behavior(state: ScenarioState) -&gt; None:
    # Check if the agent called a specific tool
    if state.has_tool_call(&quot;get_weather&quot;):
        print(&quot;Agent successfully called weather tool&quot;)

    # Get the last user message
    last_user = state.last_user_message()
    print(f&quot;User said: {last_user['content']}&quot;)

    # Check conversation length
    if len(state.messages) &gt; 10:
        print(&quot;Conversation is getting long&quot;)

# Use in scenario script
result = await scenario.run(
    name=&quot;tool usage test&quot;,
    description=&quot;Test that agent uses the correct tools&quot;,
    agents=[
        my_agent,
        scenario.UserSimulatorAgent(),
        scenario.JudgeAgent(criteria=[&quot;Agent provides helpful response&quot;])
    ],
    script=[
        scenario.user(&quot;What's the weather like?&quot;),
        scenario.agent(),
        check_agent_behavior,  # Custom inspection function
        scenario.succeed()
    ]
)
</code></pre>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises [<code>ValidationError</code>][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.</p>
<p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScenarioState(BaseModel):
    &#34;&#34;&#34;
    Represents the current state of a scenario execution.

    This class provides access to the conversation history, turn information,
    and utility methods for inspecting messages and tool calls. It&#39;s passed to
    script step functions and available through AgentInput.scenario_state.

    Attributes:
        description: The scenario description that guides the simulation
        messages: Complete conversation history as OpenAI-compatible messages
        thread_id: Unique identifier for this conversation thread
        current_turn: Current turn number in the conversation
        config: Configuration settings for this scenario execution

    Example:
        ```
        def check_agent_behavior(state: ScenarioState) -&gt; None:
            # Check if the agent called a specific tool
            if state.has_tool_call(&#34;get_weather&#34;):
                print(&#34;Agent successfully called weather tool&#34;)

            # Get the last user message
            last_user = state.last_user_message()
            print(f&#34;User said: {last_user[&#39;content&#39;]}&#34;)

            # Check conversation length
            if len(state.messages) &gt; 10:
                print(&#34;Conversation is getting long&#34;)

        # Use in scenario script
        result = await scenario.run(
            name=&#34;tool usage test&#34;,
            description=&#34;Test that agent uses the correct tools&#34;,
            agents=[
                my_agent,
                scenario.UserSimulatorAgent(),
                scenario.JudgeAgent(criteria=[&#34;Agent provides helpful response&#34;])
            ],
            script=[
                scenario.user(&#34;What&#39;s the weather like?&#34;),
                scenario.agent(),
                check_agent_behavior,  # Custom inspection function
                scenario.succeed()
            ]
        )
        ```
    &#34;&#34;&#34;

    description: str
    messages: List[ChatCompletionMessageParam]
    thread_id: str
    current_turn: int
    config: ScenarioConfig

    _executor: &#34;ScenarioExecutor&#34;

    def add_message(self, message: ChatCompletionMessageParam):
        &#34;&#34;&#34;
        Add a message to the conversation history.

        This method delegates to the scenario executor to properly handle
        message broadcasting and state updates.

        Args:
            message: OpenAI-compatible message to add to the conversation

        Example:
            ```
            def inject_system_message(state: ScenarioState) -&gt; None:
                state.add_message({
                    &#34;role&#34;: &#34;system&#34;,
                    &#34;content&#34;: &#34;The user is now in a hurry&#34;
                })
            ```
        &#34;&#34;&#34;
        self._executor.add_message(message)

    def last_message(self) -&gt; ChatCompletionMessageParam:
        &#34;&#34;&#34;
        Get the most recent message in the conversation.

        Returns:
            The last message in the conversation history

        Raises:
            ValueError: If no messages exist in the conversation

        Example:
            ```
            def check_last_response(state: ScenarioState) -&gt; None:
                last = state.last_message()
                if last[&#34;role&#34;] == &#34;assistant&#34;:
                    content = last.get(&#34;content&#34;, &#34;&#34;)
                    assert &#34;helpful&#34; in content.lower()
            ```
        &#34;&#34;&#34;
        if len(self.messages) == 0:
            raise ValueError(&#34;No messages found&#34;)
        return self.messages[-1]

    def last_user_message(self) -&gt; ChatCompletionUserMessageParam:
        &#34;&#34;&#34;
        Get the most recent user message in the conversation.

        Returns:
            The last user message in the conversation history

        Raises:
            ValueError: If no user messages exist in the conversation

        Example:
            ```
            def analyze_user_intent(state: ScenarioState) -&gt; None:
                user_msg = state.last_user_message()
                content = user_msg[&#34;content&#34;]

                if isinstance(content, str):
                    if &#34;urgent&#34; in content.lower():
                        print(&#34;User expressed urgency&#34;)
            ```
        &#34;&#34;&#34;
        user_messages = [m for m in self.messages if m[&#34;role&#34;] == &#34;user&#34;]
        if not user_messages:
            raise ValueError(&#34;No user messages found&#34;)
        return user_messages[-1]

    def last_tool_call(
        self, tool_name: str
    ) -&gt; Optional[ChatCompletionMessageToolCallParam]:
        &#34;&#34;&#34;
        Find the most recent call to a specific tool in the conversation.

        Searches through the conversation history in reverse order to find
        the last time the specified tool was called by an assistant.

        Args:
            tool_name: Name of the tool to search for

        Returns:
            The tool call object if found, None otherwise

        Example:
            ```
            def verify_weather_call(state: ScenarioState) -&gt; None:
                weather_call = state.last_tool_call(&#34;get_current_weather&#34;)
                if weather_call:
                    args = json.loads(weather_call[&#34;function&#34;][&#34;arguments&#34;])
                    assert &#34;location&#34; in args
                    print(f&#34;Weather requested for: {args[&#39;location&#39;]}&#34;)
            ```
        &#34;&#34;&#34;
        for message in reversed(self.messages):
            if message[&#34;role&#34;] == &#34;assistant&#34; and &#34;tool_calls&#34; in message:
                for tool_call in message[&#34;tool_calls&#34;]:
                    if tool_call[&#34;function&#34;][&#34;name&#34;] == tool_name:
                        return tool_call
        return None

    def has_tool_call(self, tool_name: str) -&gt; bool:
        &#34;&#34;&#34;
        Check if a specific tool has been called in the conversation.

        This is a convenience method that returns True if the specified
        tool has been called at any point in the conversation.

        Args:
            tool_name: Name of the tool to check for

        Returns:
            True if the tool has been called, False otherwise

        Example:
            ```
            def ensure_tool_usage(state: ScenarioState) -&gt; None:
                # Verify the agent used required tools
                assert state.has_tool_call(&#34;search_database&#34;)
                assert state.has_tool_call(&#34;format_results&#34;)

                # Check it didn&#39;t use forbidden tools
                assert not state.has_tool_call(&#34;delete_data&#34;)
            ```
        &#34;&#34;&#34;
        return self.last_tool_call(tool_name) is not None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.ScenarioState.config"><code class="name">var <span class="ident">config</span> : <a title="scenario.config.scenario.ScenarioConfig" href="config/scenario.html#scenario.config.scenario.ScenarioConfig">ScenarioConfig</a></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioState.current_turn"><code class="name">var <span class="ident">current_turn</span> : int</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioState.description"><code class="name">var <span class="ident">description</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioState.messages"><code class="name">var <span class="ident">messages</span> : List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioState.model_config"><code class="name">var <span class="ident">model_config</span></code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.ScenarioState.thread_id"><code class="name">var <span class="ident">thread_id</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.ScenarioState.add_message"><code class="name flex">
<span>def <span class="ident">add_message</span></span>(<span>self, message: openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a message to the conversation history.</p>
<p>This method delegates to the scenario executor to properly handle
message broadcasting and state updates.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message</code></strong></dt>
<dd>OpenAI-compatible message to add to the conversation</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>def inject_system_message(state: ScenarioState) -&gt; None:
    state.add_message({
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;The user is now in a hurry&quot;
    })
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_message(self, message: ChatCompletionMessageParam):
    &#34;&#34;&#34;
    Add a message to the conversation history.

    This method delegates to the scenario executor to properly handle
    message broadcasting and state updates.

    Args:
        message: OpenAI-compatible message to add to the conversation

    Example:
        ```
        def inject_system_message(state: ScenarioState) -&gt; None:
            state.add_message({
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: &#34;The user is now in a hurry&#34;
            })
        ```
    &#34;&#34;&#34;
    self._executor.add_message(message)</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioState.has_tool_call"><code class="name flex">
<span>def <span class="ident">has_tool_call</span></span>(<span>self, tool_name: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if a specific tool has been called in the conversation.</p>
<p>This is a convenience method that returns True if the specified
tool has been called at any point in the conversation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_name</code></strong></dt>
<dd>Name of the tool to check for</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if the tool has been called, False otherwise</p>
<h2 id="example">Example</h2>
<pre><code>def ensure_tool_usage(state: ScenarioState) -&gt; None:
    # Verify the agent used required tools
    assert state.has_tool_call(&quot;search_database&quot;)
    assert state.has_tool_call(&quot;format_results&quot;)

    # Check it didn't use forbidden tools
    assert not state.has_tool_call(&quot;delete_data&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def has_tool_call(self, tool_name: str) -&gt; bool:
    &#34;&#34;&#34;
    Check if a specific tool has been called in the conversation.

    This is a convenience method that returns True if the specified
    tool has been called at any point in the conversation.

    Args:
        tool_name: Name of the tool to check for

    Returns:
        True if the tool has been called, False otherwise

    Example:
        ```
        def ensure_tool_usage(state: ScenarioState) -&gt; None:
            # Verify the agent used required tools
            assert state.has_tool_call(&#34;search_database&#34;)
            assert state.has_tool_call(&#34;format_results&#34;)

            # Check it didn&#39;t use forbidden tools
            assert not state.has_tool_call(&#34;delete_data&#34;)
        ```
    &#34;&#34;&#34;
    return self.last_tool_call(tool_name) is not None</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioState.last_message"><code class="name flex">
<span>def <span class="ident">last_message</span></span>(<span>self) ‑> openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam</span>
</code></dt>
<dd>
<div class="desc"><p>Get the most recent message in the conversation.</p>
<h2 id="returns">Returns</h2>
<p>The last message in the conversation history</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no messages exist in the conversation</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>def check_last_response(state: ScenarioState) -&gt; None:
    last = state.last_message()
    if last[&quot;role&quot;] == &quot;assistant&quot;:
        content = last.get(&quot;content&quot;, &quot;&quot;)
        assert &quot;helpful&quot; in content.lower()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_message(self) -&gt; ChatCompletionMessageParam:
    &#34;&#34;&#34;
    Get the most recent message in the conversation.

    Returns:
        The last message in the conversation history

    Raises:
        ValueError: If no messages exist in the conversation

    Example:
        ```
        def check_last_response(state: ScenarioState) -&gt; None:
            last = state.last_message()
            if last[&#34;role&#34;] == &#34;assistant&#34;:
                content = last.get(&#34;content&#34;, &#34;&#34;)
                assert &#34;helpful&#34; in content.lower()
        ```
    &#34;&#34;&#34;
    if len(self.messages) == 0:
        raise ValueError(&#34;No messages found&#34;)
    return self.messages[-1]</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioState.last_tool_call"><code class="name flex">
<span>def <span class="ident">last_tool_call</span></span>(<span>self, tool_name: str) ‑> openai.types.chat.chat_completion_message_tool_call_param.ChatCompletionMessageToolCallParam | None</span>
</code></dt>
<dd>
<div class="desc"><p>Find the most recent call to a specific tool in the conversation.</p>
<p>Searches through the conversation history in reverse order to find
the last time the specified tool was called by an assistant.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_name</code></strong></dt>
<dd>Name of the tool to search for</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The tool call object if found, None otherwise</p>
<h2 id="example">Example</h2>
<pre><code>def verify_weather_call(state: ScenarioState) -&gt; None:
    weather_call = state.last_tool_call(&quot;get_current_weather&quot;)
    if weather_call:
        args = json.loads(weather_call[&quot;function&quot;][&quot;arguments&quot;])
        assert &quot;location&quot; in args
        print(f&quot;Weather requested for: {args['location']}&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_tool_call(
    self, tool_name: str
) -&gt; Optional[ChatCompletionMessageToolCallParam]:
    &#34;&#34;&#34;
    Find the most recent call to a specific tool in the conversation.

    Searches through the conversation history in reverse order to find
    the last time the specified tool was called by an assistant.

    Args:
        tool_name: Name of the tool to search for

    Returns:
        The tool call object if found, None otherwise

    Example:
        ```
        def verify_weather_call(state: ScenarioState) -&gt; None:
            weather_call = state.last_tool_call(&#34;get_current_weather&#34;)
            if weather_call:
                args = json.loads(weather_call[&#34;function&#34;][&#34;arguments&#34;])
                assert &#34;location&#34; in args
                print(f&#34;Weather requested for: {args[&#39;location&#39;]}&#34;)
        ```
    &#34;&#34;&#34;
    for message in reversed(self.messages):
        if message[&#34;role&#34;] == &#34;assistant&#34; and &#34;tool_calls&#34; in message:
            for tool_call in message[&#34;tool_calls&#34;]:
                if tool_call[&#34;function&#34;][&#34;name&#34;] == tool_name:
                    return tool_call
    return None</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioState.last_user_message"><code class="name flex">
<span>def <span class="ident">last_user_message</span></span>(<span>self) ‑> openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam</span>
</code></dt>
<dd>
<div class="desc"><p>Get the most recent user message in the conversation.</p>
<h2 id="returns">Returns</h2>
<p>The last user message in the conversation history</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no user messages exist in the conversation</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>def analyze_user_intent(state: ScenarioState) -&gt; None:
    user_msg = state.last_user_message()
    content = user_msg[&quot;content&quot;]

    if isinstance(content, str):
        if &quot;urgent&quot; in content.lower():
            print(&quot;User expressed urgency&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_user_message(self) -&gt; ChatCompletionUserMessageParam:
    &#34;&#34;&#34;
    Get the most recent user message in the conversation.

    Returns:
        The last user message in the conversation history

    Raises:
        ValueError: If no user messages exist in the conversation

    Example:
        ```
        def analyze_user_intent(state: ScenarioState) -&gt; None:
            user_msg = state.last_user_message()
            content = user_msg[&#34;content&#34;]

            if isinstance(content, str):
                if &#34;urgent&#34; in content.lower():
                    print(&#34;User expressed urgency&#34;)
        ```
    &#34;&#34;&#34;
    user_messages = [m for m in self.messages if m[&#34;role&#34;] == &#34;user&#34;]
    if not user_messages:
        raise ValueError(&#34;No user messages found&#34;)
    return user_messages[-1]</code></pre>
</details>
</dd>
<dt id="scenario.ScenarioState.model_post_init"><code class="name flex">
<span>def <span class="ident">model_post_init</span></span>(<span>self: BaseModel, context: Any, /) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>This function is meant to behave like a BaseModel method to initialise private attributes.</p>
<p>It takes context as an argument since that's what pydantic-core passes when calling it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>self</code></strong></dt>
<dd>The BaseModel instance.</dd>
<dt><strong><code>context</code></strong></dt>
<dd>The context.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_private_attributes(self: BaseModel, context: Any, /) -&gt; None:
    &#34;&#34;&#34;This function is meant to behave like a BaseModel method to initialise private attributes.

    It takes context as an argument since that&#39;s what pydantic-core passes when calling it.

    Args:
        self: The BaseModel instance.
        context: The context.
    &#34;&#34;&#34;
    if getattr(self, &#39;__pydantic_private__&#39;, None) is None:
        pydantic_private = {}
        for name, private_attr in self.__private_attributes__.items():
            default = private_attr.get_default()
            if default is not PydanticUndefined:
                pydantic_private[name] = default
        object_setattr(self, &#39;__pydantic_private__&#39;, pydantic_private)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scenario.UserSimulatorAgent"><code class="flex name class">
<span>class <span class="ident">UserSimulatorAgent</span></span>
<span>(</span><span>*, model: str | None = None, api_base: str | None = None, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, system_prompt: str | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Agent that simulates realistic user behavior in scenario conversations.</p>
<p>This agent generates user messages that are appropriate for the given scenario
context, simulating how a real human user would interact with the agent under test.
It uses an LLM to generate natural, contextually relevant user inputs that help
drive the conversation forward according to the scenario description.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>role</code></strong></dt>
<dd>Always AgentRole.USER for user simulator agents</dd>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier to use for generating user messages</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>Optional API key for the model provider</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for response generation</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum tokens to generate in user messages</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default user simulation behavior</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code>import scenario

# Basic user simulator with default behavior
user_sim = scenario.UserSimulatorAgent(
    model=&quot;openai/gpt-4.1&quot;
)

# Customized user simulator
custom_user_sim = scenario.UserSimulatorAgent(
    model=&quot;openai/gpt-4.1&quot;,
    temperature=0.3,
    system_prompt=&quot;You are a technical user who asks detailed questions&quot;
)

# Use in scenario
result = await scenario.run(
    name=&quot;user interaction test&quot;,
    description=&quot;User seeks help with Python programming&quot;,
    agents=[
        my_programming_agent,
        user_sim,
        scenario.JudgeAgent(criteria=[&quot;Provides helpful code examples&quot;])
    ]
)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>The user simulator automatically generates short, natural user messages</li>
<li>It follows the scenario description to stay on topic</li>
<li>Messages are generated in a casual, human-like style (lowercase, brief, etc.)</li>
<li>The simulator will not act as an assistant - it only generates user inputs</li>
</ul>
<p>Initialize a user simulator agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>LLM model identifier (e.g., "openai/gpt-4.1").
If not provided, uses the default model from global configuration.</dd>
<dt><strong><code>api_base</code></strong></dt>
<dd>Optional base URL where the model is hosted. If not provided,
uses the base URL from global configuration.</dd>
<dt><strong><code>api_key</code></strong></dt>
<dd>API key for the model provider. If not provided,
uses the key from global configuration or environment.</dd>
<dt><strong><code>temperature</code></strong></dt>
<dd>Sampling temperature for message generation (0.0-1.0).
Lower values make responses more deterministic.</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Maximum number of tokens to generate in user messages.
If not provided, uses model defaults.</dd>
<dt><strong><code>system_prompt</code></strong></dt>
<dd>Custom system prompt to override default user simulation behavior.
Use this to create specialized user personas or behaviors.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model is configured either in parameters or global config</dd>
</dl>
<h2 id="example_1">Example</h2>
<pre><code># Basic user simulator
user_sim = UserSimulatorAgent(model=&quot;openai/gpt-4.1&quot;)

# User simulator with custom persona
expert_user = UserSimulatorAgent(
    model=&quot;openai/gpt-4.1&quot;,
    temperature=0.2,
    system_prompt='''
    You are an expert software developer testing an AI coding assistant.
    Ask challenging, technical questions and be demanding about code quality.
    '''
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UserSimulatorAgent(AgentAdapter):
    &#34;&#34;&#34;
    Agent that simulates realistic user behavior in scenario conversations.

    This agent generates user messages that are appropriate for the given scenario
    context, simulating how a real human user would interact with the agent under test.
    It uses an LLM to generate natural, contextually relevant user inputs that help
    drive the conversation forward according to the scenario description.

    Attributes:
        role: Always AgentRole.USER for user simulator agents
        model: LLM model identifier to use for generating user messages
        api_base: Optional base URL where the model is hosted
        api_key: Optional API key for the model provider
        temperature: Sampling temperature for response generation
        max_tokens: Maximum tokens to generate in user messages
        system_prompt: Custom system prompt to override default user simulation behavior

    Example:
        ```
        import scenario

        # Basic user simulator with default behavior
        user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1&#34;
        )

        # Customized user simulator
        custom_user_sim = scenario.UserSimulatorAgent(
            model=&#34;openai/gpt-4.1&#34;,
            temperature=0.3,
            system_prompt=&#34;You are a technical user who asks detailed questions&#34;
        )

        # Use in scenario
        result = await scenario.run(
            name=&#34;user interaction test&#34;,
            description=&#34;User seeks help with Python programming&#34;,
            agents=[
                my_programming_agent,
                user_sim,
                scenario.JudgeAgent(criteria=[&#34;Provides helpful code examples&#34;])
            ]
        )
        ```

    Note:
        - The user simulator automatically generates short, natural user messages
        - It follows the scenario description to stay on topic
        - Messages are generated in a casual, human-like style (lowercase, brief, etc.)
        - The simulator will not act as an assistant - it only generates user inputs
    &#34;&#34;&#34;

    role = AgentRole.USER

    model: str
    api_base: Optional[str]
    api_key: Optional[str]
    temperature: float
    max_tokens: Optional[int]
    system_prompt: Optional[str]

    def __init__(
        self,
        *,
        model: Optional[str] = None,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        Initialize a user simulator agent.

        Args:
            model: LLM model identifier (e.g., &#34;openai/gpt-4.1&#34;).
                   If not provided, uses the default model from global configuration.
            api_base: Optional base URL where the model is hosted. If not provided,
                      uses the base URL from global configuration.
            api_key: API key for the model provider. If not provided,
                     uses the key from global configuration or environment.
            temperature: Sampling temperature for message generation (0.0-1.0).
                        Lower values make responses more deterministic.
            max_tokens: Maximum number of tokens to generate in user messages.
                       If not provided, uses model defaults.
            system_prompt: Custom system prompt to override default user simulation behavior.
                          Use this to create specialized user personas or behaviors.

        Raises:
            Exception: If no model is configured either in parameters or global config

        Example:
            ```
            # Basic user simulator
            user_sim = UserSimulatorAgent(model=&#34;openai/gpt-4.1&#34;)

            # User simulator with custom persona
            expert_user = UserSimulatorAgent(
                model=&#34;openai/gpt-4.1&#34;,
                temperature=0.2,
                system_prompt=&#39;&#39;&#39;
                You are an expert software developer testing an AI coding assistant.
                Ask challenging, technical questions and be demanding about code quality.
                &#39;&#39;&#39;
            )
            ```
        &#34;&#34;&#34;
        # Override the default system prompt for the user simulator agent
        self.api_base = api_base
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt

        if model:
            self.model = model

        if ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, str
        ):
            self.model = model or ScenarioConfig.default_config.default_model
        elif ScenarioConfig.default_config is not None and isinstance(
            ScenarioConfig.default_config.default_model, ModelConfig
        ):
            self.model = model or ScenarioConfig.default_config.default_model.model
            self.api_base = (
                api_base or ScenarioConfig.default_config.default_model.api_base
            )
            self.api_key = (
                api_key or ScenarioConfig.default_config.default_model.api_key
            )
            self.temperature = (
                temperature or ScenarioConfig.default_config.default_model.temperature
            )
            self.max_tokens = (
                max_tokens or ScenarioConfig.default_config.default_model.max_tokens
            )

        if not hasattr(self, &#34;model&#34;):
            raise Exception(agent_not_configured_error_message(&#34;TestingAgent&#34;))

    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Generate the next user message in the conversation.

        This method analyzes the current conversation state and scenario context
        to generate an appropriate user message that moves the conversation forward
        in a realistic, human-like manner.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: A user message in OpenAI format that continues the conversation

        Note:
            - Messages are generated in a casual, human-like style
            - The simulator follows the scenario description to stay contextually relevant
            - Uses role reversal internally to work around LLM biases toward assistant roles
            - Results are cached when cache_key is configured for deterministic testing
        &#34;&#34;&#34;

        scenario = input.scenario_state

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are pretending to be a user, you are testing an AI Agent (shown as the user role) based on a scenario.
Approach this naturally, as a human user would, with very short inputs, few words, all lowercase, imperative, not periods, like when they google or talk to chatgpt.
&lt;/role&gt;

&lt;goal&gt;
Your goal (assistant) is to interact with the Agent Under Test (user) as if you were a human user to see if it can complete the scenario successfully.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;rules&gt;
- DO NOT carry over any requests yourself, YOU ARE NOT the assistant today, you are the user, send the user message and just STOP.
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Hello, how can I help you today?&#34;},
            *input.messages,
        ]

        # User to assistant role reversal
        # LLM models are biased to always be the assistant not the user, so we need to do this reversal otherwise models like GPT 4.5 is
        # super confused, and Claude 3.7 even starts throwing exceptions.
        messages = reverse_roles(messages)

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=[],
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            message_content = message.content
            if message_content is None:
                raise Exception(f&#34;No response from LLM: {response.__repr__()}&#34;)

            return {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message_content}
        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scenario.UserSimulatorAgent.api_base"><code class="name">var <span class="ident">api_base</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.UserSimulatorAgent.api_key"><code class="name">var <span class="ident">api_key</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.UserSimulatorAgent.max_tokens"><code class="name">var <span class="ident">max_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.UserSimulatorAgent.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.UserSimulatorAgent.system_prompt"><code class="name">var <span class="ident">system_prompt</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="scenario.UserSimulatorAgent.temperature"><code class="name">var <span class="ident">temperature</span> : float</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scenario.UserSimulatorAgent.call"><code class="name flex">
<span>async def <span class="ident">call</span></span>(<span>self, input: <a title="scenario.types.AgentInput" href="types.html#scenario.types.AgentInput">AgentInput</a>) ‑> str | openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam | List[openai.types.chat.chat_completion_developer_message_param.ChatCompletionDeveloperMessageParam | openai.types.chat.chat_completion_system_message_param.ChatCompletionSystemMessageParam | openai.types.chat.chat_completion_user_message_param.ChatCompletionUserMessageParam | openai.types.chat.chat_completion_assistant_message_param.ChatCompletionAssistantMessageParam | openai.types.chat.chat_completion_tool_message_param.ChatCompletionToolMessageParam | openai.types.chat.chat_completion_function_message_param.ChatCompletionFunctionMessageParam] | <a title="scenario.types.ScenarioResult" href="types.html#scenario.types.ScenarioResult">ScenarioResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate the next user message in the conversation.</p>
<p>This method analyzes the current conversation state and scenario context
to generate an appropriate user message that moves the conversation forward
in a realistic, human-like manner.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>AgentInput containing conversation history and scenario context</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AgentReturnTypes</code></dt>
<dd>A user message in OpenAI format that continues the conversation</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Messages are generated in a casual, human-like style</li>
<li>The simulator follows the scenario description to stay contextually relevant</li>
<li>Uses role reversal internally to work around LLM biases toward assistant roles</li>
<li>Results are cached when cache_key is configured for deterministic testing</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    @scenario_cache()
    async def call(
        self,
        input: AgentInput,
    ) -&gt; AgentReturnTypes:
        &#34;&#34;&#34;
        Generate the next user message in the conversation.

        This method analyzes the current conversation state and scenario context
        to generate an appropriate user message that moves the conversation forward
        in a realistic, human-like manner.

        Args:
            input: AgentInput containing conversation history and scenario context

        Returns:
            AgentReturnTypes: A user message in OpenAI format that continues the conversation

        Note:
            - Messages are generated in a casual, human-like style
            - The simulator follows the scenario description to stay contextually relevant
            - Uses role reversal internally to work around LLM biases toward assistant roles
            - Results are cached when cache_key is configured for deterministic testing
        &#34;&#34;&#34;

        scenario = input.scenario_state

        messages = [
            {
                &#34;role&#34;: &#34;system&#34;,
                &#34;content&#34;: self.system_prompt
                or f&#34;&#34;&#34;
&lt;role&gt;
You are pretending to be a user, you are testing an AI Agent (shown as the user role) based on a scenario.
Approach this naturally, as a human user would, with very short inputs, few words, all lowercase, imperative, not periods, like when they google or talk to chatgpt.
&lt;/role&gt;

&lt;goal&gt;
Your goal (assistant) is to interact with the Agent Under Test (user) as if you were a human user to see if it can complete the scenario successfully.
&lt;/goal&gt;

&lt;scenario&gt;
{scenario.description}
&lt;/scenario&gt;

&lt;rules&gt;
- DO NOT carry over any requests yourself, YOU ARE NOT the assistant today, you are the user, send the user message and just STOP.
&lt;/rules&gt;
&#34;&#34;&#34;,
            },
            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Hello, how can I help you today?&#34;},
            *input.messages,
        ]

        # User to assistant role reversal
        # LLM models are biased to always be the assistant not the user, so we need to do this reversal otherwise models like GPT 4.5 is
        # super confused, and Claude 3.7 even starts throwing exceptions.
        messages = reverse_roles(messages)

        response = cast(
            ModelResponse,
            completion(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.api_base,
                max_tokens=self.max_tokens,
                tools=[],
            ),
        )

        # Extract the content from the response
        if hasattr(response, &#34;choices&#34;) and len(response.choices) &gt; 0:
            message = cast(Choices, response.choices[0]).message

            message_content = message.content
            if message_content is None:
                raise Exception(f&#34;No response from LLM: {response.__repr__()}&#34;)

            return {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: message_content}
        else:
            raise Exception(
                f&#34;Unexpected response format from LLM: {response.__repr__()}&#34;
            )</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scenario.agent_adapter.AgentAdapter" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter">AgentAdapter</a></b></code>:
<ul class="hlist">
<li><code><a title="scenario.agent_adapter.AgentAdapter.role" href="agent_adapter.html#scenario.agent_adapter.AgentAdapter.role">role</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="/" style="color: #000">← Back to Docs</a>
<h1>Scenario API Reference</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="scenario.agent" href="#scenario.agent">agent</a></code></li>
<li><code><a title="scenario.cache" href="#scenario.cache">cache</a></code></li>
<li><code><a title="scenario.configure" href="#scenario.configure">configure</a></code></li>
<li><code><a title="scenario.fail" href="#scenario.fail">fail</a></code></li>
<li><code><a title="scenario.judge" href="#scenario.judge">judge</a></code></li>
<li><code><a title="scenario.message" href="#scenario.message">message</a></code></li>
<li><code><a title="scenario.proceed" href="#scenario.proceed">proceed</a></code></li>
<li><code><a title="scenario.run" href="#scenario.run">run</a></code></li>
<li><code><a title="scenario.succeed" href="#scenario.succeed">succeed</a></code></li>
<li><code><a title="scenario.user" href="#scenario.user">user</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="scenario.agent_adapter" href="agent_adapter.html">scenario.agent_adapter</a></code></li>
<li><code><a title="scenario.config" href="config/index.html">scenario.config</a></code></li>
<li><code><a title="scenario.judge_agent" href="judge_agent.html">scenario.judge_agent</a></code></li>
<li><code><a title="scenario.pytest_plugin" href="pytest_plugin.html">scenario.pytest_plugin</a></code></li>
<li><code><a title="scenario.scenario_executor" href="scenario_executor.html">scenario.scenario_executor</a></code></li>
<li><code><a title="scenario.scenario_state" href="scenario_state.html">scenario.scenario_state</a></code></li>
<li><code><a title="scenario.script" href="script.html">scenario.script</a></code></li>
<li><code><a title="scenario.types" href="types.html">scenario.types</a></code></li>
<li><code><a title="scenario.user_simulator_agent" href="user_simulator_agent.html">scenario.user_simulator_agent</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scenario.AgentAdapter" href="#scenario.AgentAdapter">AgentAdapter</a></code></h4>
<ul class="">
<li><code><a title="scenario.AgentAdapter.call" href="#scenario.AgentAdapter.call">call</a></code></li>
<li><code><a title="scenario.AgentAdapter.role" href="#scenario.AgentAdapter.role">role</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.AgentInput" href="#scenario.AgentInput">AgentInput</a></code></h4>
<ul class="">
<li><code><a title="scenario.AgentInput.judgment_request" href="#scenario.AgentInput.judgment_request">judgment_request</a></code></li>
<li><code><a title="scenario.AgentInput.last_new_user_message" href="#scenario.AgentInput.last_new_user_message">last_new_user_message</a></code></li>
<li><code><a title="scenario.AgentInput.last_new_user_message_str" href="#scenario.AgentInput.last_new_user_message_str">last_new_user_message_str</a></code></li>
<li><code><a title="scenario.AgentInput.messages" href="#scenario.AgentInput.messages">messages</a></code></li>
<li><code><a title="scenario.AgentInput.model_config" href="#scenario.AgentInput.model_config">model_config</a></code></li>
<li><code><a title="scenario.AgentInput.new_messages" href="#scenario.AgentInput.new_messages">new_messages</a></code></li>
<li><code><a title="scenario.AgentInput.scenario_state" href="#scenario.AgentInput.scenario_state">scenario_state</a></code></li>
<li><code><a title="scenario.AgentInput.thread_id" href="#scenario.AgentInput.thread_id">thread_id</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.AgentRole" href="#scenario.AgentRole">AgentRole</a></code></h4>
<ul class="">
<li><code><a title="scenario.AgentRole.AGENT" href="#scenario.AgentRole.AGENT">AGENT</a></code></li>
<li><code><a title="scenario.AgentRole.JUDGE" href="#scenario.AgentRole.JUDGE">JUDGE</a></code></li>
<li><code><a title="scenario.AgentRole.USER" href="#scenario.AgentRole.USER">USER</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.JudgeAgent" href="#scenario.JudgeAgent">JudgeAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.JudgeAgent.api_base" href="#scenario.JudgeAgent.api_base">api_base</a></code></li>
<li><code><a title="scenario.JudgeAgent.api_key" href="#scenario.JudgeAgent.api_key">api_key</a></code></li>
<li><code><a title="scenario.JudgeAgent.call" href="#scenario.JudgeAgent.call">call</a></code></li>
<li><code><a title="scenario.JudgeAgent.criteria" href="#scenario.JudgeAgent.criteria">criteria</a></code></li>
<li><code><a title="scenario.JudgeAgent.max_tokens" href="#scenario.JudgeAgent.max_tokens">max_tokens</a></code></li>
<li><code><a title="scenario.JudgeAgent.model" href="#scenario.JudgeAgent.model">model</a></code></li>
<li><code><a title="scenario.JudgeAgent.system_prompt" href="#scenario.JudgeAgent.system_prompt">system_prompt</a></code></li>
<li><code><a title="scenario.JudgeAgent.temperature" href="#scenario.JudgeAgent.temperature">temperature</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.ScenarioConfig" href="#scenario.ScenarioConfig">ScenarioConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.ScenarioConfig.cache_key" href="#scenario.ScenarioConfig.cache_key">cache_key</a></code></li>
<li><code><a title="scenario.ScenarioConfig.configure" href="#scenario.ScenarioConfig.configure">configure</a></code></li>
<li><code><a title="scenario.ScenarioConfig.debug" href="#scenario.ScenarioConfig.debug">debug</a></code></li>
<li><code><a title="scenario.ScenarioConfig.default_config" href="#scenario.ScenarioConfig.default_config">default_config</a></code></li>
<li><code><a title="scenario.ScenarioConfig.default_model" href="#scenario.ScenarioConfig.default_model">default_model</a></code></li>
<li><code><a title="scenario.ScenarioConfig.headless" href="#scenario.ScenarioConfig.headless">headless</a></code></li>
<li><code><a title="scenario.ScenarioConfig.items" href="#scenario.ScenarioConfig.items">items</a></code></li>
<li><code><a title="scenario.ScenarioConfig.max_turns" href="#scenario.ScenarioConfig.max_turns">max_turns</a></code></li>
<li><code><a title="scenario.ScenarioConfig.merge" href="#scenario.ScenarioConfig.merge">merge</a></code></li>
<li><code><a title="scenario.ScenarioConfig.model_config" href="#scenario.ScenarioConfig.model_config">model_config</a></code></li>
<li><code><a title="scenario.ScenarioConfig.verbose" href="#scenario.ScenarioConfig.verbose">verbose</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.ScenarioResult" href="#scenario.ScenarioResult">ScenarioResult</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.ScenarioResult.agent_time" href="#scenario.ScenarioResult.agent_time">agent_time</a></code></li>
<li><code><a title="scenario.ScenarioResult.failed_criteria" href="#scenario.ScenarioResult.failed_criteria">failed_criteria</a></code></li>
<li><code><a title="scenario.ScenarioResult.messages" href="#scenario.ScenarioResult.messages">messages</a></code></li>
<li><code><a title="scenario.ScenarioResult.model_config" href="#scenario.ScenarioResult.model_config">model_config</a></code></li>
<li><code><a title="scenario.ScenarioResult.passed_criteria" href="#scenario.ScenarioResult.passed_criteria">passed_criteria</a></code></li>
<li><code><a title="scenario.ScenarioResult.reasoning" href="#scenario.ScenarioResult.reasoning">reasoning</a></code></li>
<li><code><a title="scenario.ScenarioResult.success" href="#scenario.ScenarioResult.success">success</a></code></li>
<li><code><a title="scenario.ScenarioResult.total_time" href="#scenario.ScenarioResult.total_time">total_time</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.ScenarioState" href="#scenario.ScenarioState">ScenarioState</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.ScenarioState.add_message" href="#scenario.ScenarioState.add_message">add_message</a></code></li>
<li><code><a title="scenario.ScenarioState.config" href="#scenario.ScenarioState.config">config</a></code></li>
<li><code><a title="scenario.ScenarioState.current_turn" href="#scenario.ScenarioState.current_turn">current_turn</a></code></li>
<li><code><a title="scenario.ScenarioState.description" href="#scenario.ScenarioState.description">description</a></code></li>
<li><code><a title="scenario.ScenarioState.has_tool_call" href="#scenario.ScenarioState.has_tool_call">has_tool_call</a></code></li>
<li><code><a title="scenario.ScenarioState.last_message" href="#scenario.ScenarioState.last_message">last_message</a></code></li>
<li><code><a title="scenario.ScenarioState.last_tool_call" href="#scenario.ScenarioState.last_tool_call">last_tool_call</a></code></li>
<li><code><a title="scenario.ScenarioState.last_user_message" href="#scenario.ScenarioState.last_user_message">last_user_message</a></code></li>
<li><code><a title="scenario.ScenarioState.messages" href="#scenario.ScenarioState.messages">messages</a></code></li>
<li><code><a title="scenario.ScenarioState.model_config" href="#scenario.ScenarioState.model_config">model_config</a></code></li>
<li><code><a title="scenario.ScenarioState.model_post_init" href="#scenario.ScenarioState.model_post_init">model_post_init</a></code></li>
<li><code><a title="scenario.ScenarioState.thread_id" href="#scenario.ScenarioState.thread_id">thread_id</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scenario.UserSimulatorAgent" href="#scenario.UserSimulatorAgent">UserSimulatorAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="scenario.UserSimulatorAgent.api_base" href="#scenario.UserSimulatorAgent.api_base">api_base</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.api_key" href="#scenario.UserSimulatorAgent.api_key">api_key</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.call" href="#scenario.UserSimulatorAgent.call">call</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.max_tokens" href="#scenario.UserSimulatorAgent.max_tokens">max_tokens</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.model" href="#scenario.UserSimulatorAgent.model">model</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.system_prompt" href="#scenario.UserSimulatorAgent.system_prompt">system_prompt</a></code></li>
<li><code><a title="scenario.UserSimulatorAgent.temperature" href="#scenario.UserSimulatorAgent.temperature">temperature</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
