import { RefLink } from "../../../components/RefLink";

# Testing Voice Agents [Evaluate your voice agent with Scenario]

Scenario lets you write **end-to-end tests** for agents that listen to audio, think, and respond with either text _or_ audio.
This page shows three common patterns and how to test them.

## Overview

1. **Audio ➜ Text** – the agent receives an audio file (e.g. WAV) and replies with a textual answer.
2. **Audio ➜ Audio** – the agent listens and replies _in audio_ (great for voice assistants).
3. **Voice-to-Voice Conversation** – both the user simulator _and_ the agent speak over multiple turns (came out **before** OpenAI’s real-time audio API and still works today).

## Use-case comparison

| Scenario                    | Input                                         | Expected Output        | Typical Judge Model                                  |
| --------------------------- | --------------------------------------------- | ---------------------- | ---------------------------------------------------- |
| Audio ➜ Text                | `file` part (audio) + optional prompt         | Text                   | `gpt-4o-audio-preview` or any GPT-4-level text model |
| Audio ➜ Audio               | `file` part (audio) + optional prompt         | Audio (voice response) | `gpt-4o-audio-preview` (handles audio)               |
| Voice-to-Voice Conversation | Multiple turns, both sides send/receive audio | Audio dialogue         | Same as above; judge runs after conversation         |

## Prerequisites & Setup

Before running the examples you’ll need a couple of things in place:

1. **Node.js ≥ 18** – the OpenAI `gpt-4o` voice model uses modern `fetch`.
2. **OPENAI_API_KEY** – export an API key that has access to `gpt-4o-audio-preview`:

```bash
export OPENAI_API_KEY="sk-…"
```

## Code Walk-through

:::code-group

```typescript [audio ➜ text (javascript)]
import scenario, {
  AgentAdapter,
  AgentInput,
  AgentRole,
} from "@langwatch/scenario";
import { describe, it, expect } from "vitest";
import OpenAI from "openai";
import { openai } from "@ai-sdk/openai";
import { ChatCompletionMessageParam } from "openai/resources/chat/completions.mjs";
import { encodeAudioToBase64, getFixturePath } from "./helpers";
import { CoreUserMessage } from "ai";
import { convertCoreMessagesToOpenAIMessages } from "./helpers/convert-core-messages-to-openai";

class AudioAgent extends AgentAdapter {
  role: AgentRole = AgentRole.AGENT;
  private openai = new OpenAI();

  call = async (input: AgentInput) => {
    // Convert Core messages → OpenAI shape so the voice-model accepts them
    const messages = convertCoreMessagesToOpenAIMessages(input.messages);
    const response = await this.respond(messages);

    // Scenario expects **text**, so we extract the transcript only
    const transcript = response.choices[0].message?.audio?.transcript;
    if (typeof transcript === "string") return transcript;
    throw new Error("Agent failed to generate a response");
  };

  private async respond(messages: ChatCompletionMessageParam[]) {
    return this.openai.chat.completions.create({
      model: "gpt-4o-audio-preview",
      modalities: ["text", "audio"],
      audio: { voice: "alloy", format: "wav" },
      messages,
      store: false,
    });
  }
}

const setId = "multimodal-audio-test";

describe("Multimodal Audio to Text Tests", () => {
  it("should handle audio input", async () => {
    const data = encodeAudioToBase64(
      getFixturePath("male_or_female_voice.wav")
    );

    const audioMessage = {
      role: "user",
      content: [
        { type: "text", text: "Answer the question in the audio…" },
        { type: "file", mimeType: "audio/wav", data },
      ],
    } satisfies CoreUserMessage;

    const audioJudge = scenario.judgeAgent({
      model: openai("gpt-4o-audio-preview"),
      criteria: [
        "Agent correctly guesses it's a male voice",
        "Agent repeats the question",
        "Agent says what format the input was in (audio or text)",
      ],
    });

    const result = await scenario.run({
      name: "multimodal audio analysis",
      description: "User sends audio, agent transcribes & analyses",
      agents: [new AudioAgent(), scenario.userSimulatorAgent(), audioJudge],
      script: [
        scenario.message(audioMessage),
        scenario.agent(),
        scenario.judge(),
      ],
      setId,
    });

    expect(result.success).toBe(true);
  });
});
```

```typescript [audio ➜ audio (javascript)]
import scenario, { AgentRole } from "@langwatch/scenario";
import { describe, it, expect } from "vitest";
import { openai } from "@ai-sdk/openai";
import {
  encodeAudioToBase64,
  getFixturePath,
  OpenAiVoiceAgent,
} from "./helpers";

class AudioAgent extends OpenAiVoiceAgent {
  role: AgentRole = AgentRole.AGENT;
}

const setId = "multimodal-audio-test";

describe("Multimodal Audio to Audio Tests", () => {
  it("should handle audio input", async () => {
    const data = encodeAudioToBase64(
      getFixturePath("male_or_female_voice.wav")
    );

    const audioMessage = {
      role: "user",
      content: [
        { type: "text", text: "Answer the question in the audio…" },
        { type: "file", mimeType: "audio/wav", data },
      ],
    };

    const judge = scenario.judgeAgent({
      model: openai("gpt-4o-audio-preview"),
      criteria: [
        "Agent guesses gender",
        "Agent repeats the question",
        "Agent identifies input format",
      ],
    });

    const result = await scenario.run({
      setId,
      name: "multimodal audio analysis",
      description: "User sends audio, agent replies in audio",
      agents: [
        new AudioAgent({ voice: "alloy" }),
        scenario.userSimulatorAgent(),
        judge,
      ],
      script: [
        scenario.message(audioMessage),
        scenario.agent(),
        scenario.judge(),
      ],
    });

    expect(result.success).toBe(true);
  });
});
```

```typescript [voice-to-voice conversation (javascript)]
import scenario, { AgentInput, AgentRole } from "@langwatch/scenario";
import { describe, it, expect } from "vitest";
import { openai } from "@ai-sdk/openai";
import { OpenAiVoiceAgent, messageRoleReversal } from "./helpers";

class AssistantAgent extends OpenAiVoiceAgent {
  role: AgentRole = AgentRole.AGENT;
  constructor() {
    super({ voice: "echo", systemPrompt: "Helpful & concise" });
  }
}

class UserSimulator extends OpenAiVoiceAgent {
  role: AgentRole = AgentRole.USER;
  constructor() {
    super({ voice: "nova", systemPrompt: "Curious novice" });
  }
  public async call(input: AgentInput) {
    // Reverse roles so the agent can impersonate the user
    return super.call({
      ...input,
      messages: messageRoleReversal(input.messages),
    });
  }
}

describe("Voice-to-Voice Conversation", () => {
  it("should handle a complete audio conversation", async () => {
    const convJudge = scenario.judgeAgent({
      model: openai("gpt-4o-audio-preview"),
      criteria: ["Conversation flows naturally between user and agent"],
    });

    const result = await scenario.run({
      name: "full audio conversation",
      description: "Multi-turn audio dialogue",
      agents: [new AssistantAgent(), new UserSimulator(), convJudge],
      script: [scenario.proceed(6), scenario.judge()],
      setId: "full-audio-conversation-test",
    });

    expect(result.success).toBe(true);
  });
});
```

:::

## Listen to a real conversation

Below is the actual audio produced by the **voice-to-voice** test above. Click play to hear the agent and user simulator exchanging ideas.

<audio controls style={{ width: "100%" }}>
  <source src="/audio/full-conversation-3.wav" type="audio/wav" />
  Your browser does not support the audio element.
</audio>

## Helper utilities & caveats

The examples above rely on several helper utilities that handle the complexity of working with OpenAI's voice models:

- **[encodeAudioToBase64](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/audio-encoding.ts)** - Converts audio files to base64-encoded strings for transmission in messages. Used to prepare audio fixtures for testing.

- **[getFixturePath](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/fixture-utils.ts)** - Utility to resolve paths to test fixtures (like audio files) relative to the test directory.

- **[convertCoreMessagesToOpenAIMessages](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/convert-core-messages-to-openai.ts)** - Converts Scenario's CoreMessage format to OpenAI's ChatCompletion format, handling audio file detection and transformation into the `input_audio` shape that GPT-4o expects.

- **[OpenAiVoiceAgent](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/openai-voice-agent.ts)** - Abstract base class that handles the OpenAI voice API calls, message conversion, and response processing. Used by both the agent and user simulator in voice-to-voice conversations.

- **[messageRoleReversal](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/audio-conversation.ts)** - Utility function that swaps user ↔ assistant roles in messages (excluding tool calls) so the voice user simulator can speak as the user.

- **[saveConversationAudio](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/audio-conversation.ts)** - Saves audio responses to files for debugging and analysis.

- **[concatenateWavFiles](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/audio-conversation.ts)** - Combines multiple WAV files into a single conversation recording.

- **[getAudioSegments](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/helpers/audio-conversation.ts)** - Extracts individual audio segments from a conversation for analysis.

These helpers handle the technical details like audio encoding, file management, message format conversion, and managing the OpenAI API's specific requirements for voice models.

💡 **Caveat**: When the assistant responds with audio, some judge models may ignore the audio chunk unless the role is set to `"user"`. Pass `forceUserRole: true` to `OpenAiVoiceAgent` when you hit that edge-case.

## Troubleshooting & FAQs

**Judge ignores assistant audio**
Some judge models drop the audio chunk when it comes from the _assistant_ role. Pass `forceUserRole: true` to `OpenAiVoiceAgent`—this wraps the audio in a `"user"` message so the judge evaluates it correctly.

**Tests time-out or hang**
Voice models are slower than text-only ones. Bump Vitest’s timeout (`--timeout 60000`) or reduce concurrency (`VITEST_MAX_WORKERS=1`) when running in CI.

**“Unsupported media type” errors**
`convertCoreMessagesToOpenAIMessages` currently supports WAV/MP3.

**CI machines without audio hardware**
All examples work headlessly—no speakers or microphone required.

**Python equivalent?**
Coming soon!

💸 **Cost tip**: each voice request is billed the same as a standard GPT-4 audio-preview call. Keep fixture lengths and turn counts reasonable.

## Complete Sources

Browse the full tests in the repo:

- [audio ➜ text](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/multimodal-audio-to-text.test.ts)
- [audio ➜ audio](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/multimodal-audio-to-audio.test.ts)
- [voice-to-voice conversation](https://github.com/langwatch/scenario/blob/main/javascript/examples/vitest/tests/multimodal-voice-to-voice-conversation.test.ts)

Need more? See the [fixtures guide](/testing-guides/fixtures) and [agent docs](/agent-integration).
