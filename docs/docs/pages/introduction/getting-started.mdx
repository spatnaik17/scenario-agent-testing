import { EmbeddedScript } from "../../components/EmbeddedScript";
import { RefLink } from "../../components/RefLink";
import { Callout } from "vocs/components";
import { Button } from "vocs/components";

# Getting Started - Your First Scenario [A scenario is a test for a situation that your agent must be able to handle]

## Installation

Install Scenario, along with a test runner (e.g. pytest or vitest):

:::code-group

```bash [python]
uv add langwatch-scenario pytest
```

```bash [typescript]
pnpm install @langwatch/scenario vitest
```

:::

## Quick Start

Let's create your first scenario test. This example shows how to test a simple vegetarian recipe agent:

### 1. Create your first test

:::code-group

```python [python]
# my_vegetarian_recipe_agent.py
import pytest
import scenario
import litellm

# Configure the default model for simulations
scenario.configure(default_model="openai/gpt-4.1-mini")

@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent():
    # 1. Create your agent adapter
    class RecipeAgent(scenario.AgentAdapter):
        async def call(self, input: scenario.AgentInput) -> scenario.AgentReturnTypes:
            return vegetarian_recipe_agent(input.messages)

    # 2. Run the scenario
    result = await scenario.run(
        name="dinner recipe request",
        description="""
            It's saturday evening, the user is very hungry and tired,
            but have no money to order out, so they are looking for a recipe.
        """,
        agents=[
            RecipeAgent(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(criteria=[
                "Agent should not ask more than two follow-up questions",
                "Agent should generate a recipe",
                "Recipe should include a list of ingredients",
                "Recipe should include step-by-step cooking instructions",
                "Recipe should be vegetarian and not include any sort of meat",
            ])
        ],
    )

    # 3. Assert the result
    assert result.success

# Example agent implementation using litellm
@scenario.cache()
def vegetarian_recipe_agent(messages) -> scenario.AgentReturnTypes:
    response = litellm.completion(
        model="openai/gpt-4.1-mini",
        messages=[
            {
                "role": "system",
                "content": """
                    You are a vegetarian recipe agent.
                    Given the user request, ask AT MOST ONE follow-up question,
                    then provide a complete recipe. Keep your responses concise and focused.
                """,
            },
            *messages,
        ],
    )
    return response.choices[0].message
```

```typescript [typescript]
// vegetarian-recipe.test.ts
import scenario, { type AgentAdapter, AgentRole } from "@langwatch/scenario";
import { describe, it, expect } from "vitest";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

// 1. Create your agent adapter
const vegetarianRecipeAgent: AgentAdapter = {
  role: AgentRole.AGENT,
  call: async (input) => {
    const response = await generateText({
      model: openai("gpt-4.1-mini"),
      messages: [
        { role: "system", content: "You are a vegetarian recipe agent." },
        ...input.messages,
      ],
    });
    return response.text;
  },
};

describe("Vegetarian Recipe Agent", () => {
  it("should generate a vegetarian recipe", async () => {
    // 2. Run the scenario
    const result = await scenario.run({
      name: "dinner recipe request",
      description: `It's saturday evening, the user is very hungry and tired, but have no money to order out, so they are looking for a recipe.`,
      agents: [
        vegetarianRecipeAgent,
        scenario.userSimulatorAgent(),
        scenario.judgeAgent({
          criteria: [
            "Agent should not ask more than two follow-up questions",
            "Agent should generate a recipe",
            "Recipe should include a list of ingredients",
            "Recipe should include step-by-step cooking instructions",
            "Recipe should be vegetarian and not include any sort of meat",
          ],
        }),
      ],
    });

    // 3. Assert the result
    expect(result.success).toBe(true);
  });
});
```

:::

### 2. Set up your environment

```bash
OPENAI_API_KEY=<your-api-key>
LANGWATCH_API_KEY=<your-api-key> # optional, for simulation reporting
```

### 3. Run your test

:::code-group

```bash [python]
uv run pytest -s test_my_agent.py
```

```bash [typescript]
npx vitest run vegetarian-recipe.test.ts
```

:::

## Visualize Your Results Instantly!

<Callout type="tip">
**After writing your first scenario, unlock the full power of Scenario by visualizing your conversations and results in the [Simulations Visualizer](https://app.langwatch.ai/@project/simulations).**

- Browse every conversation
- Debug tool calls and agent decisions
- Share and review results with your team

_See your agent's conversations and decisions come to life:_

![Simulation Results](/images/simulations/simulation-set-overview.png "Simulation Results - detailed view")

<Button href="https://app.langwatch.ai/@project/simulations">
  Open the Simulations Visualizer
</Button>
<Button href="../visualizations">
  Learn more about visualizing simulations
</Button>

</Callout>

### Console Output

You should see output showing the conversation between the simulated user and your agent, followed by the judge's evaluation:

<EmbeddedScript
  id="asciicast-nvO5GWGzqKTTCd8gtNSezQw11"
  src="https://asciinema.org/a/nvO5GWGzqKTTCd8gtNSezQw11.js"
/>

## What happened?

In this example:

1. **Agent Integration**: You created an <RefLink link={{ python: "agent_adapter.html#scenario.agent_adapter.AgentAdapter", typescript: "classes/AgentAdapter.html" }} code={{ python: "AgentAdapter", typescript: "AgentAdapter" }} />
2. **Simulation**: The <RefLink link={{ python: "user_simulator_agent.html", typescript: "user_simulator_agent.html" }} code={{ python: "UserSimulatorAgent", typescript: "userSimulatorAgent" }} /> automatically generated realistic user messages based on the scenario description
3. **Evaluation**: The <RefLink link={{ python: "judge_agent.html", typescript: "judge_agent.html" }} code={{ python: "JudgeAgent", typescript: "judgeAgent" }} /> evaluated the conversation against your criteria
4. **Caching**: The <RefLink link={{ python: "index.html#scenario.cache" }} code={{ python: "@scenario.cache()" }} /> decorator (Python) made your agent calls deterministic for testing

## Key Concepts

- **Scenarios**: Test cases that describe the situation and expected behavior
- **Agent Under Test**: Your agent being tested on the scenario
- **User Simulator Agent**: Agent that simulates user messages to interact with the agent under test
- **Judge Agent**: Agent that decides if the conversation should proceed or not, while evaluating it against your criteria
- **Script**: Optional way to control the exact flow of conversation

## Next Steps

- Learn about [Scenario Basics](../basics/concepts) to understand the framework deeply
- Explore [Agent Integration](../agent-integration) to connect your existing agents
- Check out more [python examples](https://github.com/langwatch/scenario/tree/main/python/examples) and [TypeScript examples](https://github.com/langwatch/scenario/tree/main/javascript/examples) on GitHub
- Read about [Configuration](../basics/configuration) for project setup, environment variables, and logging
- See [Test Runner Integration](../basics/test-runner-integration) for using Scenario with Vitest and Jest
